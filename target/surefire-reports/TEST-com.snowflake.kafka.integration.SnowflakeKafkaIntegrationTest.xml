<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="10.057" tests="9" errors="0" skipped="0" failures="0">
  <properties>
    <property name="java.specification.version" value="21"/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.class.path" value="/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC/target/test-classes:/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC/target/classes:/Users/tspann/.m2/repository/net/snowflake/snowflake-jdbc/3.14.4/snowflake-jdbc-3.14.4.jar:/Users/tspann/.m2/repository/org/apache/kafka/kafka-clients/3.6.0/kafka-clients-3.6.0.jar:/Users/tspann/.m2/repository/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar:/Users/tspann/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/tspann/.m2/repository/org/xerial/snappy/snappy-java/1.1.10.4/snappy-java-1.1.10.4.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.16.0/jackson-databind-2.16.0.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.16.0/jackson-annotations-2.16.0.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.16.0/jackson-core-2.16.0.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.16.0/jackson-datatype-jsr310-2.16.0.jar:/Users/tspann/.m2/repository/com/zaxxer/HikariCP/5.1.0/HikariCP-5.1.0.jar:/Users/tspann/.m2/repository/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar:/Users/tspann/.m2/repository/org/slf4j/slf4j-api/2.0.9/slf4j-api-2.0.9.jar:/Users/tspann/.m2/repository/ch/qos/logback/logback-classic/1.4.14/logback-classic-1.4.14.jar:/Users/tspann/.m2/repository/ch/qos/logback/logback-core/1.4.14/logback-core-1.4.14.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-core/1.12.0/micrometer-core-1.12.0.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-commons/1.12.0/micrometer-commons-1.12.0.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-observation/1.12.0/micrometer-observation-1.12.0.jar:/Users/tspann/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar:/Users/tspann/.m2/repository/org/latencyutils/LatencyUtils/2.0.3/LatencyUtils-2.0.3.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-registry-jmx/1.12.0/micrometer-registry-jmx-1.12.0.jar:/Users/tspann/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.2.22/metrics-jmx-4.2.22.jar:/Users/tspann/.m2/repository/io/dropwizard/metrics/metrics-core/4.2.22/metrics-core-4.2.22.jar:/Users/tspann/.m2/repository/org/bouncycastle/bcpkix-jdk18on/1.77/bcpkix-jdk18on-1.77.jar:/Users/tspann/.m2/repository/org/bouncycastle/bcutil-jdk18on/1.77/bcutil-jdk18on-1.77.jar:/Users/tspann/.m2/repository/org/bouncycastle/bcprov-jdk18on/1.77/bcprov-jdk18on-1.77.jar:/Users/tspann/.m2/repository/com/typesafe/config/1.4.3/config-1.4.3.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter/5.10.1/junit-jupiter-5.10.1.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.10.1/junit-jupiter-api-5.10.1.jar:/Users/tspann/.m2/repository/org/opentest4j/opentest4j/1.3.0/opentest4j-1.3.0.jar:/Users/tspann/.m2/repository/org/junit/platform/junit-platform-commons/1.10.1/junit-platform-commons-1.10.1.jar:/Users/tspann/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.10.1/junit-jupiter-params-5.10.1.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.10.1/junit-jupiter-engine-5.10.1.jar:/Users/tspann/.m2/repository/org/junit/platform/junit-platform-engine/1.10.1/junit-platform-engine-1.10.1.jar:/Users/tspann/.m2/repository/org/testcontainers/junit-jupiter/1.19.3/junit-jupiter-1.19.3.jar:/Users/tspann/.m2/repository/org/testcontainers/testcontainers/1.19.3/testcontainers-1.19.3.jar:/Users/tspann/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/Users/tspann/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/tspann/.m2/repository/org/apache/commons/commons-compress/1.24.0/commons-compress-1.24.0.jar:/Users/tspann/.m2/repository/org/rnorth/duct-tape/duct-tape/1.0.8/duct-tape-1.0.8.jar:/Users/tspann/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/tspann/.m2/repository/com/github/docker-java/docker-java-api/3.3.4/docker-java-api-3.3.4.jar:/Users/tspann/.m2/repository/com/github/docker-java/docker-java-transport-zerodep/3.3.4/docker-java-transport-zerodep-3.3.4.jar:/Users/tspann/.m2/repository/com/github/docker-java/docker-java-transport/3.3.4/docker-java-transport-3.3.4.jar:/Users/tspann/.m2/repository/net/java/dev/jna/jna/5.13.0/jna-5.13.0.jar:/Users/tspann/.m2/repository/org/testcontainers/kafka/1.19.3/kafka-1.19.3.jar:/Users/tspann/.m2/repository/org/mockito/mockito-core/5.8.0/mockito-core-5.8.0.jar:/Users/tspann/.m2/repository/net/bytebuddy/byte-buddy/1.14.10/byte-buddy-1.14.10.jar:/Users/tspann/.m2/repository/net/bytebuddy/byte-buddy-agent/1.14.10/byte-buddy-agent-1.14.10.jar:/Users/tspann/.m2/repository/org/objenesis/objenesis/3.3/objenesis-3.3.jar:"/>
    <property name="java.vm.vendor" value="Eclipse Adoptium"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="KAFKA_BOOTSTRAP_SERVERS" value="PLAINTEXT://localhost:50137"/>
    <property name="SNOWFLAKE_SCHEMA" value="test-schema"/>
    <property name="java.vendor.url" value="https://adoptium.net/"/>
    <property name="user.timezone" value="America/New_York"/>
    <property name="SNOWFLAKE_WAREHOUSE" value="test-warehouse"/>
    <property name="os.name" value="Mac OS X"/>
    <property name="java.vm.specification.version" value="21"/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="user.country" value="US"/>
    <property name="sun.boot.library.path" value="/Users/tspann/.sdkman/candidates/java/21.0.6-tem/lib"/>
    <property name="SNOWFLAKE_DATABASE" value="test-database"/>
    <property name="sun.java.command" value="/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC/target/surefire/surefirebooter-20251003165408133_3.jar /Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC/target/surefire 2025-10-03T16-54-08_094-jvmRun1 surefire-20251003165408133_1tmp surefire_0-20251003165408133_2tmp"/>
    <property name="SNOWFLAKE_PASSWORD" value="test-password"/>
    <property name="http.nonProxyHosts" value="local|*.local|169.254/16|*.169.254/16"/>
    <property name="jdk.debug" value="release"/>
    <property name="surefire.test.class.path" value="/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC/target/test-classes:/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC/target/classes:/Users/tspann/.m2/repository/net/snowflake/snowflake-jdbc/3.14.4/snowflake-jdbc-3.14.4.jar:/Users/tspann/.m2/repository/org/apache/kafka/kafka-clients/3.6.0/kafka-clients-3.6.0.jar:/Users/tspann/.m2/repository/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar:/Users/tspann/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/tspann/.m2/repository/org/xerial/snappy/snappy-java/1.1.10.4/snappy-java-1.1.10.4.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.16.0/jackson-databind-2.16.0.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.16.0/jackson-annotations-2.16.0.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.16.0/jackson-core-2.16.0.jar:/Users/tspann/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.16.0/jackson-datatype-jsr310-2.16.0.jar:/Users/tspann/.m2/repository/com/zaxxer/HikariCP/5.1.0/HikariCP-5.1.0.jar:/Users/tspann/.m2/repository/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar:/Users/tspann/.m2/repository/org/slf4j/slf4j-api/2.0.9/slf4j-api-2.0.9.jar:/Users/tspann/.m2/repository/ch/qos/logback/logback-classic/1.4.14/logback-classic-1.4.14.jar:/Users/tspann/.m2/repository/ch/qos/logback/logback-core/1.4.14/logback-core-1.4.14.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-core/1.12.0/micrometer-core-1.12.0.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-commons/1.12.0/micrometer-commons-1.12.0.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-observation/1.12.0/micrometer-observation-1.12.0.jar:/Users/tspann/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar:/Users/tspann/.m2/repository/org/latencyutils/LatencyUtils/2.0.3/LatencyUtils-2.0.3.jar:/Users/tspann/.m2/repository/io/micrometer/micrometer-registry-jmx/1.12.0/micrometer-registry-jmx-1.12.0.jar:/Users/tspann/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.2.22/metrics-jmx-4.2.22.jar:/Users/tspann/.m2/repository/io/dropwizard/metrics/metrics-core/4.2.22/metrics-core-4.2.22.jar:/Users/tspann/.m2/repository/org/bouncycastle/bcpkix-jdk18on/1.77/bcpkix-jdk18on-1.77.jar:/Users/tspann/.m2/repository/org/bouncycastle/bcutil-jdk18on/1.77/bcutil-jdk18on-1.77.jar:/Users/tspann/.m2/repository/org/bouncycastle/bcprov-jdk18on/1.77/bcprov-jdk18on-1.77.jar:/Users/tspann/.m2/repository/com/typesafe/config/1.4.3/config-1.4.3.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter/5.10.1/junit-jupiter-5.10.1.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.10.1/junit-jupiter-api-5.10.1.jar:/Users/tspann/.m2/repository/org/opentest4j/opentest4j/1.3.0/opentest4j-1.3.0.jar:/Users/tspann/.m2/repository/org/junit/platform/junit-platform-commons/1.10.1/junit-platform-commons-1.10.1.jar:/Users/tspann/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.10.1/junit-jupiter-params-5.10.1.jar:/Users/tspann/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.10.1/junit-jupiter-engine-5.10.1.jar:/Users/tspann/.m2/repository/org/junit/platform/junit-platform-engine/1.10.1/junit-platform-engine-1.10.1.jar:/Users/tspann/.m2/repository/org/testcontainers/junit-jupiter/1.19.3/junit-jupiter-1.19.3.jar:/Users/tspann/.m2/repository/org/testcontainers/testcontainers/1.19.3/testcontainers-1.19.3.jar:/Users/tspann/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/Users/tspann/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/tspann/.m2/repository/org/apache/commons/commons-compress/1.24.0/commons-compress-1.24.0.jar:/Users/tspann/.m2/repository/org/rnorth/duct-tape/duct-tape/1.0.8/duct-tape-1.0.8.jar:/Users/tspann/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/tspann/.m2/repository/com/github/docker-java/docker-java-api/3.3.4/docker-java-api-3.3.4.jar:/Users/tspann/.m2/repository/com/github/docker-java/docker-java-transport-zerodep/3.3.4/docker-java-transport-zerodep-3.3.4.jar:/Users/tspann/.m2/repository/com/github/docker-java/docker-java-transport/3.3.4/docker-java-transport-3.3.4.jar:/Users/tspann/.m2/repository/net/java/dev/jna/jna/5.13.0/jna-5.13.0.jar:/Users/tspann/.m2/repository/org/testcontainers/kafka/1.19.3/kafka-1.19.3.jar:/Users/tspann/.m2/repository/org/mockito/mockito-core/5.8.0/mockito-core-5.8.0.jar:/Users/tspann/.m2/repository/net/bytebuddy/byte-buddy/1.14.10/byte-buddy-1.14.10.jar:/Users/tspann/.m2/repository/net/bytebuddy/byte-buddy-agent/1.14.10/byte-buddy-agent-1.14.10.jar:/Users/tspann/.m2/repository/org/objenesis/objenesis/3.3/objenesis-3.3.jar:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/Users/tspann"/>
    <property name="user.language" value="en"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.version.date" value="2025-01-21"/>
    <property name="java.home" value="/Users/tspann/.sdkman/candidates/java/21.0.6-tem"/>
    <property name="file.separator" value="/"/>
    <property name="basedir" value="/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC"/>
    <property name="java.vm.compressedOopsMode" value="Zero based"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="apple.awt.application.name" value="ForkedBooter"/>
    <property name="SNOWFLAKE_USER" value="test-user"/>
    <property name="surefire.real.class.path" value="/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC/target/surefire/surefirebooter-20251003165408133_3.jar"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="ftp.nonProxyHosts" value="local|*.local|169.254/16|*.169.254/16"/>
    <property name="java.runtime.version" value="21.0.6+7-LTS"/>
    <property name="user.name" value="tspann"/>
    <property name="stdout.encoding" value="UTF-8"/>
    <property name="path.separator" value=":"/>
    <property name="os.version" value="15.7.1"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="java.vendor.version" value="Temurin-21.0.6+7"/>
    <property name="localRepository" value="/Users/tspann/.m2/repository"/>
    <property name="SNOWFLAKE_ACCOUNT" value="test-account"/>
    <property name="java.vendor.url.bug" value="https://github.com/adoptium/adoptium-support/issues"/>
    <property name="java.io.tmpdir" value="/var/folders/r0/v5b3ymjx2kj9nf26pmtv2rcr0000gn/T/"/>
    <property name="java.version" value="21.0.6"/>
    <property name="user.dir" value="/Users/tspann/Downloads/code/cursorai/SnowflakeToKafkaCDC"/>
    <property name="os.arch" value="aarch64"/>
    <property name="SNOWFLAKE_ROLE" value="test-role"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="native.encoding" value="UTF-8"/>
    <property name="java.library.path" value="/Users/tspann/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:."/>
    <property name="java.vm.info" value="mixed mode, sharing"/>
    <property name="stderr.encoding" value="UTF-8"/>
    <property name="java.vendor" value="Eclipse Adoptium"/>
    <property name="java.vm.version" value="21.0.6+7-LTS"/>
    <property name="sun.io.unicode.encoding" value="UnicodeBig"/>
    <property name="socksNonProxyHosts" value="local|*.local|169.254/16|*.169.254/16"/>
    <property name="java.class.version" value="65.0"/>
  </properties>
  <testcase name="testKafkaContainerIsRunning" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="0.148">
    <system-out><![CDATA[16:54:11.725 [main] DEBUG org.testcontainers.utility.TestcontainersConfiguration -- Testcontainers configuration overrides will be loaded from file:/Users/tspann/.testcontainers.properties
16:54:11.727 [main] INFO org.testcontainers.images.PullPolicy -- Image pull policy will be performed by: DefaultPullPolicy()
16:54:11.728 [main] INFO org.testcontainers.utility.ImageNameSubstitutor -- Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
16:54:11.832 [main] INFO org.testcontainers.dockerclient.DockerClientProviderStrategy -- Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
16:54:11.835 [main] DEBUG org.testcontainers.dockerclient.DockerClientProviderStrategy -- Trying out strategy: UnixSocketClientProviderStrategy
16:54:11.893 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: 
16:54:11.899 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000001: preparing request execution
16:54:11.900 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:11.901 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:11.901 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000001: target auth state: UNCHALLENGED
16:54:11.901 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000001: proxy auth state: UNCHALLENGED
16:54:11.901 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000001: acquiring connection with route {}->unix://localhost:2375
16:54:11.901 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000001: acquiring endpoint (3 MINUTES)
16:54:11.902 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000001: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 0; route allocated: 0 of 2147483647; total allocated: 0 of 2147483647]
16:54:11.902 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000001: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:11.905 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000001: acquired ep-00000000
16:54:11.905 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000001: acquired endpoint ep-00000000
16:54:11.905 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000001: opening connection {}->unix://localhost:2375
16:54:11.905 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000000: connecting endpoint (3 MINUTES)
16:54:11.905 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000000: connecting endpoint to unix://localhost:2375 (3 MINUTES)
16:54:11.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-0: connecting to localhost/127.0.0.1:2375
16:54:11.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-0: connection established /var/run/docker.sock<->/var/run/docker.sock
16:54:11.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000000: connected http-outgoing-0
16:54:11.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000000: endpoint connected
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000001: executing GET /v1.32/info HTTP/1.1
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000000: start execution ex-00000001
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000000: executing exchange ex-00000001 over http-outgoing-0
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> GET /v1.32/info HTTP/1.1
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "GET /v1.32/info HTTP/1.1[\r][\n]"
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:11.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 200 OK[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/json[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:11 GMT[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:11.959 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 200 OK
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/json
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:11 GMT
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:11.960 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:11.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000001: connection can be kept alive for 3 MINUTES
16:54:11.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "90d[\r][\n]"
16:54:11.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "{"ID":"b5a38d2b-c7c5-4000-9d24-8a223b0a8ed6","Containers":0,"ContainersRunning":0,"ContainersPaused":0,"ContainersStopped":0,"Images":19,"Driver":"overlayfs","DriverStatus":[["driver-type","io.containerd.snapshotter.v1"]],"Plugins":{"Volume":["local"],"Network":["bridge","host","ipvlan","macvlan","null","overlay"],"Authorization":null,"Log":["awslogs","fluentd","gcplogs","gelf","journald","json-file","local","splunk","syslog"]},"MemoryLimit":true,"SwapLimit":true,"CpuCfsPeriod":true,"CpuCfsQuota":true,"CPUShares":true,"CPUSet":true,"PidsLimit":true,"IPv4Forwarding":true,"Debug":false,"NFd":42,"OomKillDisable":false,"NGoroutines":101,"SystemTime":"2025-10-03T20:54:11.930997675Z","LoggingDriver":"json-file","CgroupDriver":"cgroupfs","CgroupVersion":"2","NEventsListener":12,"KernelVersion":"6.10.14-linuxkit","OperatingSystem":"Docker Desktop","OSVersion":"","OSType":"linux","Architecture":"aarch64","IndexServerAddress":"https://index.docker.io/v1/","RegistryConfig":{"IndexConfigs":{"docker.io":{"Mirrors":[],"Name":"docker.io","Official":true,"Secure":true},"hubproxy.docker.internal:5555":{"Mirrors":[],"Name":"hubproxy.docker.internal:5555","Official":false,"Secure":false}},"InsecureRegistryCIDRs":["::1/128","127.0.0.0/8"],"Mirrors":null},"NCPU":8,"MemTotal":8218316800,"GenericResources":null,"DockerRootDir":"/var/lib/docker","HttpProxy":"http.docker.internal:3128","HttpsProxy":"http.docker.internal:3128","NoProxy":"hubproxy.docker.internal","Name":"docker-desktop","Labels":["com.docker.desktop.address=unix:///Users/tspann/Library/Containers/com.docker.docker/Data/docker-cli.sock"],"ExperimentalBuild":false,"ServerVersion":"28.4.0","Runtimes":{"io.containerd.runc.v2":{"path":"runc"},"runc":{"path":"runc"}},"DefaultRuntime":"runc","Swarm":{"NodeID":"","NodeAddr":"","LocalNodeState":"inactive","ControlAvailable":false,"Error":"","RemoteManagers":null},"LiveRestoreEnabled":false,"Isolation":"","InitBinary":"docker-init","ContainerdCommit":{"ID":"05044ec0a9a75232cad458027ca83437aae3f4da","Expected":"05044ec0a9a75232cad458027ca83437aae3f4da"},"RuncCommit":{"ID":"v1.2.5-0-g59923ef","Expected":"v1.2.5-0-g59923ef"},"InitCommit":{"ID":"de40ad0","Expected":"de40ad0"},"SecurityOptions":["name=seccomp,profile=builtin","name=cgroupns"],"CDISpecDirs":["/etc/cdi","/var/run/cdi"],"Warnings":null}[\r][\n]"
16:54:11.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "0[\r][\n]"
16:54:11.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.014 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000000: releasing valid endpoint
16:54:12.014 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000000: releasing endpoint
16:54:12.014 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000000: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.014 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000000: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.014 [main] INFO org.testcontainers.dockerclient.DockerClientProviderStrategy -- Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
16:54:12.014 [main] DEBUG org.testcontainers.dockerclient.DockerClientProviderStrategy -- Transport type: 'httpclient5', Docker host: 'unix:///var/run/docker.sock'
16:54:12.014 [main] DEBUG org.testcontainers.dockerclient.DockerClientProviderStrategy -- Checking Docker OS type for local Unix socket (unix:///var/run/docker.sock)
16:54:12.015 [main] INFO org.testcontainers.DockerClientFactory -- Docker host IP address is localhost
16:54:12.015 [main] DEBUG org.testcontainers.DockerClientFactory -- Docker info: {CDISpecDirs=[/etc/cdi, /var/run/cdi], NGoroutines=101, Name=docker-desktop, Swarm={NodeID=, NodeAddr=, LocalNodeState=inactive, ControlAvailable=false, Error=, RemoteManagers=null}, RuncCommit={ID=v1.2.5-0-g59923ef, Expected=v1.2.5-0-g59923ef}, OSVersion=, Runtimes={io.containerd.runc.v2={path=runc}, runc={path=runc}}, ID=b5a38d2b-c7c5-4000-9d24-8a223b0a8ed6, DriverStatus=[[driver-type, io.containerd.snapshotter.v1]], LiveRestoreEnabled=false, SystemTime=2025-10-03T20:54:11.930997675Z, NoProxy=hubproxy.docker.internal, Architecture=aarch64, NEventsListener=12, HttpsProxy=http.docker.internal:3128, Images=19, ContainersRunning=0, Labels=[com.docker.desktop.address=unix:///Users/tspann/Library/Containers/com.docker.docker/Data/docker-cli.sock], InitCommit={ID=de40ad0, Expected=de40ad0}, NFd=42, KernelVersion=6.10.14-linuxkit, LoggingDriver=json-file, OSType=linux, SecurityOptions=[name=seccomp,profile=builtin, name=cgroupns], ContainerdCommit={ID=05044ec0a9a75232cad458027ca83437aae3f4da, Expected=05044ec0a9a75232cad458027ca83437aae3f4da}, GenericResources=null, OperatingSystem=Docker Desktop, RegistryConfig={IndexConfigs={docker.io={Mirrors=[], Name=docker.io, Official=true, Secure=true}, hubproxy.docker.internal:5555={Mirrors=[], Name=hubproxy.docker.internal:5555, Official=false, Secure=false}}, InsecureRegistryCIDRs=[::1/128, 127.0.0.0/8], Mirrors=null}, ServerVersion=28.4.0, OomKillDisable=false, CpuCfsQuota=true, Containers=0, Driver=overlayfs, IPv4Forwarding=true, ExperimentalBuild=false, DockerRootDir=/var/lib/docker, CPUSet=true, CpuCfsPeriod=true, DefaultRuntime=runc, CPUShares=true, Debug=false, NCPU=8, Plugins={Volume=[local], Network=[bridge, host, ipvlan, macvlan, null, overlay], Authorization=null, Log=[awslogs, fluentd, gcplogs, gelf, journald, json-file, local, splunk, syslog]}, SwapLimit=true, CgroupDriver=cgroupfs, ContainersPaused=0, MemTotal=8218316800, IndexServerAddress=https://index.docker.io/v1/, MemoryLimit=true, HttpProxy=http.docker.internal:3128, InitBinary=docker-init, CgroupVersion=2, PidsLimit=true, Isolation=, Warnings=null, ContainersStopped=0}
16:54:12.016 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: 
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000002: preparing request execution
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000002: target auth state: UNCHALLENGED
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000002: proxy auth state: UNCHALLENGED
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000002: acquiring connection with route {}->unix://localhost:2375
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000002: acquiring endpoint (3 MINUTES)
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000002: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000002: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000002: acquired ep-00000001
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000002: acquired endpoint ep-00000001
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000002: executing GET /v1.32/version HTTP/1.1
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000001: start execution ex-00000002
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000001: executing exchange ex-00000002 over http-outgoing-0
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> GET /v1.32/version HTTP/1.1
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> accept: application/json
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "GET /v1.32/version HTTP/1.1[\r][\n]"
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "accept: application/json[\r][\n]"
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.016 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/json[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 200 OK
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/json
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000002: connection can be kept alive for 3 MINUTES
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "339[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "{"Platform":{"Name":"Docker Desktop 4.47.0 (206054)"},"Components":[{"Name":"Engine","Version":"28.4.0","Details":{"ApiVersion":"1.51","Arch":"arm64","BuildTime":"2025-09-03T20:58:53.000000000+00:00","Experimental":"false","GitCommit":"249d679","GoVersion":"go1.24.7","KernelVersion":"6.10.14-linuxkit","MinAPIVersion":"1.24","Os":"linux"}},{"Name":"containerd","Version":"1.7.27","Details":{"GitCommit":"05044ec0a9a75232cad458027ca83437aae3f4da"}},{"Name":"runc","Version":"1.2.5","Details":{"GitCommit":"v1.2.5-0-g59923ef"}},{"Name":"docker-init","Version":"0.19.0","Details":{"GitCommit":"de40ad0"}}],"Version":"28.4.0","ApiVersion":"1.51","MinAPIVersion":"1.24","GitCommit":"249d679","GoVersion":"go1.24.7","Os":"linux","Arch":"arm64","KernelVersion":"6.10.14-linuxkit","BuildTime":"2025-09-03T20:58:53.000000000+00:00"}[\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "0[\r][\n]"
16:54:12.024 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.027 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000001: releasing valid endpoint
16:54:12.027 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000001: releasing endpoint
16:54:12.027 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000001: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.027 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000001: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.027 [main] DEBUG org.testcontainers.DockerClientFactory -- Docker version: {Components=[{Name=Engine, Version=28.4.0, Details={ApiVersion=1.51, Arch=arm64, BuildTime=2025-09-03T20:58:53.000000000+00:00, Experimental=false, GitCommit=249d679, GoVersion=go1.24.7, KernelVersion=6.10.14-linuxkit, MinAPIVersion=1.24, Os=linux}}, {Name=containerd, Version=1.7.27, Details={GitCommit=05044ec0a9a75232cad458027ca83437aae3f4da}}, {Name=runc, Version=1.2.5, Details={GitCommit=v1.2.5-0-g59923ef}}, {Name=docker-init, Version=0.19.0, Details={GitCommit=de40ad0}}], KernelVersion=6.10.14-linuxkit, ApiVersion=1.51, Platform={Name=Docker Desktop 4.47.0 (206054)}, GitCommit=249d679, Os=linux, Version=28.4.0, MinAPIVersion=1.24, Arch=arm64, GoVersion=go1.24.7, BuildTime=2025-09-03T20:58:53.000000000+00:00}
16:54:12.027 [main] INFO org.testcontainers.DockerClientFactory -- Connected to docker: 
  Server Version: 28.4.0
  API Version: 1.51
  Operating System: Docker Desktop
  Total Memory: 7837 MB
16:54:12.030 [main] DEBUG org.testcontainers.utility.RyukResourceReaper -- Ryuk is enabled
16:54:12.031 [main] DEBUG org.testcontainers.utility.PrefixingImageNameSubstitutor -- No prefix is configured
16:54:12.031 [main] DEBUG org.testcontainers.utility.ImageNameSubstitutor -- Did not find a substitute image for testcontainers/ryuk:0.5.1 (using image substitutor: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor'))
16:54:12.031 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: ListImagesCmdImpl[filters=org.testcontainers.shaded.com.github.dockerjava.core.util.FiltersBuilder@0,imageNameFilter=<null>,showAll=false]
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000003: preparing request execution
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000003: target auth state: UNCHALLENGED
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000003: proxy auth state: UNCHALLENGED
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000003: acquiring connection with route {}->unix://localhost:2375
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000003: acquiring endpoint (3 MINUTES)
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000003: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000003: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000003: acquired ep-00000002
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000003: acquired endpoint ep-00000002
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000003: executing GET /v1.32/images/json HTTP/1.1
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000002: start execution ex-00000003
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000002: executing exchange ex-00000003 over http-outgoing-0
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> GET /v1.32/images/json HTTP/1.1
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> accept: application/json
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "GET /v1.32/images/json HTTP/1.1[\r][\n]"
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "accept: application/json[\r][\n]"
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.033 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/json[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 200 OK
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/json
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000003: connection can be kept alive for 3 MINUTES
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "2571[\r][\n]"
16:54:12.147 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[{"Containers":-1,"Created":1759524613,"Id":"sha256:97a868e6709dfeeee9830897c7ff4b21863de6fa498a10fef434614ee7af445b","Labels":{"description":"High-performance Snowflake to Kafka CDC streaming application","maintainer":"Snowflake Kafka CDC Team","org.opencontainers.image.source":"https://github.com/your-org/snowflake-kafka-cdc","version":"1.0.0"},"ParentId":"","RepoDigests":["snowflake-kafka-cdc@sha256:97a868e6709dfeeee9830897c7ff4b21863de6fa498a10fef434614ee7af445b"],"RepoTags":["snowflake-kafka-cdc:jdk21-test"],"SharedSize":-1,"Size":659745520,"VirtualSize":659745520},{"Containers":-1,"Created":1755112746,"Id":"sha256:8c6701d81deff2188615e72f342df6b2be032e3c59c4036c85ac8a13fb64afb7","Labels":{"com.docker.compose.project":"mta","com.docker.compose.service":"snowpipe-streaming","com.docker.compose.version":"2.39.1"},"ParentId":"","RepoDigests":["mta-snowpipe-streaming@sha256:8c6701d81deff2188615e72f342df6b2be032e3c59c4036c85ac8a13fb64afb7"],"RepoTags":["mta-snowpipe-streaming:latest"],"SharedSize":-1,"Size":1313799486,"VirtualSize":1313799486},{"Containers":-1,"Created":1755112733,"Id":"sha256:647ee3e1be95d23c89ac2962354628311a9c59f0dd9320e2fb07b43ff3c43460","Labels":{"com.docker.compose.project":"mta","com.docker.compose.service":"dashboard","com.docker.compose.version":"2.39.1"},"ParentId":"","RepoDigests":["mta-dashboard@sha256:647ee3e1be95d23c89ac2962354628311a9c59f0dd9320e2fb07b43ff3c43460"],"RepoTags":["mta-dashboard:latest"],"SharedSize":-1,"Size":848627349,"VirtualSize":848627349},{"Containers":-1,"Created":1754579559,"Id":"sha256:313d5ae5373759b7bee6b29caa2d4a35e407292a6ccaa55be570ea55c8af4d06","Labels":{"build.timestamp":"","description":"High-performance MTA Snowpipe Streaming v2 application with SDK 4.1.0","git.commit":"","java.version":"21","maintainer":"MTA Streaming Team","snowflake.sdk.version":"4.1.0","streaming.version":"v2","version":"2.0.0"},"ParentId":"","RepoDigests":["mta-mta-streaming-java@sha256:313d5ae5373759b7bee6b29caa2d4a35e407292a6ccaa55be570ea55c8af4d06"],"RepoTags":["mta-mta-streaming-java:latest"],"SharedSize":-1,"Size":914592888,"VirtualSize":914592888},{"Containers":-1,"Created":1754427231,"Id":"sha256:914c79f4d88be7bcd3af5508e5a6474897c04e0c0e84158b231b9eda4e99b4f9","Labels":null,"ParentId":"","RepoDigests":["mta-dashboard-java@sha256:914c79f4d88be7bcd3af5508e5a6474897c04e0c0e84158b231b9eda4e99b4f9"],"RepoTags":["mta-dashboard-java:latest"],"SharedSize":-1,"Size":848464739,"VirtualSize":848464739},{"Containers":-1,"Created":1753922351,"Id":"sha256:0b0182abace9debdaa58ad3f51a40cc00c0feacc9bdf66504eabcfb9f4b0882c","Labels":null,"ParentId":"","RepoDigests":["airquality-dashboard@sha256:0b0182abace9debdaa58ad3f51a40cc00c0feacc9bdf66504eabcfb9f4b0882c"],"RepoTags":["airquality-dashboard:latest"],"SharedSize":-1,"Size":1431109049,"VirtualSize":1431109049},{"Containers":-1,"Created":1753922289,"Id":"sha256:6a4edc3183d1131cb884bdf6132ddbc1d78657a13b68e4f07e41456c0130e874","Labels":{"maintainer":"Grafana Labs \u003chello@grafana.com\u003e","org.opencontainers.image.source":"https://github.com/grafana/grafana"},"ParentId":"","RepoDigests":["grafana/grafana@sha256:6a4edc3183d1131cb884bdf6132ddbc1d78657a13b68e4f07e41456c0130e874"],"RepoTags":["grafana/grafana:latest"],"SharedSize":-1,"Size":910342767,"VirtualSize":910342767},{"Containers":-1,"Created":1753906228,"Id":"sha256:8dd4aba4411b8c6148294f7b5d304d63889feeec3f47ba12157561ed98189abf","Labels":{"architecture":"aarch64","build-date":"2025-05-15T14:25:56","com.redhat.component":"ubi9-micro-container","com.redhat.license_terms":"https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI","description":"Very small image which doesn't install the package manager.","distribution-scope":"public","io.buildah.version":"1.39.0-dev","io.k8s.description":"Very small image which doesn't install the package manager.","io.k8s.display-name":"Red Hat Universal Base Image 9 Micro","io.openshift.expose-services":"","maintainer":"Red Hat, Inc.","name":"ubi9/ubi-micro","release":"1747318857","summary":"ubi9 micro image","url":"https://www.redhat.com","vcs-ref":"1594a07d6a09f2cbf4e16dd944306eca07749a2a","vcs-type":"git","vendor":"Red Hat, Inc.","version":"9.6"},"ParentId":"","RepoDigests":["airquality-trino@sha256:8dd4aba4411b8c6148294f7b5d304d63889feeec3f47ba12157561ed98189abf"],"RepoTags":["airquality-trino:latest"],"SharedSize":-1,"Size":2364959916,"VirtualSize":2364959916},{"Containers":-1,"Created":1752511597,"Id":"sha256:63805ebb8d2b3920190daf1cb14a60871b16fd38bed42b857a3182bc621f4996","Labels":{"maintainer":"The Prometheus Authors \u003cprometheus-developers@googlegroups.com\u003e","org.opencontainers.image.source":"https://github.com/prometheus/prometheus"},"ParentId":"","RepoDigests":["prom/prometheus@sha256:63805ebb8d2b3920190daf1cb14a60871b16fd38bed42b857a3182bc621f4996"],"RepoTags":["prom/prometheus:latest"],"SharedSize":-1,"Size":423337409,"VirtualSize":423337409},{"Containers":-1,"Created":1751820718,"Id":"sha256:bb186d083732f669da90be8b0f975a37812b15e913465bb14d845db72a4e3e08","Labels":null,"ParentId":"","RepoDigests":["redis@sha256:bb186d083732f669da90be8b0f975a37812b15e913465bb14d845db72a4e3e08"],"RepoTags":["redis:7-alpine"],"SharedSize":-1,"Size":61426498,"VirtualSize":61426498},{"Containers":-1,"Created":1750798334,"Id":"sha256:d67ea0d64d518b1bb04acde3b00f722ac3e9764b3209a9b0a98924ba35e4b779","Labels":{"maintainer":"NGINX Docker Maintainers \u003cdocker-maint@nginx.com\u003e"},"ParentId":"","RepoDigests":["nginx@sha256:d67ea0d64d518b1bb04acde3b00f722ac3e9764b3209a9b0a98924ba35e4b779"],"RepoTags":["nginx:alpine"],"SharedSize":-1,"Size":80133088,"VirtualSize":80133088},{"Containers":-1,"Created":1741360233,"Id":"sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba","Labels":{"maintainer":"The Prometheus Authors \u003cprometheus-developers@googlegroups.com\u003e","org.opencontainers.image.source":"https://github.com/prometheus/alertmanager"},"ParentId":"","RepoDigests":["prom/alertmanager@sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba"],"RepoTags":["prom/alertmanager:latest"],"SharedSize":-1,"Size":106469192,"VirtualSize":106469192},{"Containers":-1,"Created":1727891735,"Id":"sha256:9ebd266ba0ddab1bf6c0f7580bb53f373bf5ac137ec02430169d20c4c139c70b","Labels":{"maintainer":"NGINX Docker Maintainers \u003cdocker-maint@nginx.com\u003e"},"ParentId":"","RepoDigests":["sfsenorthamerica.tspann_aws1.registry.snowflakecomputing.com/openflow/openflow/openflow/nginx@sha256:9ebd266ba0ddab1bf6c0f7580bb53f373bf5ac137ec02430169d20c4c139c70b"],"RepoTags":["SFSENORTHAMERICA-TSPANN-AWS1.registry.snowflakecomputing.com/openflow/registry/openflow/nginx:1.27.2-alpine3.20-slim-amd64","nginx:1.27.2-alpine3.20-slim","sfsenorthamerica-tspann-aws1.registry.snowflakecomputing.com/openflow/registry/openflow/nginx:1.27.2-alpine3.20-slim-amd64","sfsenorthamerica.tspann_aws1.registry.snowflakecomputing.com/openflow/openflow/openflow/nginx:1.27.2-alpine3.20-slim-amd64"],"SharedSize":-1,"Size":27036185,"VirtualSize":27036185},{"Containers":-1,"Created":1726223069,"Id":"sha256:5c63ed17a0c87a3c7c254ecdf0653b364c718bba6d33c234b3a84d235868a9ca","Labels":{"Description":"Fluentd docker image","Vendor":"Fluent Organization","Version":"1.17.1","maintainer":"Fluentd developers \u003cfluentd@googlegroups.com\u003e"},"ParentId":"","RepoDigests":["fluent/fluentd@sha256:5c63ed17a0c87a3c7c254ecdf0653b364c718bba6d33c234b3a84d235868a9ca"],"RepoTags":["fluent/fluentd:v1.16-debian-1"],"SharedSize":-1,"Size":345740835,"VirtualSize":345740835},{"Containers":-1,"Created":1685581650,"Id":"sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63","Labels":{"architecture":"aarch64","build-date":"2023-05-03T15:02:09","com.redhat.component":"ubi8-minimal-container","com.redhat.license_terms":"https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI","description":"Common base image for Confluent's Docker images.","distribution-scope":"public","io.buildah.version":"1.27.3","io.confluent.docker":"true","io.confluent.docker.build.number":"5","io.confluent.docker.git.id":"d0965a30f","io.confluent.docker.git.repo":"confluentinc/kafka-im"
16:54:12.150 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "ages","io.k8s.description":"The Universal Base Image Minimal is a stripped down image that uses microdnf as a package manager. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.","io.k8s.display-name":"Red Hat Universal Base Image 8 Minimal","io.openshift.expose-services":"","io.openshift.tags":"minimal rhel8","maintainer":"partner-support@confluent.io","name":"cp-kafka","release":"7.4.0","summary":"Confluent platform Kafka.","url":"https://access.redhat.com/containers/#/registry.access.redhat.com/ubi8-minimal/images/8.8-860","vcs-ref":"dee8029ddcc7ecbfbebb0905d2b15e134338616c","vcs-type":"git","vendor":"Confluent","version":"d0965a30f"},"ParentId":"","RepoDigests":["confluentinc/cp-kafka@sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63"],"RepoTags":["confluentinc/cp-kafka:7.4.0"],"SharedSize":-1,"Size":1337223096,"VirtualSize":1337223096},{"Containers":-1,"Created":1684336517,"Id":"sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775","Labels":{"org.testcontainers.ryuk":"true"},"ParentId":"","RepoDigests":["testcontainers/ryuk@sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775"],"RepoTags":["testcontainers/ryuk:0.5.1"],"SharedSize":-1,"Size":19273672,"VirtualSize":19273672}]"
16:54:12.151 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.151 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "0[\r][\n]"
16:54:12.151 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.151 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000002: releasing valid endpoint
16:54:12.151 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000002: releasing endpoint
16:54:12.151 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000002: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.151 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000002: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.152 [main] DEBUG org.testcontainers.images.AbstractImagePullPolicy -- Using locally available and not pulling image: testcontainers/ryuk:0.5.1
16:54:12.152 [main] DEBUG tc.testcontainers/ryuk:0.5.1 -- Starting container: testcontainers/ryuk:0.5.1
16:54:12.152 [main] DEBUG tc.testcontainers/ryuk:0.5.1 -- Trying to start container: testcontainers/ryuk:0.5.1 (attempt 1/1)
16:54:12.152 [main] DEBUG tc.testcontainers/ryuk:0.5.1 -- Starting container: testcontainers/ryuk:0.5.1
16:54:12.152 [main] INFO tc.testcontainers/ryuk:0.5.1 -- Creating container for image: testcontainers/ryuk:0.5.1
16:54:12.154 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- Looking up auth config for image: testcontainers/ryuk:0.5.1 at registry: https://index.docker.io/v1/
16:54:12.154 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- RegistryAuthLocator has configFile: /Users/tspann/.docker/config.json (exists) configEnv: DOCKER_AUTH_CONFIG (does not exist) and commandPathPrefix: 
16:54:12.154 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- RegistryAuthLocator reading from configFile: /Users/tspann/.docker/config.json
16:54:12.154 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- registryName [https://index.docker.io/v1/] for dockerImageName [testcontainers/ryuk:0.5.1]
16:54:12.154 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- Executing docker credential provider: docker-credential-desktop to locate auth config for: https://index.docker.io/v1/
16:54:12.157 [main] DEBUG org.testcontainers.shaded.org.zeroturnaround.exec.ProcessExecutor -- Executing [docker-credential-desktop, get].
16:54:12.167 [main] DEBUG org.testcontainers.shaded.org.zeroturnaround.exec.ProcessExecutor -- Started Process[pid=64286, exitValue="not exited"]
16:54:12.316 [WaitForProcess-Process[pid=64286, exitValue="not exited"]] DEBUG org.testcontainers.shaded.org.zeroturnaround.exec.WaitForProcess -- Process[pid=64286, exitValue=0] stopped with exit code 0
16:54:12.317 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- Credential helper/store provided auth config for: https://index.docker.io/v1/
16:54:12.318 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- found creds store auth config [AuthConfig{username=timspann623, password=hidden non-blank value, auth=blank, email=null, registryAddress=https://index.docker.io/v1/, registryToken=blank}]
16:54:12.318 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- Cached auth found: [AuthConfig{username=timspann623, password=hidden non-blank value, auth=blank, email=null, registryAddress=https://index.docker.io/v1/, registryToken=blank}]
16:54:12.318 [main] DEBUG org.testcontainers.dockerclient.AuthDelegatingDockerClientConfig -- Effective auth config [AuthConfig{username=timspann623, password=hidden non-blank value, auth=blank, email=null, registryAddress=https://index.docker.io/v1/, registryToken=blank}]
16:54:12.321 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: org.testcontainers.shaded.com.github.dockerjava.core.command.CreateContainerCmdImpl@17740dae[aliases=<null>,argsEscaped=<null>,attachStderr=<null>,attachStdin=<null>,attachStdout=<null>,authConfig=AuthConfig(username=timspann623, email=null, registryAddress=https://index.docker.io/v1/, stackOrchestrator=null),cmd={},domainName=<null>,entrypoint=<null>,env={},exposedPorts=ExposedPorts(exposedPorts=[8080/tcp]),healthcheck=<null>,hostConfig=HostConfig(binds=[/var/run/docker.sock:/var/run/docker.sock:rw], blkioWeight=null, blkioWeightDevice=null, blkioDeviceReadBps=null, blkioDeviceWriteBps=null, blkioDeviceReadIOps=null, blkioDeviceWriteIOps=null, memorySwappiness=null, nanoCPUs=null, capAdd=null, capDrop=null, containerIDFile=null, cpuPeriod=null, cpuRealtimePeriod=null, cpuRealtimeRuntime=null, cpuShares=null, cpuQuota=null, cpusetCpus=null, cpusetMems=null, devices=null, deviceCgroupRules=null, deviceRequests=null, diskQuota=null, dns=null, dnsOptions=null, dnsSearch=null, extraHosts=[], groupAdd=null, ipcMode=null, cgroup=null, links=[], logConfig=LogConfig(type=null, config=null), lxcConf=null, memory=null, memorySwap=null, memoryReservation=null, kernelMemory=null, networkMode=null, oomKillDisable=null, init=null, autoRemove=true, oomScoreAdj=null, portBindings={8080/tcp=[Lcom.github.dockerjava.api.model.Ports$Binding;@3cee53dc}, privileged=true, publishAllPorts=null, readonlyRootfs=null, restartPolicy=null, ulimits=null, cpuCount=null, cpuPercent=null, ioMaximumIOps=null, ioMaximumBandwidth=null, volumesFrom=[], mounts=null, pidMode=null, isolation=null, securityOpts=null, storageOpt=null, cgroupParent=null, volumeDriver=null, shmSize=null, pidsLimit=null, runtime=null, tmpFs=null, utSMode=null, usernsMode=null, sysctls=null, consoleSize=null, cgroupnsMode=null),hostName=<null>,image=testcontainers/ryuk:0.5.1,ipv4Address=<null>,ipv6Address=<null>,labels={org.testcontainers=true, org.testcontainers.lang=java, org.testcontainers.version=1.19.3},macAddress=<null>,name=testcontainers-ryuk-41781793-441b-4be0-9f6b-8274f7098da1,networkDisabled=<null>,networkingConfig=<null>,onBuild=<null>,platform=<null>,portSpecs=<null>,shell=<null>,stdInOnce=<null>,stdinOpen=<null>,stopSignal=<null>,stopTimeout=<null>,tty=<null>,user=<null>,volumes=Volumes(volumes=[]),workingDir=<null>]
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000004: preparing request execution
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000004: target auth state: UNCHALLENGED
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000004: proxy auth state: UNCHALLENGED
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000004: acquiring connection with route {}->unix://localhost:2375
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000004: acquiring endpoint (3 MINUTES)
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000004: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000004: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000004: acquired ep-00000003
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000004: acquired endpoint ep-00000003
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000004: executing POST /v1.32/containers/create?name=testcontainers-ryuk-41781793-441b-4be0-9f6b-8274f7098da1 HTTP/1.1
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000003: start execution ex-00000004
16:54:12.349 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000003: executing exchange ex-00000004 over http-outgoing-0
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> POST /v1.32/containers/create?name=testcontainers-ryuk-41781793-441b-4be0-9f6b-8274f7098da1 HTTP/1.1
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> X-Registry-Auth: eyJ1c2VybmFtZSI6InRpbXNwYW5uNjIzIiwicGFzc3dvcmQiOiJkY2tyX3BhdF9iQnJqVFVQSTktRjN1SmtZMGVLQzY0bGxxS0kiLCJlbWFpbCI6bnVsbCwic2VydmVyYWRkcmVzcyI6Imh0dHBzOi8vaW5kZXguZG9ja2VyLmlvL3YxLyIsImF1dGgiOm51bGwsInJlZ2lzdHJ5dG9rZW4iOm51bGwsImlkZW50aXR5dG9rZW4iOm51bGwsInN0YWNrT3JjaGVzdHJhdG9yIjpudWxsfQ==
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> accept: application/json
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> content-type: application/json
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Content-Length: 1954
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "POST /v1.32/containers/create?name=testcontainers-ryuk-41781793-441b-4be0-9f6b-8274f7098da1 HTTP/1.1[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "X-Registry-Auth: eyJ1c2VybmFtZSI6InRpbXNwYW5uNjIzIiwicGFzc3dvcmQiOiJkY2tyX3BhdF9iQnJqVFVQSTktRjN1SmtZMGVLQzY0bGxxS0kiLCJlbWFpbCI6bnVsbCwic2VydmVyYWRkcmVzcyI6Imh0dHBzOi8vaW5kZXguZG9ja2VyLmlvL3YxLyIsImF1dGgiOm51bGwsInJlZ2lzdHJ5dG9rZW4iOm51bGwsImlkZW50aXR5dG9rZW4iOm51bGwsInN0YWNrT3JjaGVzdHJhdG9yIjpudWxsfQ==[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "accept: application/json[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "content-type: application/json[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Content-Length: 1954[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.350 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "{"Hostname":null,"Domainname":null,"User":null,"AttachStdin":null,"AttachStdout":null,"AttachStderr":null,"PortSpecs":null,"Tty":null,"OpenStdin":null,"StdinOnce":null,"Env":[],"Cmd":[],"Healthcheck":null,"ArgsEscaped":null,"Entrypoint":null,"Image":"testcontainers/ryuk:0.5.1","Volumes":{},"WorkingDir":null,"MacAddress":null,"OnBuild":null,"NetworkDisabled":null,"ExposedPorts":{"8080/tcp":{}},"StopSignal":null,"StopTimeout":null,"HostConfig":{"Binds":["/var/run/docker.sock:/var/run/docker.sock:rw"],"BlkioWeight":null,"BlkioWeightDevice":null,"BlkioDeviceReadBps":null,"BlkioDeviceWriteBps":null,"BlkioDeviceReadIOps":null,"BlkioDeviceWriteIOps":null,"MemorySwappiness":null,"NanoCpus":null,"CapAdd":null,"CapDrop":null,"ContainerIDFile":null,"CpuPeriod":null,"CpuRealtimePeriod":null,"CpuRealtimeRuntime":null,"CpuShares":null,"CpuQuota":null,"CpusetCpus":null,"CpusetMems":null,"Devices":null,"DeviceCgroupRules":null,"DeviceRequests":null,"DiskQuota":null,"Dns":null,"DnsOptions":null,"DnsSearch":null,"ExtraHosts":[],"GroupAdd":null,"IpcMode":null,"Cgroup":null,"Links":[],"LogConfig":null,"LxcConf":null,"Memory":null,"MemorySwap":null,"MemoryReservation":null,"KernelMemory":null,"NetworkMode":null,"OomKillDisable":null,"Init":null,"AutoRemove":true,"OomScoreAdj":null,"PortBindings":{"8080/tcp":[{"HostIp":"","HostPort":""}]},"Privileged":true,"PublishAllPorts":null,"ReadonlyRootfs":null,"RestartPolicy":null,"Ulimits":null,"CpuCount":null,"CpuPercent":null,"IOMaximumIOps":null,"IOMaximumBandwidth":null,"VolumesFrom":[],"Mounts":null,"PidMode":null,"Isolation":null,"SecurityOpt":null,"StorageOpt":null,"CgroupParent":null,"VolumeDriver":null,"ShmSize":null,"PidsLimit":null,"Runtime":null,"Tmpfs":null,"UTSMode":null,"UsernsMode":null,"Sysctls":null,"ConsoleSize":null,"CgroupnsMode":null},"Labels":{"org.testcontainers":"true","org.testcontainers.lang":"java","org.testcontainers.version":"1.19.3"},"Shell":null,"NetworkingConfig":null}"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 201 Created[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/json[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "58[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "{"Id":"ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654","Warnings":[]}[\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 201 Created
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/json
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:12.409 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000004: connection can be kept alive for 3 MINUTES
16:54:12.411 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "0[\r][\n]"
16:54:12.411 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.411 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000003: releasing valid endpoint
16:54:12.411 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000003: releasing endpoint
16:54:12.411 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000003: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.411 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000003: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.412 [main] INFO tc.testcontainers/ryuk:0.5.1 -- Container testcontainers/ryuk:0.5.1 is starting: ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654
16:54:12.412 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000005: preparing request execution
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000005: target auth state: UNCHALLENGED
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000005: proxy auth state: UNCHALLENGED
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000005: acquiring connection with route {}->unix://localhost:2375
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000005: acquiring endpoint (3 MINUTES)
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000005: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000005: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000005: acquired ep-00000004
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000005: acquired endpoint ep-00000004
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000005: executing POST /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/start HTTP/1.1
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000004: start execution ex-00000005
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000004: executing exchange ex-00000005 over http-outgoing-0
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> POST /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/start HTTP/1.1
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> accept: application/json
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> content-type: application/json
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "POST /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/start HTTP/1.1[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "accept: application/json[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "content-type: application/json[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.413 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 204 No Content[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 204 No Content
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000005: connection can be kept alive for 3 MINUTES
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000004: releasing valid endpoint
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000004: releasing endpoint
16:54:12.565 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000004: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.566 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000004: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.570 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654,false
16:54:12.571 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.exec.InspectContainerCmdExec -- GET: DefaultWebTarget{path=[/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/json], queryParams={}}
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000006: preparing request execution
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000006: target auth state: UNCHALLENGED
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000006: proxy auth state: UNCHALLENGED
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000006: acquiring connection with route {}->unix://localhost:2375
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000006: acquiring endpoint (3 MINUTES)
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000006: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000006: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000006: acquired ep-00000005
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000006: acquired endpoint ep-00000005
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000006: executing GET /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/json HTTP/1.1
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000005: start execution ex-00000006
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000005: executing exchange ex-00000006 over http-outgoing-0
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> GET /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/json HTTP/1.1
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> accept: application/json
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "GET /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/json HTTP/1.1[\r][\n]"
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "accept: application/json[\r][\n]"
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.571 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/json[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "10d3[\r][\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "{"Id":"ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654","Created":"2025-10-03T20:54:12.373082884Z","Path":"/bin/ryuk","Args":[],"State":{"Status":"running","Running":true,"Paused":false,"Restarting":false,"OOMKilled":false,"Dead":false,"Pid":3361,"ExitCode":0,"Error":"","StartedAt":"2025-10-03T20:54:12.436068134Z","FinishedAt":"0001-01-01T00:00:00Z"},"Image":"sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775","ResolvConfPath":"/var/lib/docker/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/resolv.conf","HostnamePath":"/var/lib/docker/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/hostname","HostsPath":"/var/lib/docker/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/hosts","LogPath":"/var/lib/docker/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654-json.log","Name":"/testcontainers-ryuk-41781793-441b-4be0-9f6b-8274f7098da1","RestartCount":0,"Driver":"overlayfs","Platform":"linux","MountLabel":"","ProcessLabel":"","AppArmorProfile":"","ExecIDs":null,"HostConfig":{"Binds":["/var/run/docker.sock:/var/run/docker.sock:rw"],"ContainerIDFile":"","LogConfig":{"Type":"json-file","Config":{}},"NetworkMode":"bridge","PortBindings":{"8080/tcp":[{"HostIp":"","HostPort":""}]},"RestartPolicy":{"Name":"no","MaximumRetryCount":0},"AutoRemove":true,"VolumeDriver":"","VolumesFrom":[],"ConsoleSize":[0,0],"CapAdd":null,"CapDrop":null,"CgroupnsMode":"private","Dns":null,"DnsOptions":null,"DnsSearch":null,"ExtraHosts":[],"GroupAdd":null,"IpcMode":"shareable","Cgroup":"","Links":null,"OomScoreAdj":0,"PidMode":"","Privileged":true,"PublishAllPorts":false,"ReadonlyRootfs":false,"SecurityOpt":["label=disable"],"UTSMode":"","UsernsMode":"","ShmSize":67108864,"Runtime":"runc","Isolation":"","CpuShares":0,"Memory":0,"NanoCpus":0,"CgroupParent":"","BlkioWeight":0,"BlkioWeightDevice":null,"BlkioDeviceReadBps":null,"BlkioDeviceWriteBps":null,"BlkioDeviceReadIOps":null,"BlkioDeviceWriteIOps":null,"CpuPeriod":0,"CpuQuota":0,"CpuRealtimePeriod":0,"CpuRealtimeRuntime":0,"CpusetCpus":"","CpusetMems":"","Devices":null,"DeviceCgroupRules":null,"DeviceRequests":null,"MemoryReservation":0,"MemorySwap":0,"MemorySwappiness":null,"OomKillDisable":null,"PidsLimit":null,"Ulimits":null,"CpuCount":0,"CpuPercent":0,"IOMaximumIOps":0,"IOMaximumBandwidth":0,"MaskedPaths":null,"ReadonlyPaths":null},"GraphDriver":{"Data":null,"Name":"overlayfs"},"Mounts":[{"Type":"bind","Source":"/var/run/docker.sock","Destination":"/var/run/docker.sock","Mode":"rw","RW":true,"Propagation":"rprivate"}],"Config":{"Hostname":"ed2cfe48b555","Domainname":"","User":"","AttachStdin":false,"AttachStdout":false,"AttachStderr":false,"ExposedPorts":{"8080/tcp":{}},"Tty":false,"OpenStdin":false,"StdinOnce":false,"Env":["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"],"Cmd":["/bin/ryuk"],"Image":"testcontainers/ryuk:0.5.1","Volumes":null,"WorkingDir":"","Entrypoint":null,"OnBuild":null,"Labels":{"org.testcontainers":"true","org.testcontainers.lang":"java","org.testcontainers.ryuk":"true","org.testcontainers.version":"1.19.3"}},"NetworkSettings":{"Bridge":"","SandboxID":"73fe27811a62516e19a96762d4651f706b9458f8004f2138f1c5e04c6ca85a3a","SandboxKey":"/var/run/docker/netns/73fe27811a62","Ports":{"8080/tcp":[{"HostIp":"0.0.0.0","HostPort":"50133"},{"HostIp":"::","HostPort":"50133"}]},"HairpinMode":false,"LinkLocalIPv6Address":"","LinkLocalIPv6PrefixLen":0,"SecondaryIPAddresses":null,"SecondaryIPv6Addresses":null,"EndpointID":"cb7bc48546d99c669c3145ed39b87ff24133e7452c3d396722fbcab0962c12cd","Gateway":"172.17.0.1","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"IPAddress":"172.17.0.2","IPPrefixLen":16,"IPv6Gateway":"","MacAddress":"ba:6b:34:6b:f2:f7","Networks":{"bridge":{"IPAMConfig":null,"Links":null,"Aliases":null,"MacAddress":"ba:6b:34:6b:f2:f7","DriverOpts":null,"GwPriority":0,"NetworkID":"070182f8c4352237efe530a41d27261a044a578f4196783b1a6a7009353b1475","EndpointID":"cb7bc48546d99c669c3145ed39b87ff24133e7452c3d396722fbcab0962c12cd","Gateway":"172.17.0.1","IPAddress":"172.17.0.2","IPPrefixLen":16,"IPv6Gateway":"","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"DNSNames":null}}}}[\n]"
16:54:12.575 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 200 OK
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/json
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:12.580 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000006: connection can be kept alive for 3 MINUTES
16:54:12.636 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "0[\r][\n]"
16:54:12.637 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.637 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000005: releasing valid endpoint
16:54:12.637 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000005: releasing endpoint
16:54:12.637 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000005: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.637 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000005: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.638 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000007: preparing request execution
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000007: target auth state: UNCHALLENGED
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000007: proxy auth state: UNCHALLENGED
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000007: acquiring connection with route {}->unix://localhost:2375
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000007: acquiring endpoint (3 MINUTES)
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000007: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000007: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000007: acquired ep-00000006
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000007: acquired endpoint ep-00000006
16:54:12.638 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000007: executing GET /v1.32/images/sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775/json HTTP/1.1
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000006: start execution ex-00000007
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000006: executing exchange ex-00000007 over http-outgoing-0
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> GET /v1.32/images/sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775/json HTTP/1.1
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> accept: application/json
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "GET /v1.32/images/sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775/json HTTP/1.1[\r][\n]"
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "accept: application/json[\r][\n]"
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.639 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/json[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 200 OK
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/json
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:12.647 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000007: connection can be kept alive for 3 MINUTES
16:54:12.648 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "4a4[\r][\n]"
16:54:12.648 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "{"Id":"sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775","RepoTags":["testcontainers/ryuk:0.5.1"],"RepoDigests":["testcontainers/ryuk@sha256:533abc56c07b52a26c955d1e7ae428d810582ab01c156384ae79960eb5fa0775"],"Parent":"","Comment":"buildkit.dockerfile.v0","Created":"2023-05-17T15:15:17.079149055Z","DockerVersion":"","Author":"","Architecture":"arm64","Os":"linux","Size":5545431,"VirtualSize":5545431,"GraphDriver":{"Data":null,"Name":"overlayfs"},"RootFS":{"Type":"layers","Layers":["sha256:dc65b23a4b7c4b9ed9f634d3a1dd4cc10bbf0251e85480648acf7f22061b3949","sha256:bb024370e164f1614865ff03af6bf8611c2f71d13951a9d08d43b2a1548ded51","sha256:4350b9cff72cb14870bf7423b402f97761c9453dbba62ee2738c12492c5e7106"]},"Metadata":{"LastTagTime":"2025-10-03T20:05:18.291786837Z"},"Config":{"ArgsEscaped":true,"AttachStderr":false,"AttachStdin":false,"AttachStdout":false,"Cmd":["/bin/ryuk"],"Domainname":"","Entrypoint":null,"Env":["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"],"Hostname":"","Image":"","Labels":{"org.testcontainers.ryuk":"true"},"OnBuild":null,"OpenStdin":false,"StdinOnce":false,"Tty":false,"User":"","Volumes":null,"WorkingDir":""}}[\n]"
16:54:12.648 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.648 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "0[\r][\n]"
16:54:12.648 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000006: releasing valid endpoint
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000006: releasing endpoint
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000006: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000006: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.650 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: 
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000008: preparing request execution
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000008: target auth state: UNCHALLENGED
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000008: proxy auth state: UNCHALLENGED
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000008: acquiring connection with route {}->unix://localhost:2375
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000008: acquiring endpoint (3 MINUTES)
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000008: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000008: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000008: acquired ep-00000007
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000008: acquired endpoint ep-00000007
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000008: executing GET /v1.32/version HTTP/1.1
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000007: start execution ex-00000008
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000007: executing exchange ex-00000008 over http-outgoing-0
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> GET /v1.32/version HTTP/1.1
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> accept: application/json
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "GET /v1.32/version HTTP/1.1[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "accept: application/json[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.650 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/json[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "339[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "{"Platform":{"Name":"Docker Desktop 4.47.0 (206054)"},"Components":[{"Name":"Engine","Version":"28.4.0","Details":{"ApiVersion":"1.51","Arch":"arm64","BuildTime":"2025-09-03T20:58:53.000000000+00:00","Experimental":"false","GitCommit":"249d679","GoVersion":"go1.24.7","KernelVersion":"6.10.14-linuxkit","MinAPIVersion":"1.24","Os":"linux"}},{"Name":"containerd","Version":"1.7.27","Details":{"GitCommit":"05044ec0a9a75232cad458027ca83437aae3f4da"}},{"Name":"runc","Version":"1.2.5","Details":{"GitCommit":"v1.2.5-0-g59923ef"}},{"Name":"docker-init","Version":"0.19.0","Details":{"GitCommit":"de40ad0"}}],"Version":"28.4.0","ApiVersion":"1.51","MinAPIVersion":"1.24","GitCommit":"249d679","GoVersion":"go1.24.7","Os":"linux","Arch":"arm64","KernelVersion":"6.10.14-linuxkit","BuildTime":"2025-09-03T20:58:53.000000000+00:00"}[\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 200 OK
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/json
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000008: connection can be kept alive for 3 MINUTES
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "0[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000007: releasing valid endpoint
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000007: releasing endpoint
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000007: connection http-outgoing-0 can be kept alive for 3 MINUTES
16:54:12.654 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000007: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000009: preparing request execution
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000009: target auth state: UNCHALLENGED
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000009: proxy auth state: UNCHALLENGED
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000009: acquiring connection with route {}->unix://localhost:2375
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000009: acquiring endpoint (3 MINUTES)
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000009: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000009: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000009: acquired ep-00000008
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000009: acquired endpoint ep-00000008
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000009: executing GET /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/logs?stdout=true&stderr=true&follow=true&since=0 HTTP/1.1
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000008: start execution ex-00000009
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000008: executing exchange ex-00000009 over http-outgoing-0
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> GET /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/logs?stdout=true&stderr=true&follow=true&since=0 HTTP/1.1
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> User-Agent: tc-java/1.19.3
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Host: localhost:2375
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 >> Connection: keep-alive
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "GET /v1.32/containers/ed2cfe48b555d08d1aeeed34786a86d44b739e85eaa5125f991a970672f4e654/logs?stdout=true&stderr=true&follow=true&since=0 HTTP/1.1[\r][\n]"
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Host: localhost:2375[\r][\n]"
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "Connection: keep-alive[\r][\n]"
16:54:12.657 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 >> "[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Api-Version: 1.51[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Content-Type: application/vnd.docker.raw-stream[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Docker-Experimental: false[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Ostype: linux[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << HTTP/1.1 200 OK
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Api-Version: 1.51
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Content-Type: application/vnd.docker.raw-stream
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Docker-Experimental: false
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Ostype: linux
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Server: Docker/28.4.0 (linux)
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-0 << Transfer-Encoding: chunked
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000009: connection can be kept alive for 3 MINUTES
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "c0[\r][\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[0x2][0x0][0x0][0x0][0x0][0x0][0x0]&2025/10/03 20:54:12 Pinging Docker...[\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[0x2][0x0][0x0][0x0][0x0][0x0][0x0]02025/10/03 20:54:12 Docker daemon is available![\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[0x2][0x0][0x0][0x0][0x0][0x0][0x0]-2025/10/03 20:54:12 Starting on port 8080...[\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[0x2][0x0][0x0][0x0][0x0][0x0][0x0][0x1d]2025/10/03 20:54:12 Started![\n]"
16:54:12.659 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[\r][\n]"
16:54:12.660 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDERR: 2025/10/03 20:54:12 Pinging Docker...
16:54:12.660 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDERR: 2025/10/03 20:54:12 Started!
16:54:12.661 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000008: cancel
16:54:12.661 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[read] I/O error: null"
16:54:12.661 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultManagedHttpClientConnection -- http-outgoing-0: close connection IMMEDIATE
16:54:12.661 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-0 << "[read] I/O error: null"
16:54:12.661 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000008: endpoint closed
16:54:12.661 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000008: discarding endpoint
16:54:12.661 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000008: releasing endpoint
16:54:12.661 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000008: connection is not kept alive
16:54:12.662 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000008: connection released [route: {}->unix://localhost:2375][total available: 0; route allocated: 0 of 2147483647; total allocated: 0 of 2147483647]
16:54:12.662 [main] INFO tc.testcontainers/ryuk:0.5.1 -- Container testcontainers/ryuk:0.5.1 started in PT0.509658S
16:54:12.661 [docker-java-stream-1023888413] DEBUG com.github.dockerjava.zerodep.ApacheDockerHttpClientImpl$ApacheResponse -- Failed to close the response
java.nio.channels.ClosedChannelException: null
	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:222)
	at java.base/sun.nio.ch.SocketChannelImpl.blockingRead(SocketChannelImpl.java:1311)
	at java.base/sun.nio.ch.SocketInputStream.read(SocketInputStream.java:70)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.LoggingInputStream.read(LoggingInputStream.java:81)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:149)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:280)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.getChunkSize(ChunkedInputStream.java:261)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.nextChunk(ChunkedInputStream.java:222)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:147)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.close(ChunkedInputStream.java:314)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.io.Closer.close(Closer.java:48)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.IncomingHttpEntity.close(IncomingHttpEntity.java:111)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.io.entity.HttpEntityWrapper.close(HttpEntityWrapper.java:120)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.io.Closer.close(Closer.java:48)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.message.BasicClassicHttpResponse.close(BasicClassicHttpResponse.java:93)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.CloseableHttpResponse.close(CloseableHttpResponse.java:200)
	at com.github.dockerjava.zerodep.ApacheDockerHttpClientImpl$ApacheResponse.close(ApacheDockerHttpClientImpl.java:256)
	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$executeAndStream$1(DefaultInvocationBuilder.java:277)
	at java.base/java.lang.Thread.run(Thread.java:1583)
16:54:12.668 [testcontainers-ryuk] DEBUG org.testcontainers.utility.ResourceReaper -- Sending 'label=org.testcontainers%3Dtrue&label=org.testcontainers.lang%3Djava&label=org.testcontainers.version%3D1.19.3&label=org.testcontainers.sessionId%3D41781793-441b-4be0-9f6b-8274f7098da1' to Ryuk
16:54:12.671 [testcontainers-ryuk] DEBUG org.testcontainers.utility.RyukResourceReaper -- Received 'ACK' from Ryuk
16:54:12.671 [main] INFO org.testcontainers.utility.RyukResourceReaper -- Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
16:54:12.671 [main] DEBUG org.testcontainers.DockerClientFactory -- Checks are enabled
16:54:12.671 [main] INFO org.testcontainers.DockerClientFactory -- Checking the system...
16:54:12.671 [main] INFO org.testcontainers.DockerClientFactory --  Docker server version should be at least 1.6.0
16:54:12.672 [main] DEBUG org.testcontainers.utility.PrefixingImageNameSubstitutor -- No prefix is configured
16:54:12.672 [main] DEBUG org.testcontainers.utility.ImageNameSubstitutor -- Did not find a substitute image for confluentinc/cp-kafka:7.4.0 (using image substitutor: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor'))
16:54:12.672 [main] DEBUG org.testcontainers.images.AbstractImagePullPolicy -- Using locally available and not pulling image: confluentinc/cp-kafka:7.4.0
16:54:12.672 [main] DEBUG tc.confluentinc/cp-kafka:7.4.0 -- Starting container: confluentinc/cp-kafka:7.4.0
16:54:12.672 [main] DEBUG tc.confluentinc/cp-kafka:7.4.0 -- Trying to start container: confluentinc/cp-kafka:7.4.0 (attempt 1/1)
16:54:12.672 [main] DEBUG tc.confluentinc/cp-kafka:7.4.0 -- Starting container: confluentinc/cp-kafka:7.4.0
16:54:12.672 [main] INFO tc.confluentinc/cp-kafka:7.4.0 -- Creating container for image: confluentinc/cp-kafka:7.4.0
16:54:12.672 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- Looking up auth config for image: confluentinc/cp-kafka:7.4.0 at registry: https://index.docker.io/v1/
16:54:12.672 [main] DEBUG org.testcontainers.utility.RegistryAuthLocator -- Cached auth found: [AuthConfig{username=timspann623, password=hidden non-blank value, auth=blank, email=null, registryAddress=https://index.docker.io/v1/, registryToken=blank}]
16:54:12.672 [main] DEBUG org.testcontainers.dockerclient.AuthDelegatingDockerClientConfig -- Effective auth config [AuthConfig{username=timspann623, password=hidden non-blank value, auth=blank, email=null, registryAddress=https://index.docker.io/v1/, registryToken=blank}]
16:54:12.672 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: org.testcontainers.shaded.com.github.dockerjava.core.command.CreateContainerCmdImpl@732c9b5c[aliases=<null>,argsEscaped=<null>,attachStderr=<null>,attachStdin=<null>,attachStdout=<null>,authConfig=AuthConfig(username=timspann623, email=null, registryAddress=https://index.docker.io/v1/, stackOrchestrator=null),cmd={-c,while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh},domainName=<null>,entrypoint={sh},env={KAFKA_DEFAULT_REPLICATION_FACTOR=1,KAFKA_BROKER_ID=1,KAFKA_NUM_PARTITIONS=3,KAFKA_ZOOKEEPER_CONNECT=localhost:2181,KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1,KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1,KAFKA_INTER_BROKER_LISTENER_NAME=BROKER,KAFKA_AUTO_CREATE_TOPICS_ENABLE=true,KAFKA_LOG_FLUSH_INTERVAL_MESSAGES=9223372036854775807,KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1,KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092,KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1,KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT,KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0},exposedPorts=ExposedPorts(exposedPorts=[9093/tcp, 2181/tcp]),healthcheck=<null>,hostConfig=HostConfig(binds=[], blkioWeight=null, blkioWeightDevice=null, blkioDeviceReadBps=null, blkioDeviceWriteBps=null, blkioDeviceReadIOps=null, blkioDeviceWriteIOps=null, memorySwappiness=null, nanoCPUs=null, capAdd=null, capDrop=null, containerIDFile=null, cpuPeriod=null, cpuRealtimePeriod=null, cpuRealtimeRuntime=null, cpuShares=null, cpuQuota=null, cpusetCpus=null, cpusetMems=null, devices=null, deviceCgroupRules=null, deviceRequests=null, diskQuota=null, dns=null, dnsOptions=null, dnsSearch=null, extraHosts=[], groupAdd=null, ipcMode=null, cgroup=null, links=[], logConfig=LogConfig(type=null, config=null), lxcConf=null, memory=null, memorySwap=null, memoryReservation=null, kernelMemory=null, networkMode=null, oomKillDisable=null, init=null, autoRemove=null, oomScoreAdj=null, portBindings={9093/tcp=[Lcom.github.dockerjava.api.model.Ports$Binding;@7c29adc8, 2181/tcp=[Lcom.github.dockerjava.api.model.Ports$Binding;@4b2e3e8f}, privileged=null, publishAllPorts=null, readonlyRootfs=null, restartPolicy=null, ulimits=null, cpuCount=null, cpuPercent=null, ioMaximumIOps=null, ioMaximumBandwidth=null, volumesFrom=[], mounts=null, pidMode=null, isolation=null, securityOpts=null, storageOpt=null, cgroupParent=null, volumeDriver=null, shmSize=null, pidsLimit=null, runtime=null, tmpFs=null, utSMode=null, usernsMode=null, sysctls=null, consoleSize=null, cgroupnsMode=null),hostName=<null>,image=confluentinc/cp-kafka:7.4.0,ipv4Address=<null>,ipv6Address=<null>,labels={org.testcontainers=true, org.testcontainers.lang=java, org.testcontainers.version=1.19.3, org.testcontainers.sessionId=41781793-441b-4be0-9f6b-8274f7098da1},macAddress=<null>,name=<null>,networkDisabled=<null>,networkingConfig=<null>,onBuild=<null>,platform=<null>,portSpecs=<null>,shell=<null>,stdInOnce=<null>,stdinOpen=<null>,stopSignal=<null>,stopTimeout=<null>,tty=<null>,user=<null>,volumes=Volumes(volumes=[]),workingDir=<null>]
16:54:12.675 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000A: preparing request execution
16:54:12.675 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000A: target auth state: UNCHALLENGED
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000A: proxy auth state: UNCHALLENGED
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000A: acquiring connection with route {}->unix://localhost:2375
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000A: acquiring endpoint (3 MINUTES)
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000A: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 0; route allocated: 0 of 2147483647; total allocated: 0 of 2147483647]
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000A: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000A: acquired ep-00000009
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000A: acquired endpoint ep-00000009
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000A: opening connection {}->unix://localhost:2375
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000009: connecting endpoint (3 MINUTES)
16:54:12.676 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000009: connecting endpoint to unix://localhost:2375 (3 MINUTES)
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-1: connecting to localhost/127.0.0.1:2375
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-1: connection established /var/run/docker.sock<->/var/run/docker.sock
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000009: connected http-outgoing-1
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000009: endpoint connected
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000A: executing POST /v1.32/containers/create HTTP/1.1
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000009: start execution ex-0000000A
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000009: executing exchange ex-0000000A over http-outgoing-1
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> POST /v1.32/containers/create HTTP/1.1
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> X-Registry-Auth: eyJ1c2VybmFtZSI6InRpbXNwYW5uNjIzIiwicGFzc3dvcmQiOiJkY2tyX3BhdF9iQnJqVFVQSTktRjN1SmtZMGVLQzY0bGxxS0kiLCJlbWFpbCI6bnVsbCwic2VydmVyYWRkcmVzcyI6Imh0dHBzOi8vaW5kZXguZG9ja2VyLmlvL3YxLyIsImF1dGgiOm51bGwsInJlZ2lzdHJ5dG9rZW4iOm51bGwsImlkZW50aXR5dG9rZW4iOm51bGwsInN0YWNrT3JjaGVzdHJhdG9yIjpudWxsfQ==
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> accept: application/json
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> content-type: application/json
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> User-Agent: tc-java/1.19.3
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Content-Length: 2745
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Host: localhost:2375
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Connection: keep-alive
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "POST /v1.32/containers/create HTTP/1.1[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "X-Registry-Auth: eyJ1c2VybmFtZSI6InRpbXNwYW5uNjIzIiwicGFzc3dvcmQiOiJkY2tyX3BhdF9iQnJqVFVQSTktRjN1SmtZMGVLQzY0bGxxS0kiLCJlbWFpbCI6bnVsbCwic2VydmVyYWRkcmVzcyI6Imh0dHBzOi8vaW5kZXguZG9ja2VyLmlvL3YxLyIsImF1dGgiOm51bGwsInJlZ2lzdHJ5dG9rZW4iOm51bGwsImlkZW50aXR5dG9rZW4iOm51bGwsInN0YWNrT3JjaGVzdHJhdG9yIjpudWxsfQ==[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "accept: application/json[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "content-type: application/json[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Content-Length: 2745[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Host: localhost:2375[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Connection: keep-alive[\r][\n]"
16:54:12.677 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[\r][\n]"
16:54:12.678 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "{"Hostname":null,"Domainname":null,"User":null,"AttachStdin":null,"AttachStdout":null,"AttachStderr":null,"PortSpecs":null,"Tty":null,"OpenStdin":null,"StdinOnce":null,"Env":["KAFKA_DEFAULT_REPLICATION_FACTOR=1","KAFKA_BROKER_ID=1","KAFKA_NUM_PARTITIONS=3","KAFKA_ZOOKEEPER_CONNECT=localhost:2181","KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1","KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1","KAFKA_INTER_BROKER_LISTENER_NAME=BROKER","KAFKA_AUTO_CREATE_TOPICS_ENABLE=true","KAFKA_LOG_FLUSH_INTERVAL_MESSAGES=9223372036854775807","KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1","KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092","KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1","KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT","KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0"],"Cmd":["-c","while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh"],"Healthcheck":null,"ArgsEscaped":null,"Entrypoint":["sh"],"Image":"confluentinc/cp-kafka:7.4.0","Volumes":{},"WorkingDir":null,"MacAddress":null,"OnBuild":null,"NetworkDisabled":null,"ExposedPorts":{"9093/tcp":{},"2181/tcp":{}},"StopSignal":null,"StopTimeout":null,"HostConfig":{"Binds":[],"BlkioWeight":null,"BlkioWeightDevice":null,"BlkioDeviceReadBps":null,"BlkioDeviceWriteBps":null,"BlkioDeviceReadIOps":null,"BlkioDeviceWriteIOps":null,"MemorySwappiness":null,"NanoCpus":null,"CapAdd":null,"CapDrop":null,"ContainerIDFile":null,"CpuPeriod":null,"CpuRealtimePeriod":null,"CpuRealtimeRuntime":null,"CpuShares":null,"CpuQuota":null,"CpusetCpus":null,"CpusetMems":null,"Devices":null,"DeviceCgroupRules":null,"DeviceRequests":null,"DiskQuota":null,"Dns":null,"DnsOptions":null,"DnsSearch":null,"ExtraHosts":[],"GroupAdd":null,"IpcMode":null,"Cgroup":null,"Links":[],"LogConfig":null,"LxcConf":null,"Memory":null,"MemorySwap":null,"MemoryReservation":null,"KernelMemory":null,"NetworkMode":null,"OomKillDisable":null,"Init":null,"AutoRemove":null,"OomScoreAdj":null,"PortBindings":{"9093/tcp":[{"HostIp":"","HostPort":""}],"2181/tcp":[{"HostIp":"","HostPort":""}]},"Privileged":null,"PublishAllPorts":null,"ReadonlyRootfs":null,"RestartPolicy":null,"Ulimits":null,"CpuCount":null,"CpuPercent":null,"IOMaximumIOps":null,"IOMaximumBandwidth":null,"VolumesFrom":[],"Mounts":null,"PidMode":null,"Isolation":null,"SecurityOpt":null,"StorageOpt":null,"CgroupParent":null,"VolumeDriver":null,"ShmSize":null,"PidsLimit":null,"Runtime":null,"Tmpfs":null,"UTSMode":null,"UsernsMode":null,"Sysctls":null,"ConsoleSize":null,"CgroupnsMode":null},"Labels":{"org.testcontainers":"true","org.testcontainers.lang":"java","org.testcontainers.version":"1.19.3","org.testcontainers.sessionId":"41781793-441b-4be0-9f6b-8274f7098da1"},"Shell":null,"NetworkingConfig":null}"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "HTTP/1.1 201 Created[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Api-Version: 1.51[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Content-Type: application/json[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Docker-Experimental: false[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Ostype: linux[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << HTTP/1.1 201 Created
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Api-Version: 1.51
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Content-Type: application/json
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Docker-Experimental: false
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Ostype: linux
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Server: Docker/28.4.0 (linux)
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Transfer-Encoding: chunked
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000A: connection can be kept alive for 3 MINUTES
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "58[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "{"Id":"70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a","Warnings":[]}[\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "0[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000009: releasing valid endpoint
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000009: releasing endpoint
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000009: connection http-outgoing-1 can be kept alive for 3 MINUTES
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000009: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.709 [main] INFO tc.confluentinc/cp-kafka:7.4.0 -- Container confluentinc/cp-kafka:7.4.0 is starting: 70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a
16:54:12.709 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: 70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a
16:54:12.709 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000B: preparing request execution
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000B: target auth state: UNCHALLENGED
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000B: proxy auth state: UNCHALLENGED
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000B: acquiring connection with route {}->unix://localhost:2375
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000B: acquiring endpoint (3 MINUTES)
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000B: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000B: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000B: acquired ep-0000000A
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000B: acquired endpoint ep-0000000A
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000B: executing POST /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/start HTTP/1.1
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000A: start execution ex-0000000B
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000A: executing exchange ex-0000000B over http-outgoing-1
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> POST /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/start HTTP/1.1
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> accept: application/json
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> content-type: application/json
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> User-Agent: tc-java/1.19.3
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Host: localhost:2375
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Connection: keep-alive
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "POST /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/start HTTP/1.1[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "accept: application/json[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "content-type: application/json[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Host: localhost:2375[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Connection: keep-alive[\r][\n]"
16:54:12.710 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "HTTP/1.1 204 No Content[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Api-Version: 1.51[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Docker-Experimental: false[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Ostype: linux[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << HTTP/1.1 204 No Content
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Api-Version: 1.51
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Docker-Experimental: false
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Ostype: linux
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Server: Docker/28.4.0 (linux)
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000B: connection can be kept alive for 3 MINUTES
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000A: releasing valid endpoint
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000A: releasing endpoint
16:54:12.853 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000A: connection http-outgoing-1 can be kept alive for 3 MINUTES
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000A: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.854 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: 70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a,false
16:54:12.854 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.exec.InspectContainerCmdExec -- GET: DefaultWebTarget{path=[/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json], queryParams={}}
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000C: preparing request execution
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000C: target auth state: UNCHALLENGED
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000C: proxy auth state: UNCHALLENGED
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000C: acquiring connection with route {}->unix://localhost:2375
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000C: acquiring endpoint (3 MINUTES)
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000C: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000C: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000C: acquired ep-0000000B
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000C: acquired endpoint ep-0000000B
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000C: executing GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json HTTP/1.1
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000B: start execution ex-0000000C
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000B: executing exchange ex-0000000C over http-outgoing-1
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json HTTP/1.1
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> accept: application/json
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> User-Agent: tc-java/1.19.3
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Host: localhost:2375
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Connection: keep-alive
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json HTTP/1.1[\r][\n]"
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "accept: application/json[\r][\n]"
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Host: localhost:2375[\r][\n]"
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Connection: keep-alive[\r][\n]"
16:54:12.854 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Api-Version: 1.51[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Content-Type: application/json[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Docker-Experimental: false[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Ostype: linux[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << HTTP/1.1 200 OK
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Api-Version: 1.51
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Content-Type: application/json
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Docker-Experimental: false
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Ostype: linux
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Server: Docker/28.4.0 (linux)
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Transfer-Encoding: chunked
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000C: connection can be kept alive for 3 MINUTES
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "1cee[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "{"Id":"70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a","Created":"2025-10-03T20:54:12.685485717Z","Path":"sh","Args":["-c","while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh"],"State":{"Status":"running","Running":true,"Paused":false,"Restarting":false,"OOMKilled":false,"Dead":false,"Pid":3430,"ExitCode":0,"Error":"","StartedAt":"2025-10-03T20:54:12.732924217Z","FinishedAt":"0001-01-01T00:00:00Z"},"Image":"sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63","ResolvConfPath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/resolv.conf","HostnamePath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/hostname","HostsPath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/hosts","LogPath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a-json.log","Name":"/hopeful_albattani","RestartCount":0,"Driver":"overlayfs","Platform":"linux","MountLabel":"","ProcessLabel":"","AppArmorProfile":"","ExecIDs":null,"HostConfig":{"Binds":[],"ContainerIDFile":"","LogConfig":{"Type":"json-file","Config":{}},"NetworkMode":"bridge","PortBindings":{"2181/tcp":[{"HostIp":"","HostPort":""}],"9093/tcp":[{"HostIp":"","HostPort":""}]},"RestartPolicy":{"Name":"no","MaximumRetryCount":0},"AutoRemove":false,"VolumeDriver":"","VolumesFrom":[],"ConsoleSize":[0,0],"CapAdd":null,"CapDrop":null,"CgroupnsMode":"private","Dns":null,"DnsOptions":null,"DnsSearch":null,"ExtraHosts":[],"GroupAdd":null,"IpcMode":"shareable","Cgroup":"","Links":null,"OomScoreAdj":0,"PidMode":"","Privileged":false,"PublishAllPorts":false,"ReadonlyRootfs":false,"SecurityOpt":null,"UTSMode":"","UsernsMode":"","ShmSize":67108864,"Runtime":"runc","Isolation":"","CpuShares":0,"Memory":0,"NanoCpus":0,"CgroupParent":"","BlkioWeight":0,"BlkioWeightDevice":null,"BlkioDeviceReadBps":null,"BlkioDeviceWriteBps":null,"BlkioDeviceReadIOps":null,"BlkioDeviceWriteIOps":null,"CpuPeriod":0,"CpuQuota":0,"CpuRealtimePeriod":0,"CpuRealtimeRuntime":0,"CpusetCpus":"","CpusetMems":"","Devices":null,"DeviceCgroupRules":null,"DeviceRequests":null,"MemoryReservation":0,"MemorySwap":0,"MemorySwappiness":null,"OomKillDisable":null,"PidsLimit":null,"Ulimits":null,"CpuCount":0,"CpuPercent":0,"IOMaximumIOps":0,"IOMaximumBandwidth":0,"MaskedPaths":["/proc/asound","/proc/acpi","/proc/interrupts","/proc/kcore","/proc/keys","/proc/latency_stats","/proc/timer_list","/proc/timer_stats","/proc/sched_debug","/proc/scsi","/sys/firmware","/sys/devices/virtual/powercap"],"ReadonlyPaths":["/proc/bus","/proc/fs","/proc/irq","/proc/sys","/proc/sysrq-trigger"]},"GraphDriver":{"Data":null,"Name":"overlayfs"},"Mounts":[{"Type":"volume","Name":"cc01bcf954d0c6b7eb7d9e22d9bd695f952602f81b513e5c1b7f2d19e29926c2","Source":"/var/lib/docker/volumes/cc01bcf954d0c6b7eb7d9e22d9bd695f952602f81b513e5c1b7f2d19e29926c2/_data","Destination":"/etc/kafka/secrets","Driver":"local","Mode":"","RW":true,"Propagation":""},{"Type":"volume","Name":"28e334bdccbdadfc8f06b0f275765c77237cc7e4508a02165e73656776b4390e","Source":"/var/lib/docker/volumes/28e334bdccbdadfc8f06b0f275765c77237cc7e4508a02165e73656776b4390e/_data","Destination":"/var/lib/kafka/data","Driver":"local","Mode":"","RW":true,"Propagation":""}],"Config":{"Hostname":"70e45fd582a9","Domainname":"","User":"appuser","AttachStdin":false,"AttachStdout":false,"AttachStderr":false,"ExposedPorts":{"2181/tcp":{},"9092/tcp":{},"9093/tcp":{}},"Tty":false,"OpenStdin":false,"StdinOnce":false,"Env":["KAFKA_DEFAULT_REPLICATION_FACTOR=1","KAFKA_BROKER_ID=1","KAFKA_NUM_PARTITIONS=3","KAFKA_ZOOKEEPER_CONNECT=localhost:2181","KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1","KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1","KAFKA_INTER_BROKER_LISTENER_NAME=BROKER","KAFKA_AUTO_CREATE_TOPICS_ENABLE=true","KAFKA_LOG_FLUSH_INTERVAL_MESSAGES=9223372036854775807","KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1","KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092","KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1","KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT","KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0","PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin","container=oci","LANG=C.UTF-8","CUB_CLASSPATH=\"/usr/share/java/cp-base-new/*\"","KAFKA_ADVERTISED_LISTENERS=","CLUSTER_ID=","COMPONENT=kafka"],"Cmd":["-c","while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh"],"Image":"confluentinc/cp-kafka:7.4.0","Volumes":{"/etc/kafka/secrets":{},"/var/lib/kafka/data":{}},"WorkingDir":"/home/appuser","Entrypoint":["sh"],"OnBuild":null,"Labels":{"architecture":"aarch64","build-date":"2023-05-03T15:02:09","com.redhat.component":"ubi8-minimal-container","com.redhat.license_terms":"https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI","description":"Common base image for Confluent's Docker images.","distribution-scope":"public","io.buildah.version":"1.27.3","io.confluent.docker":"true","io.confluent.docker.build.number":"5","io.confluent.docker.git.id":"d0965a30f","io.confluent.docker.git.repo":"confluentinc/kafka-images","io.k8s.description":"The Universal Base Image Minimal is a stripped down image that uses microdnf as a package manager. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.","io.k8s.display-name":"Red Hat Universal Base Image 8 Minimal","io.openshift.expose-services":"","io.openshift.tags":"minimal rhel8","maintainer":"partner-support@confluent.io","name":"cp-kafka","org.testcontainers":"true","org.testcontainers.lang":"java","org.testcontainers.sessionId":"41781793-441b-4be0-9f6b-8274f7098da1","org.testcontainers.version":"1.19.3","release":"7.4.0","summary":"Confluent platform Kafka.","url":"https://access.redhat.com/containers/#/registry.access.redhat.com/ubi8-minimal/images/8.8-860","vcs-ref":"dee8029ddcc7ecbfbebb0905d2b15e134338616c","vcs-type":"git","vendor":"Confluent","version":"d0965a30f"}},"NetworkSettings":{"Bridge":"","SandboxID":"937606067d4adb7b3d7335a93b35235e8c6e709610b07aa39c78e5c3c3d71b07","SandboxKey":"/var/run/docker/netns/937606067d4a","Ports":{"2181/tcp":[{"HostIp":"0.0.0.0","HostPort":"50138"},{"HostIp":"::","HostPort":"50138"}],"9093/tcp":[{"HostIp":"0.0.0.0","HostPort":"50137"},{"HostIp":"::","HostPort":"50137"}]},"HairpinMode":false,"LinkLocalIPv6Address":"","LinkLocalIPv6PrefixLen":0,"SecondaryIPAddresses":null,"SecondaryIPv6Addresses":null,"EndpointID":"637551cc0f3590daff1ae17cd2aedf737706c58228b5a0994ba8246b7da2db84","Gateway":"172.17.0.1","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"IPAddress":"172.17.0.3","IPPrefixLen":16,"IPv6Gateway":"","MacAddress":"f6:eb:2f:89:04:d1","Networks":{"bridge":{"IPAMConfig":null,"Links":null,"Aliases":null,"MacAddress":"f6:eb:2f:89:04:d1","DriverOpts":null,"GwPriority":0,"NetworkID":"070182f8c4352237efe530a41d27261a044a578f4196783b1a6a7009353b1475","EndpointID":"637551cc0f3590daff1ae17cd2aedf737706c58228b5a0994ba8246b7da2db84","Gateway":"172.17.0.1","IPAddress":"172.17.0.3","IPPrefixLen":16,"IPv6Gateway":"","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"DNSNames":null}}}}[\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "0[\r][\n]"
16:54:12.856 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.858 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000B: releasing valid endpoint
16:54:12.858 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000B: releasing endpoint
16:54:12.858 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000B: connection http-outgoing-1 can be kept alive for 3 MINUTES
16:54:12.858 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000B: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.859 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000D: preparing request execution
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000D: target auth state: UNCHALLENGED
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000D: proxy auth state: UNCHALLENGED
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000D: acquiring connection with route {}->unix://localhost:2375
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000D: acquiring endpoint (3 MINUTES)
16:54:12.859 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000D: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000D: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000D: acquired ep-0000000C
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000D: acquired endpoint ep-0000000C
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000D: executing GET /v1.32/images/sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63/json HTTP/1.1
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000C: start execution ex-0000000D
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000C: executing exchange ex-0000000D over http-outgoing-1
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> GET /v1.32/images/sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63/json HTTP/1.1
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> accept: application/json
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> User-Agent: tc-java/1.19.3
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Host: localhost:2375
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Connection: keep-alive
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "GET /v1.32/images/sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63/json HTTP/1.1[\r][\n]"
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "accept: application/json[\r][\n]"
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Host: localhost:2375[\r][\n]"
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Connection: keep-alive[\r][\n]"
16:54:12.860 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Api-Version: 1.51[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Content-Type: application/json[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Docker-Experimental: false[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Ostype: linux[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << HTTP/1.1 200 OK
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Api-Version: 1.51
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Content-Type: application/json
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Docker-Experimental: false
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Ostype: linux
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Server: Docker/28.4.0 (linux)
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Transfer-Encoding: chunked
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000D: connection can be kept alive for 3 MINUTES
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "cd0[\r][\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "{"Id":"sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63","RepoTags":["confluentinc/cp-kafka:7.4.0"],"RepoDigests":["confluentinc/cp-kafka@sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63"],"Parent":"","Comment":"","Created":"2023-06-01T01:07:30.576873149Z","DockerVersion":"","Author":"","Architecture":"arm64","Os":"linux","Size":430914207,"VirtualSize":430914207,"GraphDriver":{"Data":null,"Name":"overlayfs"},"RootFS":{"Type":"layers","Layers":["sha256:9c16453a7dec20143548658040b4ef6431908f00b6e9dc5c85a3111e76da430d","sha256:42f30fde5b4dd8806982b571c6186d1b3acb45e7dd8e116c4c1d276731fbab21","sha256:f0e73fa4d6b527c75e6d88051abad40c19ecc09442a64e4d018f1b0a426134ce","sha256:89840e26aa94647d3635fe938deb6fec1a4f3fc634fdcf9bad2317c436732fbb","sha256:1bf9f8f6f3177b5a44b122dbb46b12acbd825e8850085c05b22183977415c376","sha256:432f242824bfdb506c772f794119ef28e5764ce4e9001fc424fc96c257f3f64b","sha256:140ed51c57fc2cf2f9e376e9a7aa0c15bc94454316d34437ae61b8efe1a6eea3","sha256:01cc90969fa711ede98d06ce7864f51ba026c1ac9948c75a7559f45cdb59df24","sha256:9fb9734be2733e16630328662ff8ce62ee0bb86d942db6ade483e3dda491857d","sha256:eb50e5bad80e5c17c17d457a1270e9d30448b855458f250277b405b3a92d5be2","sha256:5a23b8c0b6704cb6a1099b2813d17e3cca1217133b3ed262bd82ef4d0209c04b"]},"Metadata":{"LastTagTime":"2025-10-03T20:05:27.176940633Z"},"Config":{"AttachStderr":false,"AttachStdin":false,"AttachStdout":false,"Cmd":["/etc/confluent/docker/run"],"Domainname":"","Entrypoint":null,"Env":["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin","container=oci","LANG=C.UTF-8","CUB_CLASSPATH=\"/usr/share/java/cp-base-new/*\"","KAFKA_ZOOKEEPER_CONNECT=","KAFKA_ADVERTISED_LISTENERS=","CLUSTER_ID=","COMPONENT=kafka"],"ExposedPorts":{"9092/tcp":{}},"Hostname":"","Image":"","Labels":{"architecture":"aarch64","build-date":"2023-05-03T15:02:09","com.redhat.component":"ubi8-minimal-container","com.redhat.license_terms":"https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI","description":"Common base image for Confluent's Docker images.","distribution-scope":"public","io.buildah.version":"1.27.3","io.confluent.docker":"true","io.confluent.docker.build.number":"5","io.confluent.docker.git.id":"d0965a30f","io.confluent.docker.git.repo":"confluentinc/kafka-images","io.k8s.description":"The Universal Base Image Minimal is a stripped down image that uses microdnf as a package manager. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.","io.k8s.display-name":"Red Hat Universal Base Image 8 Minimal","io.openshift.expose-services":"","io.openshift.tags":"minimal rhel8","maintainer":"partner-support@confluent.io","name":"cp-kafka","release":"7.4.0","summary":"Confluent platform Kafka.","url":"https://access.redhat.com/containers/#/registry.access.redhat.com/ubi8-minimal/images/8.8-860","vcs-ref":"dee8029ddcc7ecbfbebb0905d2b15e134338616c","vcs-type":"git","vendor":"Confluent","version":"d0965a30f"},"OnBuild":null,"OpenStdin":false,"StdinOnce":false,"Tty":false,"User":"appuser","Volumes":{"/etc/kafka/secrets":{},"/var/lib/kafka/data":{}},"WorkingDir":"/home/appuser"}}[\n]"
16:54:12.879 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.880 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "0[\r][\n]"
16:54:12.880 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.880 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000C: releasing valid endpoint
16:54:12.880 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000C: releasing endpoint
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000C: connection http-outgoing-1 can be kept alive for 3 MINUTES
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000C: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.881 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: 
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000E: preparing request execution
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000E: target auth state: UNCHALLENGED
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000E: proxy auth state: UNCHALLENGED
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000E: acquiring connection with route {}->unix://localhost:2375
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000E: acquiring endpoint (3 MINUTES)
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000E: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000E: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000E: acquired ep-0000000D
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000E: acquired endpoint ep-0000000D
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000E: executing GET /v1.32/version HTTP/1.1
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000D: start execution ex-0000000E
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000D: executing exchange ex-0000000E over http-outgoing-1
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> GET /v1.32/version HTTP/1.1
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> accept: application/json
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> User-Agent: tc-java/1.19.3
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Host: localhost:2375
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Connection: keep-alive
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "GET /v1.32/version HTTP/1.1[\r][\n]"
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "accept: application/json[\r][\n]"
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Host: localhost:2375[\r][\n]"
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Connection: keep-alive[\r][\n]"
16:54:12.881 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[\r][\n]"
16:54:12.892 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.892 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Api-Version: 1.51[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Content-Type: application/json[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Docker-Experimental: false[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Ostype: linux[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "339[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "{"Platform":{"Name":"Docker Desktop 4.47.0 (206054)"},"Components":[{"Name":"Engine","Version":"28.4.0","Details":{"ApiVersion":"1.51","Arch":"arm64","BuildTime":"2025-09-03T20:58:53.000000000+00:00","Experimental":"false","GitCommit":"249d679","GoVersion":"go1.24.7","KernelVersion":"6.10.14-linuxkit","MinAPIVersion":"1.24","Os":"linux"}},{"Name":"containerd","Version":"1.7.27","Details":{"GitCommit":"05044ec0a9a75232cad458027ca83437aae3f4da"}},{"Name":"runc","Version":"1.2.5","Details":{"GitCommit":"v1.2.5-0-g59923ef"}},{"Name":"docker-init","Version":"0.19.0","Details":{"GitCommit":"de40ad0"}}],"Version":"28.4.0","ApiVersion":"1.51","MinAPIVersion":"1.24","GitCommit":"249d679","GoVersion":"go1.24.7","Os":"linux","Arch":"arm64","KernelVersion":"6.10.14-linuxkit","BuildTime":"2025-09-03T20:58:53.000000000+00:00"}[\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << HTTP/1.1 200 OK
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Api-Version: 1.51
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Content-Type: application/json
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Docker-Experimental: false
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Ostype: linux
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Server: Docker/28.4.0 (linux)
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Transfer-Encoding: chunked
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000E: connection can be kept alive for 3 MINUTES
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "0[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000D: releasing valid endpoint
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000D: releasing endpoint
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000D: connection http-outgoing-1 can be kept alive for 3 MINUTES
16:54:12.893 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000D: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.902 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: org.testcontainers.shaded.com.github.dockerjava.core.command.CopyArchiveToContainerCmdImpl@37ed010a[cp ,-a=false ,<null>, ,70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a,:,/]
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000F: preparing request execution
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000F: target auth state: UNCHALLENGED
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000F: proxy auth state: UNCHALLENGED
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000F: acquiring connection with route {}->unix://localhost:2375
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000F: acquiring endpoint (3 MINUTES)
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000F: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000F: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000F: acquired ep-0000000E
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000F: acquired endpoint ep-0000000E
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000F: executing PUT /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/archive?noOverwriteDirNonDir=false&path=%2F&copyUIDGID=false HTTP/1.1
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000E: start execution ex-0000000F
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000E: executing exchange ex-0000000F over http-outgoing-1
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> PUT /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/archive?noOverwriteDirNonDir=false&path=%2F&copyUIDGID=false HTTP/1.1
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> content-type: APPLICATION_X_TAR
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> User-Agent: tc-java/1.19.3
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Transfer-Encoding: chunked
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Host: localhost:2375
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 >> Connection: keep-alive
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "PUT /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/archive?noOverwriteDirNonDir=false&path=%2F&copyUIDGID=false HTTP/1.1[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "content-type: APPLICATION_X_TAR[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Transfer-Encoding: chunked[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Host: localhost:2375[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "Connection: keep-alive[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[\r][\n]"
16:54:12.903 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "c00[\r][\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "./PaxHeaders.X/testcontainers_start.sh[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]0100644 0000000 0000000 00000000034 15070033764 017124[0x0] x[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]ustar[0x0]00[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]0000000 0000000 [0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]28 mtime=1759524852.8976640[\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]testcontainers_start.sh[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]0000777 0000000 0000000 00000000604 15070033764 014436[0x0] 0[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]ustar[0x0]00[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]0000000 0000000 [0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0]#!/bin/bash[\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:50137,BROKER://70e45fd582a9:9092[\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "echo '' > /etc/confluent/docker/ensure [\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "echo 'clientPort=2181' > zookeeper.properties[\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "echo 'dataDir=/var/lib/zookeeper/data' >> zookeeper.properties[\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "echo 'dataLogDir=/var/lib/zookeeper/log' >> zookeeper.properties[\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "zookeeper-server-start zookeeper.properties &[\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "/etc/confluent/docker/run [\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0x0][\r][\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "0[\r][\n]"
16:54:12.904 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 >> "[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Api-Version: 1.51[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Docker-Experimental: false[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Ostype: linux[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[\r][\n]"
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << HTTP/1.1 200 OK
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Api-Version: 1.51
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Docker-Experimental: false
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Ostype: linux
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Server: Docker/28.4.0 (linux)
16:54:12.910 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-1 << Transfer-Encoding: chunked
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000F: connection can be kept alive for 3 MINUTES
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000F: Cannot retry non-repeatable request
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.HttpRequestRetryExec -- ex-0000000F: cannot retry non-repeatable request
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000E: cancel
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultManagedHttpClientConnection -- http-outgoing-1: close connection IMMEDIATE
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000E: endpoint closed
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000E: discarding endpoint
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000E: releasing endpoint
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000E: connection is not kept alive
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000E: connection released [route: {}->unix://localhost:2375][total available: 0; route allocated: 0 of 2147483647; total allocated: 0 of 2147483647]
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-1 << "[read] I/O error: null"
16:54:12.911 [main] DEBUG com.github.dockerjava.zerodep.ApacheDockerHttpClientImpl$ApacheResponse -- Failed to close the response
java.nio.channels.ClosedChannelException: null
	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:222)
	at java.base/sun.nio.ch.SocketChannelImpl.blockingRead(SocketChannelImpl.java:1311)
	at java.base/sun.nio.ch.SocketInputStream.read(SocketInputStream.java:70)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.LoggingInputStream.read(LoggingInputStream.java:81)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:149)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:280)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.getChunkSize(ChunkedInputStream.java:261)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.nextChunk(ChunkedInputStream.java:222)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:183)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:210)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.close(ChunkedInputStream.java:319)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.io.Closer.close(Closer.java:48)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.IncomingHttpEntity.close(IncomingHttpEntity.java:111)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.io.entity.HttpEntityWrapper.close(HttpEntityWrapper.java:120)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.io.Closer.close(Closer.java:48)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.message.BasicClassicHttpResponse.close(BasicClassicHttpResponse.java:93)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.CloseableHttpResponse.close(CloseableHttpResponse.java:200)
	at com.github.dockerjava.zerodep.ApacheDockerHttpClientImpl$ApacheResponse.close(ApacheDockerHttpClientImpl.java:256)
	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.put(DefaultInvocationBuilder.java:223)
	at org.testcontainers.shaded.com.github.dockerjava.core.exec.CopyArchiveToContainerCmdExec.execute(CopyArchiveToContainerCmdExec.java:34)
	at org.testcontainers.shaded.com.github.dockerjava.core.exec.CopyArchiveToContainerCmdExec.execute(CopyArchiveToContainerCmdExec.java:13)
	at org.testcontainers.shaded.com.github.dockerjava.core.exec.AbstrSyncDockerCmdExec.exec(AbstrSyncDockerCmdExec.java:21)
	at org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd.exec(AbstrDockerCmd.java:33)
	at org.testcontainers.shaded.com.github.dockerjava.core.command.CopyArchiveToContainerCmdImpl.exec(CopyArchiveToContainerCmdImpl.java:162)
	at org.testcontainers.containers.ContainerState.copyFileToContainer(ContainerState.java:339)
	at org.testcontainers.containers.KafkaContainer.containerIsStarting(KafkaContainer.java:199)
	at org.testcontainers.containers.GenericContainer.containerIsStarting(GenericContainer.java:715)
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:492)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:357)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:347)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:333)
	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.start(TestcontainersExtension.java:280)
	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.access$200(TestcontainersExtension.java:267)
	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$null$4(TestcontainersExtension.java:82)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore.lambda$getOrComputeIfAbsent$5(NamespacedHierarchicalStore.java:147)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore$MemoizingSupplier.computeValue(NamespacedHierarchicalStore.java:372)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore$MemoizingSupplier.get(NamespacedHierarchicalStore.java:361)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore$StoredValue.evaluate(NamespacedHierarchicalStore.java:308)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore$StoredValue.access$200(NamespacedHierarchicalStore.java:287)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore.getOrComputeIfAbsent(NamespacedHierarchicalStore.java:149)
	at org.junit.jupiter.engine.execution.NamespaceAwareStore.lambda$getOrComputeIfAbsent$2(NamespaceAwareStore.java:57)
	at org.junit.jupiter.engine.execution.NamespaceAwareStore.accessStore(NamespaceAwareStore.java:90)
	at org.junit.jupiter.engine.execution.NamespaceAwareStore.getOrComputeIfAbsent(NamespaceAwareStore.java:57)
	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$startContainers$5(TestcontainersExtension.java:82)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
	at org.testcontainers.junit.jupiter.TestcontainersExtension.startContainers(TestcontainersExtension.java:82)
	at org.testcontainers.junit.jupiter.TestcontainersExtension.beforeAll(TestcontainersExtension.java:56)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$12(ClassBasedTestDescriptor.java:396)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:396)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:212)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:85)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
	at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:56)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:184)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:148)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:122)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495)
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000010: preparing request execution
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000010: target auth state: UNCHALLENGED
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000010: proxy auth state: UNCHALLENGED
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000010: acquiring connection with route {}->unix://localhost:2375
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000010: acquiring endpoint (3 MINUTES)
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000010: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 0; route allocated: 0 of 2147483647; total allocated: 0 of 2147483647]
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000010: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000010: acquired ep-0000000F
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000010: acquired endpoint ep-0000000F
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000010: opening connection {}->unix://localhost:2375
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000F: connecting endpoint (3 MINUTES)
16:54:12.912 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000F: connecting endpoint to unix://localhost:2375 (3 MINUTES)
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-2: connecting to localhost/127.0.0.1:2375
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-2: connection established /var/run/docker.sock<->/var/run/docker.sock
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000F: connected http-outgoing-2
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000F: endpoint connected
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000010: executing GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/logs?stdout=true&stderr=true&follow=true&since=0 HTTP/1.1
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000F: start execution ex-00000010
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000F: executing exchange ex-00000010 over http-outgoing-2
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 >> GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/logs?stdout=true&stderr=true&follow=true&since=0 HTTP/1.1
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 >> User-Agent: tc-java/1.19.3
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 >> Host: localhost:2375
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 >> Connection: keep-alive
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 >> "GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/logs?stdout=true&stderr=true&follow=true&since=0 HTTP/1.1[\r][\n]"
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 >> "Host: localhost:2375[\r][\n]"
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 >> "Connection: keep-alive[\r][\n]"
16:54:12.913 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 >> "[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "HTTP/1.1 200 OK[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "Api-Version: 1.51[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "Content-Type: application/vnd.docker.raw-stream[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "Date: Fri, 03 Oct 2025 20:54:12 GMT[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "Docker-Experimental: false[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "Ostype: linux[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "Transfer-Encoding: chunked[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << HTTP/1.1 200 OK
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << Api-Version: 1.51
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << Content-Type: application/vnd.docker.raw-stream
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << Date: Fri, 03 Oct 2025 20:54:12 GMT
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << Docker-Experimental: false
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << Ostype: linux
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << Server: Docker/28.4.0 (linux)
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-2 << Transfer-Encoding: chunked
16:54:12.916 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000010: connection can be kept alive for 3 MINUTES
16:54:12.957 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "12[\r][\n]"
16:54:12.957 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][\n]"
16:54:12.957 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "===> User[\n]"
16:54:12.957 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:12.957 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: ===> User
16:54:12.958 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "41[\r][\n]"
16:54:12.958 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]9uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)[\n]"
16:54:12.958 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:12.958 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1d[\r][\n]"
16:54:12.958 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x15]===> Configuring ...[\n]"
16:54:12.958 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:12.959 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "25[\r][\n]"
16:54:12.959 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1d]Running in Zookeeper mode...[\n]"
16:54:12.959 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:12.969 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: Running in Zookeeper mode...
16:54:12.969 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: ===> Configuring ...
16:54:12.969 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
16:54:13.370 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8e[\r][\n]"
16:54:13.370 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff86][2025-10-03 20:54:13,367] INFO Reading configuration from: zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.370 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.370 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,367] INFO Reading configuration from: zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.370 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "a8[\r][\n]"
16:54:13.370 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa0][2025-10-03 20:54:13,367] WARN zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.370 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.370 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,367] WARN zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.374 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "226[\r][\n]"
16:54:13.374 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]w[2025-10-03 20:54:13,370] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.374 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]q[2025-10-03 20:54:13,370] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.374 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]s[2025-10-03 20:54:13,370] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.374 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffab][2025-10-03 20:54:13,370] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.374 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.381 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "219[\r][\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]v[2025-10-03 20:54:13,378] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)[\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:13,379] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)[\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]p[2025-10-03 20:54:13,379] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)[\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9f][2025-10-03 20:54:13,379] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)[\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7d[\r][\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]u[2025-10-03 20:54:13,379] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)[\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "136[\r][\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff86][2025-10-03 20:54:13,380] INFO Reading configuration from: zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa0][2025-10-03 20:54:13,380] WARN zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.382 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,380] WARN zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,380] INFO Reading configuration from: zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,379] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7f[\r][\n]"
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]w[2025-10-03 20:54:13,380] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,379] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,380] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,379] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "210[\r][\n]"
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]q[2025-10-03 20:54:13,380] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]s[2025-10-03 20:54:13,380] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffab][2025-10-03 20:54:13,380] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[\n]"
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,379] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]a[2025-10-03 20:54:13,381] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)[\n]"
16:54:13.383 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,378] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,380] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,381] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,380] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,380] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,370] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,370] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.383 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,370] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.384 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,370] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
16:54:13.389 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "bd[\r][\n]"
16:54:13.389 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffb5][2025-10-03 20:54:13,386] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@26275bef (org.apache.zookeeper.server.ServerMetrics)[\n]"
16:54:13.389 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.391 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "87[\r][\n]"
16:54:13.391 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x7f][2025-10-03 20:54:13,388] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)[\n]"
16:54:13.391 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.396 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,388] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
16:54:13.397 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,386] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@26275bef (org.apache.zookeeper.server.ServerMetrics)
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "57e[\r][\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]N[2025-10-03 20:54:13,396] INFO  (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff93][2025-10-03 20:54:13,396] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff93][2025-10-03 20:54:13,396] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff93][2025-10-03 20:54:13,396] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff91][2025-10-03 20:54:13,396] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff92][2025-10-03 20:54:13,396] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8e][2025-10-03 20:54:13,396] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff94][2025-10-03 20:54:13,396] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff94][2025-10-03 20:54:13,396] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]N[2025-10-03 20:54:13,396] INFO  (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1c5[\r][\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffc1][2025-10-03 20:54:13,397] INFO Server environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]w[2025-10-03 20:54:13,397] INFO Server environment:host.name=70e45fd582a9 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]u[2025-10-03 20:54:13,397] INFO Server environment:java.version=11.0.18 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.399 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1911[\r][\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x7f][2025-10-03 20:54:13,397] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff81][2025-10-03 20:54:13,397] INFO Server environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x17][0xfffffff9][2025-10-03 20:54:13,397] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.4.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jose4j-0.7.9.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.4.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jline-3.21.0.jar:/usr/bin/../share/java/kafka/connect-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/kafka-group-coordinator-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-mirror-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.4.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.86.Final.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.4.jar:/usr/bin/../share/java/kafka/kafka-raft-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/kafka-clients-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.6.0.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.4.jar:/usr/bin/../share/java/kafka/reload4j-1.2.19.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/audience-annotations-0.13.0.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.4.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/kafka-streams-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.4.2.jar:/usr/bin/../share/java/kafka/connect-transforms-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.4.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-storage-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-shell-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-json-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/connect-runtime-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/kafka-tools-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.2-1.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.86.Final.jar:/usr/bin/../share/java/kafka/trogdor-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.86.Final.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.4.0-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "394[\r][\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa9][2025-10-03 20:54:13,397] INFO Server environment:java.library.path=/usr/java/packages/lib:/lib:/usr/lib:/usr/lib64:/lib64 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:13,397] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]s[2025-10-03 20:54:13,397] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]n[2025-10-03 20:54:13,397] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]p[2025-10-03 20:54:13,397] INFO Server environment:os.arch=aarch64 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]|[2025-10-03 20:54:13,397] INFO Server environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]r[2025-10-03 20:54:13,397] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "4c7[\r][\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]x[2025-10-03 20:54:13,397] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]w[2025-10-03 20:54:13,397] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]u[2025-10-03 20:54:13,397] INFO Server environment:os.memory.free=492MB (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:13,397] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]v[2025-10-03 20:54:13,397] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]s[2025-10-03 20:54:13,397] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]m[2025-10-03 20:54:13,397] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]v[2025-10-03 20:54:13,398] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.400 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]d[2025-10-03 20:54:13,398] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]o[2025-10-03 20:54:13,398] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "f2[\r][\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]i[2025-10-03 20:54:13,398] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]y[2025-10-03 20:54:13,398] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7c[\r][\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:13,398] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)[\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "e7[\r][\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]k[2025-10-03 20:54:13,399] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]l[2025-10-03 20:54:13,399] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.401 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "10c[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]~[2025-10-03 20:54:13,399] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]~[2025-10-03 20:54:13,399] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "118[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff84][2025-10-03 20:54:13,400] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff84][2025-10-03 20:54:13,400] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "112[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x7f][2025-10-03 20:54:13,400] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff83][2025-10-03 20:54:13,400] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "10e[\r][\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]|[2025-10-03 20:54:13,400] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff82][2025-10-03 20:54:13,400] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)[\n]"
16:54:13.402 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.407 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,400] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
16:54:13.407 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,400] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,400] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,400] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,400] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,400] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,399] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,399] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,399] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,399] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,398] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,398] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,398] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,398] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,398] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,398] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:os.memory.free=492MB (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.408 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.409 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.409 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.409 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.409 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:os.arch=aarch64 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.409 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.409 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.409 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.410 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:java.library.path=/usr/java/packages/lib:/lib:/usr/lib:/usr/lib64:/lib64 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.410 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.4.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jose4j-0.7.9.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.4.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jline-3.21.0.jar:/usr/bin/../share/java/kafka/connect-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/kafka-group-coordinator-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-mirror-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.4.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.86.Final.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.4.jar:/usr/bin/../share/java/kafka/kafka-raft-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/kafka-clients-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.6.0.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.4.jar:/usr/bin/../share/java/kafka/reload4j-1.2.19.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/audience-annotations-0.13.0.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.4.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/kafka-streams-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.4.2.jar:/usr/bin/../share/java/kafka/connect-transforms-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.4.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-storage-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-shell-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-json-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/connect-runtime-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/kafka-tools-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.2-1.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.86.Final.jar:/usr/bin/../share/java/kafka/trogdor-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.86.Final.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.4.0-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.411 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.411 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.411 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:java.version=11.0.18 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.411 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:host.name=70e45fd582a9 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.411 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,397] INFO Server environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.411 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.411 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.412 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,396] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.413 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8f[\r][\n]"
16:54:13.413 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff87][2025-10-03 20:54:13,405] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.413 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.413 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "192[\r][\n]"
16:54:13.413 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]v[2025-10-03 20:54:13,405] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.413 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x1][0xc][2025-10-03 20:54:13,405] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.413 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.424 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,405] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.424 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,405] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.424 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,405] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.439 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "86[\r][\n]"
16:54:13.439 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]~[2025-10-03 20:54:13,436] INFO Logging initialized @416ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)[\n]"
16:54:13.439 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.439 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,436] INFO Logging initialized @416ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
16:54:13.513 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "114[\r][\n]"
16:54:13.513 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa1][2025-10-03 20:54:13,510] WARN o.e.j.s.ServletContextHandler@1c9b0314{/,null,STOPPED} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)[\n]"
16:54:13.513 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]c[2025-10-03 20:54:13,510] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)[\n]"
16:54:13.513 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.513 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,510] WARN o.e.j.s.ServletContextHandler@1c9b0314{/,null,STOPPED} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
16:54:13.513 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,510] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
16:54:13.521 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "c4[\r][\n]"
16:54:13.521 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffbc][2025-10-03 20:54:13,518] INFO jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.18+10-LTS (org.eclipse.jetty.server.Server)[\n]"
16:54:13.521 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.524 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,518] INFO jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.18+10-LTS (org.eclipse.jetty.server.Server)
16:54:13.535 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "e5[\r][\n]"
16:54:13.535 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]k[2025-10-03 20:54:13,532] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)[\n]"
16:54:13.535 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]j[2025-10-03 20:54:13,532] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)[\n]"
16:54:13.535 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.535 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "6a[\r][\n]"
16:54:13.535 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]b[2025-10-03 20:54:13,533] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)[\n]"
16:54:13.535 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "c3[\r][\n]"
16:54:13.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffbb][2025-10-03 20:54:13,535] WARN ServletContext@o.e.j.s.ServletContextHandler@1c9b0314{/,null,STARTING} has uncovered http methods for path: /* (org.eclipse.jetty.security.SecurityHandler)[\n]"
16:54:13.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.537 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,535] WARN ServletContext@o.e.j.s.ServletContextHandler@1c9b0314{/,null,STARTING} has uncovered http methods for path: /* (org.eclipse.jetty.security.SecurityHandler)
16:54:13.537 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,533] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
16:54:13.537 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,532] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
16:54:13.537 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,532] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
16:54:13.541 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "9a[\r][\n]"
16:54:13.542 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff92][2025-10-03 20:54:13,539] INFO Started o.e.j.s.ServletContextHandler@1c9b0314{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)[\n]"
16:54:13.542 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.549 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "99[\r][\n]"
16:54:13.549 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff91][2025-10-03 20:54:13,546] INFO Started ServerConnector@533bda92{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)[\n]"
16:54:13.549 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.549 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "100[\r][\n]"
16:54:13.549 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]P[2025-10-03 20:54:13,546] INFO Started @526ms (org.eclipse.jetty.server.Server)[\n]"
16:54:13.549 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa0][2025-10-03 20:54:13,546] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)[\n]"
16:54:13.549 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.549 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,546] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
16:54:13.549 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,546] INFO Started @526ms (org.eclipse.jetty.server.Server)
16:54:13.549 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,546] INFO Started ServerConnector@533bda92{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
16:54:13.549 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,539] INFO Started o.e.j.s.ServletContextHandler@1c9b0314{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
16:54:13.551 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "ab[\r][\n]"
16:54:13.551 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa3][2025-10-03 20:54:13,548] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)[\n]"
16:54:13.551 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.551 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8a[\r][\n]"
16:54:13.551 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff82][2025-10-03 20:54:13,549] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)[\n]"
16:54:13.551 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.552 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "e9[\r][\n]"
16:54:13.552 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffe1][2025-10-03 20:54:13,549] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 16 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)[\n]"
16:54:13.552 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.552 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7f[\r][\n]"
16:54:13.552 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]w[2025-10-03 20:54:13,550] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)[\n]"
16:54:13.552 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.562 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "22b[\r][\n]"
16:54:13.562 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9d][2025-10-03 20:54:13,557] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)[\n]"
16:54:13.562 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9d][2025-10-03 20:54:13,557] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)[\n]"
16:54:13.562 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]l[2025-10-03 20:54:13,557] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)[\n]"
16:54:13.562 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]e[2025-10-03 20:54:13,557] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)[\n]"
16:54:13.562 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.562 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,550] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
16:54:13.563 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,557] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
16:54:13.563 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,557] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)
16:54:13.563 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,557] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
16:54:13.563 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,557] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
16:54:13.563 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,549] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 16 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
16:54:13.563 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,549] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)
16:54:13.563 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,548] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
16:54:13.564 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "12e[\r][\n]"
16:54:13.564 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff84][2025-10-03 20:54:13,561] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)[\n]"
16:54:13.564 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9a][2025-10-03 20:54:13,561] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)[\n]"
16:54:13.564 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "93[\r][\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8b][2025-10-03 20:54:13,562] INFO Snapshot loaded in 5 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)[\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "a2[\r][\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9a][2025-10-03 20:54:13,562] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)[\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "6c[\r][\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]d[2025-10-03 20:54:13,563] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)[\n]"
16:54:13.565 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.570 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "96[\r][\n]"
16:54:13.571 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8e][2025-10-03 20:54:13,567] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)[\n]"
16:54:13.571 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.571 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8a[\r][\n]"
16:54:13.571 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff82][2025-10-03 20:54:13,567] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)[\n]"
16:54:13.571 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.581 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,567] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)
16:54:13.582 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "9e[\r][\n]"
16:54:13.581 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,567] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)
16:54:13.582 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff96][2025-10-03 20:54:13,574] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)[\n]"
16:54:13.582 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.582 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "71[\r][\n]"
16:54:13.582 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]i[2025-10-03 20:54:13,575] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)[\n]"
16:54:13.582 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,563] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)
16:54:13.582 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:13.582 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,562] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
16:54:13.582 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,562] INFO Snapshot loaded in 5 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)
16:54:13.582 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,561] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
16:54:13.582 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,574] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)
16:54:13.582 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,575] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)
16:54:13.582 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:13,561] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)
16:54:14.357 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "2b[\r][\n]"
16:54:14.357 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#===> Running preflight checks ... [\n]"
16:54:14.357 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.357 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: ===> Running preflight checks ... 
16:54:14.358 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1c[\r][\n]"
16:54:14.358 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x14]===> Launching ... [\n]"
16:54:14.358 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.361 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "22[\r][\n]"
16:54:14.361 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1a]===> Launching kafka ... [\n]"
16:54:14.361 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.370 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: ===> Launching kafka ... 
16:54:14.370 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: ===> Launching ... 
16:54:14.640 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "84[\r][\n]"
16:54:14.640 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]|[2025-10-03 20:54:14,636] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)[\n]"
16:54:14.640 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.640 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,636] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
16:54:14.787 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "b7[\r][\n]"
16:54:14.787 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffaf][2025-10-03 20:54:14,785] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)[\n]"
16:54:14.787 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.788 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,785] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
16:54:14.842 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8a[\r][\n]"
16:54:14.842 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff82][2025-10-03 20:54:14,839] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)[\n]"
16:54:14.842 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.842 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,839] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
16:54:14.842 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "4b[\r][\n]"
16:54:14.842 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]C[2025-10-03 20:54:14,840] INFO starting (kafka.server.KafkaServer)[\n]"
16:54:14.842 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.843 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,840] INFO starting (kafka.server.KafkaServer)
16:54:14.843 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "6c[\r][\n]"
16:54:14.843 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]d[2025-10-03 20:54:14,840] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)[\n]"
16:54:14.843 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.849 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "96[\r][\n]"
16:54:14.849 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8e][2025-10-03 20:54:14,847] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)[\n]"
16:54:14.849 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.851 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "294[\r][\n]"
16:54:14.851 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffb4][2025-10-03 20:54:14,849] INFO Client environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]j[2025-10-03 20:54:14,849] INFO Client environment:host.name=70e45fd582a9 (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]h[2025-10-03 20:54:14,849] INFO Client environment:java.version=11.0.18 (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]r[2025-10-03 20:54:14,849] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:14,849] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1d65[\r][\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x17][0xffffffec][2025-10-03 20:54:14,849] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.4.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jose4j-0.7.9.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.4.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jline-3.21.0.jar:/usr/bin/../share/java/kafka/connect-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/kafka-group-coordinator-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-mirror-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.4.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.86.Final.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.4.jar:/usr/bin/../share/java/kafka/kafka-raft-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/kafka-clients-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.6.0.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.4.jar:/usr/bin/../share/java/kafka/reload4j-1.2.19.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/audience-annotations-0.13.0.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.4.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/kafka-streams-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.4.2.jar:/usr/bin/../share/java/kafka/connect-transforms-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.4.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-storage-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-shell-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-json-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/connect-runtime-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/kafka-tools-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.2-1.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.86.Final.jar:/usr/bin/../share/java/kafka/trogdor-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.86.Final.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.4.0-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9c][2025-10-03 20:54:14,849] INFO Client environment:java.library.path=/usr/java/packages/lib:/lib:/usr/lib:/usr/lib64:/lib64 (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]g[2025-10-03 20:54:14,849] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]f[2025-10-03 20:54:14,849] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]a[2025-10-03 20:54:14,849] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]c[2025-10-03 20:54:14,849] INFO Client environment:os.arch=aarch64 (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]o[2025-10-03 20:54:14,849] INFO Client environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]e[2025-10-03 20:54:14,849] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]k[2025-10-03 20:54:14,849] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]j[2025-10-03 20:54:14,849] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]i[2025-10-03 20:54:14,849] INFO Client environment:os.memory.free=1011MB (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]h[2025-10-03 20:54:14,849] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]j[2025-10-03 20:54:14,849] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "e1[\r][\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffd9][2025-10-03 20:54:14,850] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@1e0f9063 (org.apache.zookeeper.ZooKeeper)[\n]"
16:54:14.852 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.855 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "75[\r][\n]"
16:54:14.855 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]m[2025-10-03 20:54:14,852] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)[\n]"
16:54:14.855 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,852] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,850] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@1e0f9063 (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:os.memory.free=1011MB (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:os.arch=aarch64 (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
16:54:14.855 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:java.library.path=/usr/java/packages/lib:/lib:/usr/lib:/usr/lib64:/lib64 (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.4.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jose4j-0.7.9.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.4.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jline-3.21.0.jar:/usr/bin/../share/java/kafka/connect-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/kafka-group-coordinator-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-mirror-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.4.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.86.Final.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.4.jar:/usr/bin/../share/java/kafka/kafka-raft-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/kafka-clients-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.6.0.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.4.jar:/usr/bin/../share/java/kafka/reload4j-1.2.19.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/audience-annotations-0.13.0.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.4.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/kafka-streams-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.4.2.jar:/usr/bin/../share/java/kafka/connect-transforms-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.4.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-storage-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/kafka-shell-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/connect-json-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.86.Final.jar:/usr/bin/../share/java/kafka/connect-runtime-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/kafka-tools-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.2-1.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.86.Final.jar:/usr/bin/../share/java/kafka/trogdor-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.86.Final.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.86.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.86.Final.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.4.0-ccs.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.4.0-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:java.version=11.0.18 (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:host.name=70e45fd582a9 (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,849] INFO Client environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT (org.apache.zookeeper.ZooKeeper)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,847] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
16:54:14.856 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,840] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
16:54:14.868 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "85[\r][\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]}[2025-10-03 20:54:14,854] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)[\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.869 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,854] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "81[\r][\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]y[2025-10-03 20:54:14,856] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)[\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8d[\r][\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff85][2025-10-03 20:54:14,856] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. (org.apache.zookeeper.ClientCnxn)[\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "c3[\r][\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffbb][2025-10-03 20:54:14,862] INFO Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:43470, server: localhost/0:0:0:0:0:0:0:1:2181 (org.apache.zookeeper.ClientCnxn)[\n]"
16:54:14.869 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.869 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,856] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. (org.apache.zookeeper.ClientCnxn)
16:54:14.869 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,862] INFO Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:43470, server: localhost/0:0:0:0:0:0:0:1:2181 (org.apache.zookeeper.ClientCnxn)
16:54:14.869 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,856] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
16:54:14.873 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "79[\r][\n]"
16:54:14.874 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]q[2025-10-03 20:54:14,870] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)[\n]"
16:54:14.874 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.881 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,870] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
16:54:14.886 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "cd[\r][\n]"
16:54:14.887 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffc5][2025-10-03 20:54:14,884] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, session id = 0x1000009f1630000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)[\n]"
16:54:14.887 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.888 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "73[\r][\n]"
16:54:14.888 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]k[2025-10-03 20:54:14,885] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)[\n]"
16:54:14.888 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:14.893 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,885] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
16:54:14.893 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:14,884] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, session id = 0x1000009f1630000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
16:54:15.015 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "66[\r][\n]"
16:54:15.015 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]^[2025-10-03 20:54:15,012] INFO Cluster ID = 68rYXGUVS86JmY125F2z-Q (kafka.server.KafkaServer)[\n]"
16:54:15.015 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.016 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "95[\r][\n]"
16:54:15.016 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8d][2025-10-03 20:54:15,013] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)[\n]"
16:54:15.016 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.019 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,013] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
16:54:15.020 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,012] INFO Cluster ID = 68rYXGUVS86JmY125F2z-Q (kafka.server.KafkaServer)
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7b7[\r][\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]4[2025-10-03 20:54:15,032] INFO KafkaConfig values: [\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]O[0x9]advertised.listeners = PLAINTEXT://localhost:50137,BROKER://70e45fd582a9:9092[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]alter.config.policy.class.name = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]2[0x9]alter.log.dirs.replication.quota.window.num = 11[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]:[0x9]alter.log.dirs.replication.quota.window.size.seconds = 1[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1a][0x9]authorizer.class.name = [\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]auto.create.topics.enable = true[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]auto.include.jmx.reporter = true[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]auto.leader.rebalance.enable = true[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]background.threads = 10[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]broker.heartbeat.interval.ms = 2000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xf][0x9]broker.id = 1[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]broker.id.generation.enable = true[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x14][0x9]broker.rack = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]broker.session.timeout.ms = 9000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]client.quota.callback.class = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1d][0x9]compression.type = producer[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]1[0x9]connection.failed.authentication.delay.ms = 100[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]connections.max.idle.ms = 600000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1f][0x9]connections.max.reauth.ms = 0[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]control.plane.listener.name = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]controlled.shutdown.enable = true[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]controlled.shutdown.max.retries = 3[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]controlled.shutdown.retry.backoff.ms = 5000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]controller.listener.names = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]controller.quorum.append.linger.ms = 25[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]2[0x9]controller.quorum.election.backoff.max.ms = 1000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0].[0x9]controller.quorum.election.timeout.ms = 1000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]controller.quorum.fetch.timeout.ms = 2000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]controller.quorum.request.timeout.ms = 2000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]controller.quorum.retry.backoff.ms = 20[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1f][0x9]controller.quorum.voters = [][\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]controller.quota.window.num = 11[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]controller.quota.window.size.seconds = 1[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]controller.socket.timeout.ms = 30000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]create.topic.policy.class.name = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]default.replication.factor = 1[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]5[0x9]delegation.token.expiry.check.interval.ms = 3600000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]delegation.token.expiry.time.ms = 86400000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]delegation.token.master.key = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0].[0x9]delegation.token.max.lifetime.ms = 604800000[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]delegation.token.secret.key = null[\n]"
16:54:15.035 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.035 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	advertised.listeners = PLAINTEXT://localhost:50137,BROKER://70e45fd582a9:9092
16:54:15.035 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	connection.failed.authentication.delay.ms = 100
16:54:15.035 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	connections.max.idle.ms = 600000
16:54:15.035 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controlled.shutdown.enable = true
16:54:15.035 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.listener.names = null
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quorum.election.timeout.ms = 1000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	delegation.token.secret.key = null
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	delegation.token.max.lifetime.ms = 604800000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	delegation.token.master.key = null
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	delegation.token.expiry.time.ms = 86400000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	delegation.token.expiry.check.interval.ms = 3600000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	default.replication.factor = 1
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	create.topic.policy.class.name = null
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.socket.timeout.ms = 30000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quota.window.size.seconds = 1
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quota.window.num = 11
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quorum.voters = []
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quorum.retry.backoff.ms = 20
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quorum.request.timeout.ms = 2000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quorum.fetch.timeout.ms = 2000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quorum.election.backoff.max.ms = 1000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controller.quorum.append.linger.ms = 25
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controlled.shutdown.retry.backoff.ms = 5000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	controlled.shutdown.max.retries = 3
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	control.plane.listener.name = null
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	connections.max.reauth.ms = 0
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	compression.type = producer
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	client.quota.callback.class = null
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	broker.session.timeout.ms = 9000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	broker.rack = null
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	broker.id.generation.enable = true
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	broker.id = 1
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	broker.heartbeat.interval.ms = 2000
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	background.threads = 10
16:54:15.036 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "519[\r][\n]"
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	auto.leader.rebalance.enable = true
16:54:15.036 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]6[0x9]delete.records.purgatory.purge.interval.requests = 1[\n]"
16:54:15.036 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1c][0x9]delete.topic.enable = true[\n]"
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	auto.include.jmx.reporter = true
16:54:15.036 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1e][0x9]early.start.listeners = null[\n]"
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	auto.create.topics.enable = true
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	authorizer.class.name = 
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	alter.log.dirs.replication.quota.window.size.seconds = 1
16:54:15.036 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	alter.log.dirs.replication.quota.window.num = 11
16:54:15.037 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	alter.config.policy.class.name = null
16:54:15.037 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,032] INFO KafkaConfig values: 
16:54:15.036 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1c][0x9]fetch.max.bytes = 57671680[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]0[0x9]fetch.purgatory.purge.interval.requests = 1000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]group.initial.rebalance.delay.ms = 0[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]([0x9]group.max.session.timeout.ms = 1800000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1d][0x9]group.max.size = 2147483647[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]group.min.session.timeout.ms = 6000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]0[0x9]initial.broker.registration.timeout.ms = 60000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]inter.broker.listener.name = BROKER[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]inter.broker.protocol.version = 3.4-IV0[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]kafka.metrics.polling.interval.secs = 10[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1e][0x9]kafka.metrics.reporters = [][\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]/[0x9]leader.imbalance.check.interval.seconds = 300[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]leader.imbalance.per.broker.percentage = 10[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]G[0x9]listener.security.protocol.map = BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]<[0x9]listeners = PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]log.cleaner.backoff.ms = 15000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]log.cleaner.dedupe.buffer.size = 134217728[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]log.cleaner.delete.retention.ms = 86400000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]log.cleaner.enable = true[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]log.cleaner.io.buffer.load.factor = 0.9[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]log.cleaner.io.buffer.size = 524288[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]>[0x9]log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]9[0x9]log.cleaner.max.compaction.lag.ms = 9223372036854775807[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "571[\r][\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]log.cleaner.min.cleanable.ratio = 0.5[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]log.cleaner.min.compaction.lag.ms = 0[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]log.cleaner.threads = 1[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1f][0x9]log.cleanup.policy = [delete][\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]log.dir = /tmp/kafka-logs[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]log.dirs = /var/lib/kafka/data[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]3[0x9]log.flush.interval.messages = 9223372036854775807[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1e][0x9]log.flush.interval.ms = null[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]1[0x9]log.flush.offset.checkpoint.interval.ms = 60000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]7[0x9]log.flush.scheduler.interval.ms = 9223372036854775807[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]7[0x9]log.flush.start.offset.checkpoint.interval.ms = 60000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]![0x9]log.index.interval.bytes = 4096[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]log.index.size.max.bytes = 10485760[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]log.message.downconversion.enable = true[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]log.message.format.version = 3.0-IV1[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]?[0x9]log.message.timestamp.difference.max.ms = 9223372036854775807[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]log.message.timestamp.type = CreateTime[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]log.preallocate = false[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1a][0x9]log.retention.bytes = -1[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]log.retention.check.interval.ms = 300000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]log.retention.hours = 168[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1e][0x9]log.retention.minutes = null[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]log.retention.ms = null[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x16][0x9]log.roll.hours = 168[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]log.roll.jitter.hours = 0[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]log.roll.jitter.ms = null[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x14][0x9]log.roll.ms = null[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]log.segment.bytes = 1073741824[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]log.segment.delete.delay.ms = 60000[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]max.connection.creation.rate = 2147483647[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1e][0x9]max.connections = 2147483647[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]max.connections.per.ip = 2147483647[\n]"
16:54:15.037 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "61d[\r][\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]max.connections.per.ip.overrides = [\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]2[0x9]max.incremental.fetch.session.cache.slots = 1000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1d][0x9]message.max.bytes = 1048588[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]metadata.log.dir = null[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]<[0x9]metadata.log.max.record.bytes.between.snapshots = 20971520[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]1[0x9]metadata.log.max.snapshot.interval.ms = 3600000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]metadata.log.segment.bytes = 1073741824[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]metadata.log.segment.min.bytes = 8388608[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]metadata.log.segment.ms = 604800000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]metadata.max.idle.interval.ms = 500[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]metadata.max.retention.bytes = 104857600[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]metadata.max.retention.ms = 604800000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x17][0x9]metric.reporters = [][\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]metrics.num.samples = 2[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]metrics.recording.level = INFO[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]metrics.sample.window.ms = 30000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]min.insync.replicas = 1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][\r][0x9]node.id = 1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x14][0x9]num.io.threads = 8[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]num.network.threads = 3[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x14][0x9]num.partitions = 3[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]num.recovery.threads.per.data.dir = 1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]num.replica.alter.log.dirs.threads = null[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1a][0x9]num.replica.fetchers = 1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]offset.metadata.max.bytes = 4096[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]offsets.commit.required.acks = -1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]offsets.commit.timeout.ms = 5000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]offsets.load.buffer.size = 5242880[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0].[0x9]offsets.retention.check.interval.ms = 600000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]offsets.retention.minutes = 10080[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]offsets.topic.compression.codec = 0[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]offsets.topic.num.partitions = 1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]offsets.topic.replication.factor = 1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]offsets.topic.segment.bytes = 104857600[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]:[0x9]password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]password.encoder.iterations = 4096[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1549[\r][\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]password.encoder.key.length = 128[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0].[0x9]password.encoder.keyfactory.algorithm = null[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]password.encoder.old.secret = null[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]password.encoder.secret = null[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]m[0x9]principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x14][0x9]process.roles = [][\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]3[0x9]producer.id.expiration.check.interval.ms = 600000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]producer.id.expiration.ms = 86400000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]3[0x9]producer.purgatory.purge.interval.requests = 1000[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1f][0x9]queued.max.request.bytes = -1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]queued.max.requests = 500[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x17][0x9]quota.window.num = 11[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1f][0x9]quota.window.size.seconds = 1[\n]"
16:54:15.040 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0];[0x9]remote.log.index.file.cache.total.size.bytes = 1073741824[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]remote.log.manager.task.interval.ms = 30000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]6[0x9]remote.log.manager.task.retry.backoff.max.ms = 30000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]0[0x9]remote.log.manager.task.retry.backoff.ms = 500[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]remote.log.manager.task.retry.jitter = 0.2[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]remote.log.manager.thread.pool.size = 10[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]/[0x9]remote.log.metadata.manager.class.name = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]/[0x9]remote.log.metadata.manager.class.path = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]0[0x9]remote.log.metadata.manager.impl.prefix = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]2[0x9]remote.log.metadata.manager.listener.name = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]remote.log.reader.max.pending.tasks = 100[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]remote.log.reader.threads = 10[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0].[0x9]remote.log.storage.manager.class.name = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0].[0x9]remote.log.storage.manager.class.path = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]/[0x9]remote.log.storage.manager.impl.prefix = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]remote.log.storage.system.enable = false[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]![0x9]replica.fetch.backoff.ms = 1000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]replica.fetch.max.bytes = 1048576[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1d][0x9]replica.fetch.min.bytes = 1[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]replica.fetch.response.max.bytes = 10485760[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]![0x9]replica.fetch.wait.max.ms = 500[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]6[0x9]replica.high.watermark.checkpoint.interval.ms = 5000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]![0x9]replica.lag.time.max.ms = 30000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1f][0x9]replica.selector.class = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]replica.socket.receive.buffer.bytes = 65536[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]replica.socket.timeout.ms = 30000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]replication.quota.window.num = 11[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]replication.quota.window.size.seconds = 1[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1c][0x9]request.timeout.ms = 30000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1f][0x9]reserved.broker.max.id = 1000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]sasl.client.callback.handler.class = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]sasl.enabled.mechanisms = [GSSAPI][\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]sasl.jaas.config = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]sasl.kerberos.kinit.cmd = /usr/bin/kinit[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]/[0x9]sasl.kerberos.min.time.before.relogin = 60000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]4[0x9]sasl.kerberos.principal.to.local.rules = [DEFAULT][\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]sasl.kerberos.service.name = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]sasl.kerberos.ticket.renew.jitter = 0.05[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]0[0x9]sasl.kerberos.ticket.renew.window.factor = 0.8[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]sasl.login.callback.handler.class = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]sasl.login.class = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]sasl.login.connect.timeout.ms = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]sasl.login.read.timeout.ms = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]sasl.login.refresh.buffer.seconds = 300[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]sasl.login.refresh.min.period.seconds = 60[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]([0x9]sasl.login.refresh.window.factor = 0.8[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]sasl.login.refresh.window.jitter = 0.05[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]sasl.login.retry.backoff.max.ms = 10000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]sasl.login.retry.backoff.ms = 100[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]sasl.mechanism.controller.protocol = GSSAPI[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]/[0x9]sasl.mechanism.inter.broker.protocol = GSSAPI[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]sasl.oauthbearer.clock.skew.seconds = 30[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]sasl.oauthbearer.expected.audience = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]sasl.oauthbearer.expected.issuer = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]5[0x9]sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]=[0x9]sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]7[0x9]sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]sasl.oauthbearer.jwks.endpoint.url = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]sasl.oauthbearer.scope.claim.name = scope[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]sasl.oauthbearer.sub.claim.name = sub[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]sasl.oauthbearer.token.endpoint.url = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]sasl.server.callback.handler.class = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]sasl.server.max.receive.size = 524288[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]security.inter.broker.protocol = PLAINTEXT[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]security.providers = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]0[0x9]socket.connection.setup.timeout.max.ms = 30000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]socket.connection.setup.timeout.ms = 10000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]![0x9]socket.listen.backlog.size = 50[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]socket.receive.buffer.bytes = 102400[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]socket.request.max.bytes = 104857600[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]socket.send.buffer.bytes = 102400[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x18][0x9]ssl.cipher.suites = [][\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x18][0x9]ssl.client.auth = none[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]ssl.enabled.protocols = [TLSv1.2, TLSv1.3][\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]/[0x9]ssl.endpoint.identification.algorithm = https[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]![0x9]ssl.engine.factory.class = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]ssl.key.password = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]ssl.keymanager.algorithm = SunX509[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]ssl.keystore.certificate.chain = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]ssl.keystore.key = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1e][0x9]ssl.keystore.location = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1e][0x9]ssl.keystore.password = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x19][0x9]ssl.keystore.type = JKS[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]ssl.principal.mapping.rules = DEFAULT[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x18][0x9]ssl.protocol = TLSv1.3[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x15][0x9]ssl.provider = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0])[0x9]ssl.secure.random.implementation = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]ssl.trustmanager.algorithm = PKIX[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]ssl.truststore.certificates = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]ssl.truststore.location = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0] [0x9]ssl.truststore.password = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]ssl.truststore.type = JKS[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]E[0x9]transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]transaction.max.timeout.ms = 900000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]F[0x9]transaction.remove.expired.transaction.cleanup.interval.ms = 3600000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]2[0x9]transaction.state.log.load.buffer.size = 5242880[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]transaction.state.log.min.isr = 1[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]+[0x9]transaction.state.log.num.partitions = 50[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0].[0x9]transaction.state.log.replication.factor = 1[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]1[0x9]transaction.state.log.segment.bytes = 104857600[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "42b[\r][\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0],[0x9]transactional.id.expiration.ms = 604800000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]([0x9]unclean.leader.election.enable = false[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]zookeeper.clientCnxnSocket = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]zookeeper.connect = localhost:2181[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]([0x9]zookeeper.connection.timeout.ms = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]'[0x9]zookeeper.max.in.flight.requests = 10[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]-[0x9]zookeeper.metadata.migration.enable = false[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]zookeeper.session.timeout.ms = 18000[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1b][0x9]zookeeper.set.acl = false[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]zookeeper.ssl.cipher.suites = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]%[0x9]zookeeper.ssl.client.enable = false[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]zookeeper.ssl.crl.enable = false[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]([0x9]zookeeper.ssl.enabled.protocols = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]9[0x9]zookeeper.ssl.endpoint.identification.algorithm = HTTPS[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]([0x9]zookeeper.ssl.keystore.location = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]([0x9]zookeeper.ssl.keystore.password = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]$[0x9]zookeeper.ssl.keystore.type = null[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]#[0x9]zookeeper.ssl.ocsp.enable = false[\n]"
16:54:15.041 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]"[0x9]zookeeper.ssl.protocol = TLSv1.2[\n]"
16:54:15.042 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]zookeeper.ssl.truststore.location = null[\n]"
16:54:15.042 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]*[0x9]zookeeper.ssl.truststore.password = null[\n]"
16:54:15.042 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]&[0x9]zookeeper.ssl.truststore.type = null[\n]"
16:54:15.042 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1c] (kafka.server.KafkaConfig)[\n]"
16:54:15.042 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT:  (kafka.server.KafkaConfig)
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.truststore.type = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.truststore.password = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.truststore.location = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.protocol = TLSv1.2
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.ocsp.enable = false
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.keystore.type = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.keystore.password = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.keystore.location = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.enabled.protocols = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.crl.enable = false
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.client.enable = false
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.ssl.cipher.suites = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.set.acl = false
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.session.timeout.ms = 18000
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.metadata.migration.enable = false
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.max.in.flight.requests = 10
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.connection.timeout.ms = null
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.connect = localhost:2181
16:54:15.049 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	zookeeper.clientCnxnSocket = null
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	unclean.leader.election.enable = false
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transactional.id.expiration.ms = 604800000
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.state.log.segment.bytes = 104857600
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.state.log.replication.factor = 1
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.state.log.num.partitions = 50
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.state.log.min.isr = 1
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.state.log.load.buffer.size = 5242880
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.max.timeout.ms = 900000
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.truststore.type = JKS
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.truststore.password = null
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.truststore.location = null
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.truststore.certificates = null
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.trustmanager.algorithm = PKIX
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.secure.random.implementation = null
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.provider = null
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.protocol = TLSv1.3
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.principal.mapping.rules = DEFAULT
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.keystore.type = JKS
16:54:15.050 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.keystore.password = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.keystore.location = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.keystore.key = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.keystore.certificate.chain = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.keymanager.algorithm = SunX509
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.key.password = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.engine.factory.class = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.endpoint.identification.algorithm = https
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.client.auth = none
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	ssl.cipher.suites = []
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	socket.send.buffer.bytes = 102400
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	socket.request.max.bytes = 104857600
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	socket.receive.buffer.bytes = 102400
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	socket.listen.backlog.size = 50
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	socket.connection.setup.timeout.ms = 10000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	socket.connection.setup.timeout.max.ms = 30000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	security.providers = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	security.inter.broker.protocol = PLAINTEXT
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.server.max.receive.size = 524288
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.server.callback.handler.class = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.token.endpoint.url = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.sub.claim.name = sub
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.scope.claim.name = scope
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.jwks.endpoint.url = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.expected.issuer = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.expected.audience = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.oauthbearer.clock.skew.seconds = 30
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.mechanism.inter.broker.protocol = GSSAPI
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.mechanism.controller.protocol = GSSAPI
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.retry.backoff.ms = 100
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.retry.backoff.max.ms = 10000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.refresh.window.jitter = 0.05
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.refresh.window.factor = 0.8
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.refresh.min.period.seconds = 60
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.refresh.buffer.seconds = 300
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.read.timeout.ms = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.connect.timeout.ms = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.class = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.login.callback.handler.class = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.kerberos.ticket.renew.window.factor = 0.8
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.kerberos.ticket.renew.jitter = 0.05
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.kerberos.service.name = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.kerberos.min.time.before.relogin = 60000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.jaas.config = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.enabled.mechanisms = [GSSAPI]
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	sasl.client.callback.handler.class = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	reserved.broker.max.id = 1000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	request.timeout.ms = 30000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replication.quota.window.size.seconds = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replication.quota.window.num = 11
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.socket.timeout.ms = 30000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.socket.receive.buffer.bytes = 65536
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.selector.class = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.lag.time.max.ms = 30000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.high.watermark.checkpoint.interval.ms = 5000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.fetch.wait.max.ms = 500
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.fetch.response.max.bytes = 10485760
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.fetch.min.bytes = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.fetch.max.bytes = 1048576
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	replica.fetch.backoff.ms = 1000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.storage.system.enable = false
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.storage.manager.impl.prefix = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.storage.manager.class.path = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.storage.manager.class.name = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.reader.threads = 10
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.reader.max.pending.tasks = 100
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.metadata.manager.listener.name = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.metadata.manager.impl.prefix = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.metadata.manager.class.path = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.metadata.manager.class.name = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.manager.thread.pool.size = 10
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.manager.task.retry.jitter = 0.2
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.manager.task.retry.backoff.ms = 500
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.manager.task.retry.backoff.max.ms = 30000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.manager.task.interval.ms = 30000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	remote.log.index.file.cache.total.size.bytes = 1073741824
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	quota.window.size.seconds = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	quota.window.num = 11
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	queued.max.requests = 500
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	queued.max.request.bytes = -1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	producer.purgatory.purge.interval.requests = 1000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	producer.id.expiration.ms = 86400000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	producer.id.expiration.check.interval.ms = 600000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	process.roles = []
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	password.encoder.secret = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	password.encoder.old.secret = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	password.encoder.keyfactory.algorithm = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	password.encoder.key.length = 128
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	password.encoder.iterations = 4096
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.topic.segment.bytes = 104857600
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.topic.replication.factor = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.topic.num.partitions = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.topic.compression.codec = 0
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.retention.minutes = 10080
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.retention.check.interval.ms = 600000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.load.buffer.size = 5242880
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.commit.timeout.ms = 5000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offsets.commit.required.acks = -1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	offset.metadata.max.bytes = 4096
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	num.replica.fetchers = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	num.replica.alter.log.dirs.threads = null
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	num.recovery.threads.per.data.dir = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	num.partitions = 3
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	num.network.threads = 3
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	num.io.threads = 8
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	node.id = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	min.insync.replicas = 1
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metrics.sample.window.ms = 30000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metrics.recording.level = INFO
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metrics.num.samples = 2
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metric.reporters = []
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.max.retention.ms = 604800000
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.max.retention.bytes = 104857600
16:54:15.051 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.max.idle.interval.ms = 500
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.log.segment.ms = 604800000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.log.segment.min.bytes = 8388608
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.log.segment.bytes = 1073741824
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.log.max.snapshot.interval.ms = 3600000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.log.max.record.bytes.between.snapshots = 20971520
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	metadata.log.dir = null
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	message.max.bytes = 1048588
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	max.incremental.fetch.session.cache.slots = 1000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	max.connections.per.ip.overrides = 
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	max.connections.per.ip = 2147483647
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	max.connections = 2147483647
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	max.connection.creation.rate = 2147483647
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.segment.delete.delay.ms = 60000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.segment.bytes = 1073741824
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.roll.ms = null
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.roll.jitter.ms = null
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.roll.jitter.hours = 0
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.roll.hours = 168
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.retention.ms = null
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.retention.minutes = null
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.retention.hours = 168
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.retention.check.interval.ms = 300000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.retention.bytes = -1
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.preallocate = false
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.message.timestamp.type = CreateTime
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.message.timestamp.difference.max.ms = 9223372036854775807
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.message.format.version = 3.0-IV1
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.message.downconversion.enable = true
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.index.size.max.bytes = 10485760
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.index.interval.bytes = 4096
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.flush.start.offset.checkpoint.interval.ms = 60000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.flush.scheduler.interval.ms = 9223372036854775807
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.flush.offset.checkpoint.interval.ms = 60000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.flush.interval.ms = null
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.flush.interval.messages = 9223372036854775807
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.dirs = /var/lib/kafka/data
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.dir = /tmp/kafka-logs
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleanup.policy = [delete]
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.threads = 1
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.min.compaction.lag.ms = 0
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.min.cleanable.ratio = 0.5
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.io.buffer.size = 524288
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.io.buffer.load.factor = 0.9
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.enable = true
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.delete.retention.ms = 86400000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.dedupe.buffer.size = 134217728
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	log.cleaner.backoff.ms = 15000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	listeners = PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	listener.security.protocol.map = BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	leader.imbalance.per.broker.percentage = 10
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	leader.imbalance.check.interval.seconds = 300
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	kafka.metrics.reporters = []
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	kafka.metrics.polling.interval.secs = 10
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	inter.broker.protocol.version = 3.4-IV0
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	inter.broker.listener.name = BROKER
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	initial.broker.registration.timeout.ms = 60000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	group.min.session.timeout.ms = 6000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	group.max.size = 2147483647
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	group.max.session.timeout.ms = 1800000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	group.initial.rebalance.delay.ms = 0
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	fetch.purgatory.purge.interval.requests = 1000
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	fetch.max.bytes = 57671680
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	early.start.listeners = null
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	delete.topic.enable = true
16:54:15.052 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: 	delete.records.purgatory.purge.interval.requests = 1
16:54:15.055 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "89[\r][\n]"
16:54:15.055 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff81][2025-10-03 20:54:15,052] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[\n]"
16:54:15.055 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.055 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8b[\r][\n]"
16:54:15.055 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff83][2025-10-03 20:54:15,052] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[\n]"
16:54:15.055 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.056 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8b[\r][\n]"
16:54:15.056 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff83][2025-10-03 20:54:15,053] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[\n]"
16:54:15.056 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.058 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "96[\r][\n]"
16:54:15.058 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8e][2025-10-03 20:54:15,055] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[\n]"
16:54:15.058 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.064 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,055] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
16:54:15.064 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,053] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
16:54:15.064 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,052] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
16:54:15.064 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,052] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
16:54:15.074 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "77[\r][\n]"
16:54:15.075 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]o[2025-10-03 20:54:15,072] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)[\n]"
16:54:15.075 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.075 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "9d[\r][\n]"
16:54:15.075 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff95][2025-10-03 20:54:15,073] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)[\n]"
16:54:15.075 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.076 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,073] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)
16:54:15.076 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,072] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
16:54:15.082 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "c3[\r][\n]"
16:54:15.082 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]L[2025-10-03 20:54:15,080] INFO Loaded 0 logs in 8ms. (kafka.log.LogManager)[\n]"
16:54:15.082 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]g[2025-10-03 20:54:15,080] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)[\n]"
16:54:15.082 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.085 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "84[\r][\n]"
16:54:15.085 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]|[2025-10-03 20:54:15,083] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)[\n]"
16:54:15.085 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.087 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,083] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
16:54:15.087 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,080] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
16:54:15.087 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,080] INFO Loaded 0 logs in 8ms. (kafka.log.LogManager)
16:54:15.090 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "57[\r][\n]"
16:54:15.090 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]O[2025-10-03 20:54:15,088] INFO Starting the log cleaner (kafka.log.LogCleaner)[\n]"
16:54:15.090 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.103 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,088] INFO Starting the log cleaner (kafka.log.LogCleaner)
16:54:15.231 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "65[\r][\n]"
16:54:15.231 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]][2025-10-03 20:54:15,228] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)[\n]"
16:54:15.231 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.231 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,228] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
16:54:15.247 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "a8[\r][\n]"
16:54:15.247 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa0][2025-10-03 20:54:15,244] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)[\n]"
16:54:15.247 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.247 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,244] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
16:54:15.252 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "86[\r][\n]"
16:54:15.252 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]~[2025-10-03 20:54:15,249] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)[\n]"
16:54:15.252 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.260 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,249] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
16:54:15.276 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "9c[\r][\n]"
16:54:15.276 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff94][2025-10-03 20:54:15,273] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)[\n]"
16:54:15.276 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.276 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,273] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
16:54:15.452 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "92[\r][\n]"
16:54:15.452 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8a][2025-10-03 20:54:15,448] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)[\n]"
16:54:15.452 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.452 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,448] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
16:54:15.455 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "76[\r][\n]"
16:54:15.455 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]n[2025-10-03 20:54:15,452] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)[\n]"
16:54:15.455 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.464 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,452] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)
16:54:15.473 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "158[\r][\n]"
16:54:15.473 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffbe][2025-10-03 20:54:15,464] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)[\n]"
16:54:15.473 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8a][2025-10-03 20:54:15,465] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)[\n]"
16:54:15.473 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.474 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "139[\r][\n]"
16:54:15.474 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]n[2025-10-03 20:54:15,465] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)[\n]"
16:54:15.474 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffbb][2025-10-03 20:54:15,467] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(BROKER) (kafka.network.SocketServer)[\n]"
16:54:15.474 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.475 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "a0[\r][\n]"
16:54:15.475 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff98][2025-10-03 20:54:15,472] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)[\n]"
16:54:15.475 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.475 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,472] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
16:54:15.475 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,467] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(BROKER) (kafka.network.SocketServer)
16:54:15.475 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,465] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
16:54:15.475 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,465] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
16:54:15.475 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,464] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
16:54:15.493 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8e[\r][\n]"
16:54:15.493 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff86][2025-10-03 20:54:15,489] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.493 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.493 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,489] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.494 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8c[\r][\n]"
16:54:15.494 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff84][2025-10-03 20:54:15,491] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.494 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.495 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "94[\r][\n]"
16:54:15.495 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8c][2025-10-03 20:54:15,493] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.495 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.497 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "92[\r][\n]"
16:54:15.497 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8a][2025-10-03 20:54:15,494] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.497 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.505 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,494] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.505 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,493] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.505 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,491] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.506 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7b[\r][\n]"
16:54:15.506 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]s[2025-10-03 20:54:15,503] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)[\n]"
16:54:15.506 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.518 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,503] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
16:54:15.522 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "6e[\r][\n]"
16:54:15.522 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]f[2025-10-03 20:54:15,516] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)[\n]"
16:54:15.522 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.528 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,516] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
16:54:15.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "188[\r][\n]"
16:54:15.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff92][2025-10-03 20:54:15,532] INFO Stat of the created znode at /brokers/ids/1 is: 25,25,1759524855527,1759524855527,1,0,0,72057636742430720,254,0,25[\n]"
16:54:15.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0x1a] (kafka.zk.KafkaZkClient)[\n]"
16:54:15.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffc4][2025-10-03 20:54:15,533] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://localhost:50137,BROKER://70e45fd582a9:9092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)[\n]"
16:54:15.537 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.539 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,533] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://localhost:50137,BROKER://70e45fd582a9:9092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)
16:54:15.539 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT:  (kafka.zk.KafkaZkClient)
16:54:15.539 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,532] INFO Stat of the created znode at /brokers/ids/1 is: 25,25,1759524855527,1759524855527,1,0,0,72057636742430720,254,0,25
16:54:15.630 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "97[\r][\n]"
16:54:15.630 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8f][2025-10-03 20:54:15,626] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)[\n]"
16:54:15.630 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.630 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,626] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
16:54:15.635 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8c[\r][\n]"
16:54:15.635 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff84][2025-10-03 20:54:15,632] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.635 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.639 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "120[\r][\n]"
16:54:15.639 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff88][2025-10-03 20:54:15,636] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.639 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff88][2025-10-03 20:54:15,636] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.639 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.642 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,636] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.642 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,636] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.642 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,632] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.643 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7c[\r][\n]"
16:54:15.643 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:15,639] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)[\n]"
16:54:15.643 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.653 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,639] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
16:54:15.654 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "bb[\r][\n]"
16:54:15.654 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffb3][2025-10-03 20:54:15,649] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)[\n]"
16:54:15.654 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.659 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "75[\r][\n]"
16:54:15.659 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]m[2025-10-03 20:54:15,656] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)[\n]"
16:54:15.659 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.659 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "b1[\r][\n]"
16:54:15.659 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffa9][2025-10-03 20:54:15,656] INFO [Controller id=1] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map()) (kafka.controller.KafkaController)[\n]"
16:54:15.659 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.663 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,656] INFO [Controller id=1] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map()) (kafka.controller.KafkaController)
16:54:15.664 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,656] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
16:54:15.664 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,649] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
16:54:15.664 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7f[\r][\n]"
16:54:15.664 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]w[2025-10-03 20:54:15,660] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)[\n]"
16:54:15.664 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.666 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7a[\r][\n]"
16:54:15.666 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]r[2025-10-03 20:54:15,663] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)[\n]"
16:54:15.666 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.675 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,663] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
16:54:15.675 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,660] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
16:54:15.683 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "89[\r][\n]"
16:54:15.683 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff81][2025-10-03 20:54:15,679] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)[\n]"
16:54:15.683 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.687 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,679] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
16:54:15.689 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8e[\r][\n]"
16:54:15.689 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff86][2025-10-03 20:54:15,686] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)[\n]"
16:54:15.689 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.690 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "71[\r][\n]"
16:54:15.690 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]i[2025-10-03 20:54:15,687] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)[\n]"
16:54:15.690 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.691 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "98[\r][\n]"
16:54:15.692 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff90][2025-10-03 20:54:15,688] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)[\n]"
16:54:15.692 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.692 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "cc[\r][\n]"
16:54:15.692 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffc4][2025-10-03 20:54:15,688] INFO [MetadataCache brokerId=1] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)[\n]"
16:54:15.692 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.698 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "81[\r][\n]"
16:54:15.698 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]y[2025-10-03 20:54:15,695] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)[\n]"
16:54:15.698 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.699 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,695] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
16:54:15.699 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,688] INFO [MetadataCache brokerId=1] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)
16:54:15.699 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,688] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
16:54:15.699 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,687] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
16:54:15.699 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,686] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
16:54:15.700 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7e[\r][\n]"
16:54:15.700 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]v[2025-10-03 20:54:15,697] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)[\n]"
16:54:15.700 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.702 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7c[\r][\n]"
16:54:15.702 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:15,699] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)[\n]"
16:54:15.702 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.711 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,699] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
16:54:15.711 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,697] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
16:54:15.713 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8e[\r][\n]"
16:54:15.713 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff86][2025-10-03 20:54:15,710] INFO [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 25) (kafka.controller.KafkaController)[\n]"
16:54:15.713 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.722 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "8d[\r][\n]"
16:54:15.722 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff85][2025-10-03 20:54:15,718] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)[\n]"
16:54:15.722 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.723 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "90[\r][\n]"
16:54:15.723 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff88][2025-10-03 20:54:15,720] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[\n]"
16:54:15.723 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.723 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,720] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
16:54:15.723 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,718] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
16:54:15.723 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,710] INFO [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 25) (kafka.controller.KafkaController)
16:54:15.729 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "a2[\r][\n]"
16:54:15.729 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9a][2025-10-03 20:54:15,724] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)[\n]"
16:54:15.729 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.735 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "78[\r][\n]"
16:54:15.736 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]p[2025-10-03 20:54:15,733] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)[\n]"
16:54:15.736 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.736 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,733] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
16:54:15.736 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,724] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
16:54:15.737 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "22b[\r][\n]"
16:54:15.737 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff84][2025-10-03 20:54:15,734] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)[\n]"
16:54:15.737 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff89][2025-10-03 20:54:15,734] INFO [Controller id=1] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)[\n]"
16:54:15.737 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff85][2025-10-03 20:54:15,734] INFO [Controller id=1] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)[\n]"
16:54:15.737 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]y[2025-10-03 20:54:15,734] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)[\n]"
16:54:15.737 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.744 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7b[\r][\n]"
16:54:15.744 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]s[2025-10-03 20:54:15,741] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)[\n]"
16:54:15.744 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.747 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1cb[\r][\n]"
16:54:15.747 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]}[2025-10-03 20:54:15,742] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)[\n]"
16:54:15.747 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]x[2025-10-03 20:54:15,743] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)[\n]"
16:54:15.747 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffbe][2025-10-03 20:54:15,743] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)[\n]"
16:54:15.747 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,743] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,743] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,742] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,741] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,734] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,734] INFO [Controller id=1] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,734] INFO [Controller id=1] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
16:54:15.747 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,734] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
16:54:15.748 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7c[\r][\n]"
16:54:15.748 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]t[2025-10-03 20:54:15,745] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)[\n]"
16:54:15.748 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.754 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "13e[\r][\n]"
16:54:15.754 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff95][2025-10-03 20:54:15,749] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions (state.change.logger)[\n]"
16:54:15.754 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff99][2025-10-03 20:54:15,751] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)[\n]"
16:54:15.754 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.759 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "12d[\r][\n]"
16:54:15.759 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff88][2025-10-03 20:54:15,756] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)[\n]"
16:54:15.759 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff95][2025-10-03 20:54:15,756] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)[\n]"
16:54:15.759 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.760 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,756] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
16:54:15.760 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,756] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
16:54:15.760 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,751] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
16:54:15.760 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,749] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions (state.change.logger)
16:54:15.760 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,745] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "3d3[\r][\n]"
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff96][2025-10-03 20:54:15,760] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)[\n]"
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffac][2025-10-03 20:54:15,760] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)[\n]"
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8e][2025-10-03 20:54:15,760] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)[\n]"
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9b][2025-10-03 20:54:15,760] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)[\n]"
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffb2][2025-10-03 20:54:15,763] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)[\n]"
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff86][2025-10-03 20:54:15,763] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)[\n]"
16:54:15.767 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.771 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "d0[\r][\n]"
16:54:15.771 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffc8][2025-10-03 20:54:15,767] INFO [RequestSendThread controllerId=1] Controller 1 connected to 70e45fd582a9:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)[\n]"
16:54:15.771 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.772 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,767] INFO [RequestSendThread controllerId=1] Controller 1 connected to 70e45fd582a9:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
16:54:15.772 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,763] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
16:54:15.772 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,763] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
16:54:15.772 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,760] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
16:54:15.772 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,760] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
16:54:15.772 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,760] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
16:54:15.772 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,760] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
16:54:15.780 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "1c9[\r][\n]"
16:54:15.780 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff87][2025-10-03 20:54:15,776] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)[\n]"
16:54:15.780 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff8b][2025-10-03 20:54:15,776] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)[\n]"
16:54:15.780 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9f][2025-10-03 20:54:15,776] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)[\n]"
16:54:15.780 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.781 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "91[\r][\n]"
16:54:15.781 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff89][2025-10-03 20:54:15,777] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)[\n]"
16:54:15.781 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.782 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "b2[\r][\n]"
16:54:15.782 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffffaa][2025-10-03 20:54:15,777] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)[\n]"
16:54:15.782 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.783 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,777] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
16:54:15.783 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,777] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
16:54:15.783 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,776] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
16:54:15.783 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,776] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
16:54:15.783 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,776] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
16:54:15.797 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "91[\r][\n]"
16:54:15.797 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff89][2025-10-03 20:54:15,791] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)[\n]"
16:54:15.797 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.797 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,791] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
16:54:15.801 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "7e[\r][\n]"
16:54:15.801 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]v[2025-10-03 20:54:15,796] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)[\n]"
16:54:15.801 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.809 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,796] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
16:54:15.813 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "fc[\r][\n]"
16:54:15.813 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]f[2025-10-03 20:54:15,811] INFO Kafka version: 7.4.0-ccs (org.apache.kafka.common.utils.AppInfoParser)[\n]"
16:54:15.813 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff86][2025-10-03 20:54:15,811] INFO Kafka commitId: 30969fa33c185e880b9e02044761dfaac013151d (org.apache.kafka.common.utils.AppInfoParser)[\n]"
16:54:15.813 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.814 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "76[\r][\n]"
16:54:15.814 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]n[2025-10-03 20:54:15,811] INFO Kafka startTimeMs: 1759524855800 (org.apache.kafka.common.utils.AppInfoParser)[\n]"
16:54:15.814 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.815 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "5d[\r][\n]"
16:54:15.815 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[0x1][0x0][0x0][0x0][0x0][0x0][0x0]U[2025-10-03 20:54:15,812] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)[\n]"
16:54:15.815 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[\r][\n]"
16:54:15.820 [main] DEBUG org.testcontainers.containers.output.WaitingConsumer -- STDOUT: [2025-10-03 20:54:15,812] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
16:54:15.820 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000F: cancel
16:54:15.820 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultManagedHttpClientConnection -- http-outgoing-2: close connection IMMEDIATE
16:54:15.820 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[read] I/O error: null"
16:54:15.820 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000F: endpoint closed
16:54:15.820 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000F: discarding endpoint
16:54:15.820 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-2 << "[read] I/O error: null"
16:54:15.820 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000F: releasing endpoint
16:54:15.820 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000F: connection is not kept alive
16:54:15.820 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000F: connection released [route: {}->unix://localhost:2375][total available: 0; route allocated: 0 of 2147483647; total allocated: 0 of 2147483647]
16:54:15.820 [main] INFO tc.confluentinc/cp-kafka:7.4.0 -- Container confluentinc/cp-kafka:7.4.0 started in PT3.148376S
16:54:15.820 [docker-java-stream-1995971659] DEBUG com.github.dockerjava.zerodep.ApacheDockerHttpClientImpl$ApacheResponse -- Failed to close the response
java.nio.channels.ClosedChannelException: null
	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:222)
	at java.base/sun.nio.ch.SocketChannelImpl.blockingRead(SocketChannelImpl.java:1311)
	at java.base/sun.nio.ch.SocketInputStream.read(SocketInputStream.java:70)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.LoggingInputStream.read(LoggingInputStream.java:81)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:149)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:280)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.getChunkSize(ChunkedInputStream.java:261)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.nextChunk(ChunkedInputStream.java:222)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:147)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.ChunkedInputStream.close(ChunkedInputStream.java:314)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.io.Closer.close(Closer.java:48)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.impl.io.IncomingHttpEntity.close(IncomingHttpEntity.java:111)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.io.entity.HttpEntityWrapper.close(HttpEntityWrapper.java:120)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.io.Closer.close(Closer.java:48)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.core5.http.message.BasicClassicHttpResponse.close(BasicClassicHttpResponse.java:93)
	at com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.CloseableHttpResponse.close(CloseableHttpResponse.java:200)
	at com.github.dockerjava.zerodep.ApacheDockerHttpClientImpl$ApacheResponse.close(ApacheDockerHttpClientImpl.java:256)
	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$executeAndStream$1(DefaultInvocationBuilder.java:277)
	at java.base/java.lang.Thread.run(Thread.java:1583)
16:54:15.847 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:15.851 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1, groupId=test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f] Initializing the Kafka consumer
16:54:15.960 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:15.960 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:15.960 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524855959
16:54:15.960 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1, groupId=test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f] Kafka consumer initialized
16:54:15.961 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd -- Cmd: 70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a,false
16:54:15.961 [main] DEBUG org.testcontainers.shaded.com.github.dockerjava.core.exec.InspectContainerCmdExec -- GET: DefaultWebTarget{path=[/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json], queryParams={}}
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000011: preparing request execution
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- Cookie spec selected: strict
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.protocol.RequestAuthCache -- Auth cache not set in the context
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000011: target auth state: UNCHALLENGED
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-00000011: proxy auth state: UNCHALLENGED
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000011: acquiring connection with route {}->unix://localhost:2375
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000011: acquiring endpoint (3 MINUTES)
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000011: endpoint lease request (3 MINUTES) [route: {}->unix://localhost:2375][total available: 0; route allocated: 0 of 2147483647; total allocated: 0 of 2147483647]
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000011: endpoint leased [route: {}->unix://localhost:2375][total available: 0; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-00000011: acquired ep-00000010
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-00000011: acquired endpoint ep-00000010
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-00000011: opening connection {}->unix://localhost:2375
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000010: connecting endpoint (3 MINUTES)
16:54:15.961 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000010: connecting endpoint to unix://localhost:2375 (3 MINUTES)
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-3: connecting to localhost/127.0.0.1:2375
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- http-outgoing-3: connection established /var/run/docker.sock<->/var/run/docker.sock
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000010: connected http-outgoing-3
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000010: endpoint connected
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000011: executing GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json HTTP/1.1
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000010: start execution ex-00000011
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000010: executing exchange ex-00000011 over http-outgoing-3
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 >> GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json HTTP/1.1
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 >> accept: application/json
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 >> x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 >> User-Agent: tc-java/1.19.3
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 >> Accept-Encoding: gzip, x-gzip, deflate
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 >> Host: localhost:2375
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 >> Connection: keep-alive
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "GET /v1.32/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/json HTTP/1.1[\r][\n]"
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "accept: application/json[\r][\n]"
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "x-tc-sid: 41781793-441b-4be0-9f6b-8274f7098da1[\r][\n]"
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "User-Agent: tc-java/1.19.3[\r][\n]"
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "Accept-Encoding: gzip, x-gzip, deflate[\r][\n]"
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "Host: localhost:2375[\r][\n]"
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "Connection: keep-alive[\r][\n]"
16:54:15.962 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 >> "[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "HTTP/1.1 200 OK[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "Api-Version: 1.51[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "Content-Type: application/json[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "Date: Fri, 03 Oct 2025 20:54:15 GMT[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "Docker-Experimental: false[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "Ostype: linux[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "Server: Docker/28.4.0 (linux)[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "Transfer-Encoding: chunked[\r][\n]"
16:54:15.968 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "[\r][\n]"
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << HTTP/1.1 200 OK
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << Api-Version: 1.51
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << Content-Type: application/json
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << Date: Fri, 03 Oct 2025 20:54:15 GMT
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << Docker-Experimental: false
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << Ostype: linux
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << Server: Docker/28.4.0 (linux)
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.headers -- http-outgoing-3 << Transfer-Encoding: chunked
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-00000011: connection can be kept alive for 3 MINUTES
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "1cee[\r][\n]"
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "{"Id":"70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a","Created":"2025-10-03T20:54:12.685485717Z","Path":"sh","Args":["-c","while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh"],"State":{"Status":"running","Running":true,"Paused":false,"Restarting":false,"OOMKilled":false,"Dead":false,"Pid":3430,"ExitCode":0,"Error":"","StartedAt":"2025-10-03T20:54:12.732924217Z","FinishedAt":"0001-01-01T00:00:00Z"},"Image":"sha256:187dac6627e7906c350f5f8c982f80ce735ff1a0e571a20de6000a309a12ce63","ResolvConfPath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/resolv.conf","HostnamePath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/hostname","HostsPath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/hosts","LogPath":"/var/lib/docker/containers/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a/70e45fd582a9c5b7b9768b98850c6281f1147d0ccf0b477c1295c9e01596805a-json.log","Name":"/hopeful_albattani","RestartCount":0,"Driver":"overlayfs","Platform":"linux","MountLabel":"","ProcessLabel":"","AppArmorProfile":"","ExecIDs":null,"HostConfig":{"Binds":[],"ContainerIDFile":"","LogConfig":{"Type":"json-file","Config":{}},"NetworkMode":"bridge","PortBindings":{"2181/tcp":[{"HostIp":"","HostPort":""}],"9093/tcp":[{"HostIp":"","HostPort":""}]},"RestartPolicy":{"Name":"no","MaximumRetryCount":0},"AutoRemove":false,"VolumeDriver":"","VolumesFrom":[],"ConsoleSize":[0,0],"CapAdd":null,"CapDrop":null,"CgroupnsMode":"private","Dns":null,"DnsOptions":null,"DnsSearch":null,"ExtraHosts":[],"GroupAdd":null,"IpcMode":"shareable","Cgroup":"","Links":null,"OomScoreAdj":0,"PidMode":"","Privileged":false,"PublishAllPorts":false,"ReadonlyRootfs":false,"SecurityOpt":null,"UTSMode":"","UsernsMode":"","ShmSize":67108864,"Runtime":"runc","Isolation":"","CpuShares":0,"Memory":0,"NanoCpus":0,"CgroupParent":"","BlkioWeight":0,"BlkioWeightDevice":null,"BlkioDeviceReadBps":null,"BlkioDeviceWriteBps":null,"BlkioDeviceReadIOps":null,"BlkioDeviceWriteIOps":null,"CpuPeriod":0,"CpuQuota":0,"CpuRealtimePeriod":0,"CpuRealtimeRuntime":0,"CpusetCpus":"","CpusetMems":"","Devices":null,"DeviceCgroupRules":null,"DeviceRequests":null,"MemoryReservation":0,"MemorySwap":0,"MemorySwappiness":null,"OomKillDisable":null,"PidsLimit":null,"Ulimits":null,"CpuCount":0,"CpuPercent":0,"IOMaximumIOps":0,"IOMaximumBandwidth":0,"MaskedPaths":["/proc/asound","/proc/acpi","/proc/interrupts","/proc/kcore","/proc/keys","/proc/latency_stats","/proc/timer_list","/proc/timer_stats","/proc/sched_debug","/proc/scsi","/sys/firmware","/sys/devices/virtual/powercap"],"ReadonlyPaths":["/proc/bus","/proc/fs","/proc/irq","/proc/sys","/proc/sysrq-trigger"]},"GraphDriver":{"Data":null,"Name":"overlayfs"},"Mounts":[{"Type":"volume","Name":"cc01bcf954d0c6b7eb7d9e22d9bd695f952602f81b513e5c1b7f2d19e29926c2","Source":"/var/lib/docker/volumes/cc01bcf954d0c6b7eb7d9e22d9bd695f952602f81b513e5c1b7f2d19e29926c2/_data","Destination":"/etc/kafka/secrets","Driver":"local","Mode":"","RW":true,"Propagation":""},{"Type":"volume","Name":"28e334bdccbdadfc8f06b0f275765c77237cc7e4508a02165e73656776b4390e","Source":"/var/lib/docker/volumes/28e334bdccbdadfc8f06b0f275765c77237cc7e4508a02165e73656776b4390e/_data","Destination":"/var/lib/kafka/data","Driver":"local","Mode":"","RW":true,"Propagation":""}],"Config":{"Hostname":"70e45fd582a9","Domainname":"","User":"appuser","AttachStdin":false,"AttachStdout":false,"AttachStderr":false,"ExposedPorts":{"2181/tcp":{},"9092/tcp":{},"9093/tcp":{}},"Tty":false,"OpenStdin":false,"StdinOnce":false,"Env":["KAFKA_DEFAULT_REPLICATION_FACTOR=1","KAFKA_BROKER_ID=1","KAFKA_NUM_PARTITIONS=3","KAFKA_ZOOKEEPER_CONNECT=localhost:2181","KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS=1","KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1","KAFKA_INTER_BROKER_LISTENER_NAME=BROKER","KAFKA_AUTO_CREATE_TOPICS_ENABLE=true","KAFKA_LOG_FLUSH_INTERVAL_MESSAGES=9223372036854775807","KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1","KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093,BROKER://0.0.0.0:9092","KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1","KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=BROKER:PLAINTEXT,PLAINTEXT:PLAINTEXT","KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0","PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin","container=oci","LANG=C.UTF-8","CUB_CLASSPATH=\"/usr/share/java/cp-base-new/*\"","KAFKA_ADVERTISED_LISTENERS=","CLUSTER_ID=","COMPONENT=kafka"],"Cmd":["-c","while [ ! -f /testcontainers_start.sh ]; do sleep 0.1; done; /testcontainers_start.sh"],"Image":"confluentinc/cp-kafka:7.4.0","Volumes":{"/etc/kafka/secrets":{},"/var/lib/kafka/data":{}},"WorkingDir":"/home/appuser","Entrypoint":["sh"],"OnBuild":null,"Labels":{"architecture":"aarch64","build-date":"2023-05-03T15:02:09","com.redhat.component":"ubi8-minimal-container","com.redhat.license_terms":"https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI","description":"Common base image for Confluent's Docker images.","distribution-scope":"public","io.buildah.version":"1.27.3","io.confluent.docker":"true","io.confluent.docker.build.number":"5","io.confluent.docker.git.id":"d0965a30f","io.confluent.docker.git.repo":"confluentinc/kafka-images","io.k8s.description":"The Universal Base Image Minimal is a stripped down image that uses microdnf as a package manager. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.","io.k8s.display-name":"Red Hat Universal Base Image 8 Minimal","io.openshift.expose-services":"","io.openshift.tags":"minimal rhel8","maintainer":"partner-support@confluent.io","name":"cp-kafka","org.testcontainers":"true","org.testcontainers.lang":"java","org.testcontainers.sessionId":"41781793-441b-4be0-9f6b-8274f7098da1","org.testcontainers.version":"1.19.3","release":"7.4.0","summary":"Confluent platform Kafka.","url":"https://access.redhat.com/containers/#/registry.access.redhat.com/ubi8-minimal/images/8.8-860","vcs-ref":"dee8029ddcc7ecbfbebb0905d2b15e134338616c","vcs-type":"git","vendor":"Confluent","version":"d0965a30f"}},"NetworkSettings":{"Bridge":"","SandboxID":"937606067d4adb7b3d7335a93b35235e8c6e709610b07aa39c78e5c3c3d71b07","SandboxKey":"/var/run/docker/netns/937606067d4a","Ports":{"2181/tcp":[{"HostIp":"0.0.0.0","HostPort":"50138"},{"HostIp":"::","HostPort":"50138"}],"9093/tcp":[{"HostIp":"0.0.0.0","HostPort":"50137"},{"HostIp":"::","HostPort":"50137"}]},"HairpinMode":false,"LinkLocalIPv6Address":"","LinkLocalIPv6PrefixLen":0,"SecondaryIPAddresses":null,"SecondaryIPv6Addresses":null,"EndpointID":"637551cc0f3590daff1ae17cd2aedf737706c58228b5a0994ba8246b7da2db84","Gateway":"172.17.0.1","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"IPAddress":"172.17.0.3","IPPrefixLen":16,"IPv6Gateway":"","MacAddress":"f6:eb:2f:89:04:d1","Networks":{"bridge":{"IPAMConfig":null,"Links":null,"Aliases":null,"MacAddress":"f6:eb:2f:89:04:d1","DriverOpts":null,"GwPriority":0,"NetworkID":"070182f8c4352237efe530a41d27261a044a578f4196783b1a6a7009353b1475","EndpointID":"637551cc0f3590daff1ae17cd2aedf737706c58228b5a0994ba8246b7da2db84","Gateway":"172.17.0.1","IPAddress":"172.17.0.3","IPPrefixLen":16,"IPv6Gateway":"","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"DNSNames":null}}}}[\n]"
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "[\r][\n]"
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "0[\r][\n]"
16:54:15.969 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.wire -- http-outgoing-3 << "[\r][\n]"
16:54:15.970 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-00000010: releasing valid endpoint
16:54:15.970 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000010: releasing endpoint
16:54:15.970 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000010: connection http-outgoing-3 can be kept alive for 3 MINUTES
16:54:15.970 [main] DEBUG com.github.dockerjava.zerodep.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-00000010: connection released [route: {}->unix://localhost:2375][total available: 1; route allocated: 1 of 2147483647; total allocated: 1 of 2147483647]
Kafka is running on: PLAINTEXT://localhost:50137
16:54:15.972 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1, groupId=test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f] Sending synchronous auto-commit of offsets {}
16:54:15.972 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1, groupId=test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'}
16:54:15.972 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1, groupId=test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:15.972 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1, groupId=test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f] Request joining group due to: consumer pro-actively leaving the group
16:54:15.973 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:15.973 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:15.973 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:15.974 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1 unregistered
16:54:15.974 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f-1, groupId=test-consumer-group-8cb79864-056c-4c8d-bdd6-6a06e1cf017f] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testAppConfigLoading" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="0.034">
    <system-out><![CDATA[16:54:15.975 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:15.976 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2, groupId=test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387] Initializing the Kafka consumer
16:54:15.980 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:15.980 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:15.980 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524855980
16:54:15.980 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2, groupId=test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387] Kafka consumer initialized
16:54:16.008 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2, groupId=test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387] Sending synchronous auto-commit of offsets {}
16:54:16.008 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2, groupId=test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'}
16:54:16.008 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2, groupId=test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:16.008 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2, groupId=test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387] Request joining group due to: consumer pro-actively leaving the group
16:54:16.008 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:16.008 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:16.008 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:16.009 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2 unregistered
16:54:16.009 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387-2, groupId=test-consumer-group-4820e9d8-a8f8-4088-8b13-6f6d6a402387] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testStreamRecordSerialization" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="0.094">
    <system-out><![CDATA[16:54:16.010 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:16.010 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3, groupId=test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8] Initializing the Kafka consumer
16:54:16.011 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:16.011 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:16.011 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524856011
16:54:16.012 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3, groupId=test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8] Kafka consumer initialized
16:54:16.102 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3, groupId=test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8] Sending synchronous auto-commit of offsets {}
16:54:16.102 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3, groupId=test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'}
16:54:16.102 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3, groupId=test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:16.102 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3, groupId=test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8] Request joining group due to: consumer pro-actively leaving the group
16:54:16.102 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:16.102 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:16.102 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:16.102 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3 unregistered
16:54:16.102 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8-3, groupId=test-consumer-group-36691ecb-c7ed-4021-aea4-6105184f36e8] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testKafkaProducerConnectivity" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="0.595">
    <system-out><![CDATA[16:54:16.104 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:16.104 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4, groupId=test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110] Initializing the Kafka consumer
16:54:16.105 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:16.105 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:16.105 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524856105
16:54:16.105 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4, groupId=test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110] Kafka consumer initialized
16:54:16.110 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 131072
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	buffer.memory = 268435456
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = zstd
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 300000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 60000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

16:54:16.113 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-1] Instantiated an idempotent producer.
16:54:16.118 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:16.125 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:16.125 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524856118
16:54:16.125 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-1] Starting Kafka producer I/O thread.
16:54:16.125 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-1] Kafka producer started
16:54:16.125 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-1] Transition from state UNINITIALIZED to INITIALIZING
16:54:16.206 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-1] Enqueuing transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:16.206 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Initialize connection to node localhost:50137 (id: -1 rack: null) for sending metadata request
16:54:16.207 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:16.207 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:16.210 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-1] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:16.210 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Completed connection to node -1. Fetching API versions.
16:54:16.210 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Initiating API versions fetch from node -1.
16:54:16.217 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=0, headerVersion=2) and timeout 60000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:16.260 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=0, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:16.275 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:16.275 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-connectivity-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:16.275 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-1, correlationId=1, headerVersion=2) and timeout 60000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-connectivity-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:16.276 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-1] Sending transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) to node localhost:50137 (id: -1 rack: null) with correlation ID 2
16:54:16.276 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending INIT_PRODUCER_ID request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-1, correlationId=2, headerVersion=2) and timeout 60000 to node -1: InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:16.309 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-1, correlationId=1, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='test-connectivity-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:16.311 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {test-connectivity-topic=LEADER_NOT_AVAILABLE}
16:54:16.311 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Requesting metadata update for topic test-connectivity-topic due to error LEADER_NOT_AVAILABLE
16:54:16.312 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:16.312 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:16.340 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Received INIT_PRODUCER_ID response from node -1 for request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-1, correlationId=2, headerVersion=2): InitProducerIdResponseData(throttleTimeMs=0, errorCode=0, producerId=0, producerEpoch=0)
16:54:16.340 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
16:54:16.340 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-1] Transition from state INITIALIZING to READY
16:54:16.417 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:16.418 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:16.418 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:16.419 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-1] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:16.420 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Completed connection to node 1. Fetching API versions.
16:54:16.420 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Initiating API versions fetch from node 1.
16:54:16.420 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=3, headerVersion=2) and timeout 60000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:16.422 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:16.422 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:16.422 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-connectivity-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:16.422 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-1, correlationId=4, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-connectivity-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:16.425 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-1, correlationId=4, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='test-connectivity-topic', topicId=gEW4XHVDS3KO9i52e5VIYQ, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:16.425 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Setting the last seen epoch of partition test-connectivity-topic-0 to 0 since the last known epoch was undefined.
16:54:16.425 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Setting the last seen epoch of partition test-connectivity-topic-1 to 0 since the last known epoch was undefined.
16:54:16.425 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Setting the last seen epoch of partition test-connectivity-topic-2 to 0 since the last known epoch was undefined.
16:54:16.426 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=test-connectivity-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-connectivity-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-connectivity-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:16.634 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-1] ProducerId of partition test-connectivity-topic-2 set to 0 with epoch 0. Reinitialize sequence at beginning.
16:54:16.634 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-1] Assigned producerId 0 and producerEpoch 0 to batch with base sequence 0 being sent to partition test-connectivity-topic-2
16:54:16.643 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-1, correlationId=5, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[test-connectivity-topic-2=98]}
16:54:16.687 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-1, correlationId=5, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='test-connectivity-topic', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:16.691 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-1] ProducerId: 0; Set last ack'd sequence number for topic-partition test-connectivity-topic-2 to 0
Successfully sent message to topic: test-connectivity-topic, partition: 2, offset: 0
16:54:16.694 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16:54:16.694 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-1] Beginning shutdown of Kafka producer I/O thread, sending remaining records.
16:54:16.696 [kafka-producer-network-thread | producer-1] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-1] Shutdown of Kafka producer I/O thread has completed.
16:54:16.696 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:16.696 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:16.696 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:16.696 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.producer for producer-1 unregistered
16:54:16.696 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-1] Kafka producer has been closed
16:54:16.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4, groupId=test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110] Sending synchronous auto-commit of offsets {}
16:54:16.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4, groupId=test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'}
16:54:16.697 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4, groupId=test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:16.697 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4, groupId=test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110] Request joining group due to: consumer pro-actively leaving the group
16:54:16.697 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:16.697 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:16.697 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:16.697 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4 unregistered
16:54:16.697 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110-4, groupId=test-consumer-group-549d7a3e-7b22-4d45-b3a6-ca5fe6385110] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testKafkaConsumerConnectivity" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="1.259">
    <system-out><![CDATA[16:54:16.699 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:16.700 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initializing the Kafka consumer
16:54:16.702 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:16.702 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:16.702 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524856702
16:54:16.702 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Kafka consumer initialized
16:54:16.703 [main] INFO org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Subscribed to topic(s): test-consumer-topic
16:54:16.704 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 131072
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	buffer.memory = 268435456
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = zstd
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 300000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 60000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

16:54:16.704 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-2] Instantiated an idempotent producer.
16:54:16.705 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:16.705 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:16.705 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524856704
16:54:16.705 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-2] Kafka producer started
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-2] Starting Kafka producer I/O thread.
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-2] Transition from state UNINITIALIZED to INITIALIZING
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-2] Enqueuing transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Initialize connection to node localhost:50137 (id: -1 rack: null) for sending metadata request
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-2] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Completed connection to node -1. Fetching API versions.
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Initiating API versions fetch from node -1.
16:54:16.705 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-2, correlationId=0, headerVersion=2) and timeout 60000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:16.708 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-2, correlationId=0, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:16.708 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:16.709 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:16.709 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-2, correlationId=1, headerVersion=2) and timeout 60000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:16.709 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-2] Sending transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) to node localhost:50137 (id: -1 rack: null) with correlation ID 2
16:54:16.709 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending INIT_PRODUCER_ID request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-2, correlationId=2, headerVersion=2) and timeout 60000 to node -1: InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:16.719 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-2, correlationId=1, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='test-consumer-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:16.719 [kafka-producer-network-thread | producer-2] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Error while fetching metadata with correlation id 1 : {test-consumer-topic=LEADER_NOT_AVAILABLE}
16:54:16.719 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Requesting metadata update for topic test-consumer-topic due to error LEADER_NOT_AVAILABLE
16:54:16.719 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:16.719 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:16.720 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Received INIT_PRODUCER_ID response from node -1 for request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-2, correlationId=2, headerVersion=2): InitProducerIdResponseData(throttleTimeMs=0, errorCode=0, producerId=1, producerEpoch=0)
16:54:16.720 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-2] ProducerId set to 1 with epoch 0
16:54:16.720 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-2] Transition from state INITIALIZING to READY
16:54:16.918 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:16.919 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:16.919 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:16.925 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-2] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:16.925 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Completed connection to node 1. Fetching API versions.
16:54:16.925 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Initiating API versions fetch from node 1.
16:54:16.925 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-2, correlationId=3, headerVersion=2) and timeout 60000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:17.037 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-2, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:17.037 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:17.037 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:17.037 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-2, correlationId=4, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:17.050 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-2, correlationId=4, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='test-consumer-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:17.050 [kafka-producer-network-thread | producer-2] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Error while fetching metadata with correlation id 4 : {test-consumer-topic=LEADER_NOT_AVAILABLE}
16:54:17.050 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Requesting metadata update for topic test-consumer-topic due to error LEADER_NOT_AVAILABLE
16:54:17.051 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:17.151 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:17.151 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-2, correlationId=5, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:17.155 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-2, correlationId=5, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='test-consumer-topic', topicId=yTUsZqGcQVSdF0_bWB4Sqg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:17.156 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Setting the last seen epoch of partition test-consumer-topic-0 to 0 since the last known epoch was undefined.
16:54:17.156 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Setting the last seen epoch of partition test-consumer-topic-1 to 0 since the last known epoch was undefined.
16:54:17.156 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Setting the last seen epoch of partition test-consumer-topic-2 to 0 since the last known epoch was undefined.
16:54:17.156 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Updated cluster metadata updateVersion 4 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=test-consumer-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-consumer-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-consumer-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:17.162 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-2] ProducerId of partition test-consumer-topic-2 set to 1 with epoch 0. Reinitialize sequence at beginning.
16:54:17.162 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-2] Assigned producerId 1 and producerEpoch 0 to batch with base sequence 0 being sent to partition test-consumer-topic-2
16:54:17.162 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-2, correlationId=6, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[test-consumer-topic-2=126]}
16:54:17.169 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-2, correlationId=6, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='test-consumer-topic', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:17.170 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-2] ProducerId: 1; Set last ack'd sequence number for topic-partition test-consumer-topic-2 to 0
16:54:17.170 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16:54:17.170 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-2] Beginning shutdown of Kafka producer I/O thread, sending remaining records.
16:54:17.170 [kafka-producer-network-thread | producer-2] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-2] Shutdown of Kafka producer I/O thread has completed.
16:54:17.170 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:17.170 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:17.170 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:17.170 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.producer for producer-2 unregistered
16:54:17.170 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-2] Kafka producer has been closed
16:54:17.171 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending FindCoordinator request to broker localhost:50137 (id: -1 rack: null)
16:54:17.171 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:17.171 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:17.172 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:17.172 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Completed connection to node -1. Fetching API versions.
16:54:17.172 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initiating API versions fetch from node -1.
16:54:17.172 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=1, headerVersion=2) and timeout 30000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:17.176 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=1, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:17.176 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:17.176 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:17.176 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=2, headerVersion=2) and timeout 30000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:17.176 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=0, headerVersion=2) and timeout 30000 to node -1: FindCoordinatorRequestData(key='', keyType=0, coordinatorKeys=[test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133])
16:54:17.177 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=2, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='test-consumer-topic', topicId=yTUsZqGcQVSdF0_bWB4Sqg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:17.178 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Setting the last seen epoch of partition test-consumer-topic-0 to 0 since the last known epoch was undefined.
16:54:17.178 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Setting the last seen epoch of partition test-consumer-topic-1 to 0 since the last known epoch was undefined.
16:54:17.178 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Setting the last seen epoch of partition test-consumer-topic-2 to 0 since the last known epoch was undefined.
16:54:17.178 [main] INFO org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:17.178 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=test-consumer-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-consumer-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-consumer-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:17.189 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received FIND_COORDINATOR response from node -1 for request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=0, headerVersion=2): FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')])
16:54:17.189 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received FindCoordinator response ClientResponse(receivedTimeMs=1759524857189, latencyMs=18, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=0, headerVersion=2), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
16:54:17.190 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Group coordinator lookup failed: 
16:54:17.190 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
16:54:17.277 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:17.277 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:17.277 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:17.278 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:17.279 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Completed connection to node 1. Fetching API versions.
16:54:17.279 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initiating API versions fetch from node 1.
16:54:17.279 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=3, headerVersion=2) and timeout 30000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:17.281 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:17.282 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:17.282 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:17.282 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=4, headerVersion=2) and timeout 30000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='test-consumer-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:17.286 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=4, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='test-consumer-topic', topicId=yTUsZqGcQVSdF0_bWB4Sqg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:17.286 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Updating last seen epoch for partition test-consumer-topic-0 from 0 to epoch 0 from new metadata
16:54:17.286 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Updating last seen epoch for partition test-consumer-topic-2 from 0 to epoch 0 from new metadata
16:54:17.286 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Updating last seen epoch for partition test-consumer-topic-1 from 0 to epoch 0 from new metadata
16:54:17.286 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=test-consumer-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-consumer-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=test-consumer-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:17.287 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending FindCoordinator request to broker localhost:50137 (id: 1 rack: null)
16:54:17.287 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=5, headerVersion=2) and timeout 30000 to node 1: FindCoordinatorRequestData(key='', keyType=0, coordinatorKeys=[test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133])
16:54:17.291 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received FIND_COORDINATOR response from node 1 for request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=5, headerVersion=2): FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')])
16:54:17.291 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received FindCoordinator response ClientResponse(receivedTimeMs=1759524857291, latencyMs=4, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=5, headerVersion=2), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')]))
16:54:17.291 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Discovered group coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:17.292 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:17.292 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initiating connection to node localhost:50137 (id: 2147483646 rack: null) using address localhost/127.0.0.1
16:54:17.293 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Executing onJoinPrepare with generation -1 and memberId 
16:54:17.293 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Marking assigned partitions pending for revocation: []
16:54:17.293 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending asynchronous auto-commit of offsets {}
16:54:17.293 [kafka-coordinator-heartbeat-thread | test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Heartbeat thread started
16:54:17.296 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 2147483646
16:54:17.296 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Completed connection to node 2147483646. Fetching API versions.
16:54:17.296 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Initiating API versions fetch from node 2147483646.
16:54:17.296 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=6, headerVersion=2) and timeout 30000 to node 2147483646: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:17.297 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] (Re-)joining group
16:54:17.298 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Joining group with current subscription: [test-consumer-topic]
16:54:17.301 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:17.302 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received API_VERSIONS response from node 2147483646 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=6, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:17.302 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Node 2147483646 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:17.302 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=7, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')
16:54:17.304 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Completed asynchronous auto-commit of offsets {}
16:54:17.316 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=7, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=79, generationId=-1, protocolType=null, protocolName=null, leader='', skipAssignment=false, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', members=[])
16:54:17.316 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] JoinGroup failed due to non-fatal error: MEMBER_ID_REQUIRED. Will set the member id as consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f and then rejoin. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
16:54:17.316 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Request joining group due to: need to re-join with the given member-id: consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f
16:54:17.316 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
16:54:17.316 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] (Re-)joining group
16:54:17.316 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Joining group with current subscription: [test-consumer-topic]
16:54:17.317 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:17.317 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=8, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')
16:54:17.329 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=8, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', skipAssignment=false, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:17.330 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', skipAssignment=false, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:17.330 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Enabling heartbeat thread
16:54:17.330 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', protocol='range'}
16:54:17.330 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Performing assignment using strategy range with subscriptions {consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f=Subscription(topics=[test-consumer-topic], ownedPartitions=[], groupInstanceId=null, generationId=-1, rackId=null)}
16:54:17.333 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Finished assignment for group at generation 1: {consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f=Assignment(partitions=[test-consumer-topic-0, test-consumer-topic-1, test-consumer-topic-2])}
16:54:17.334 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending leader SyncGroup to coordinator localhost:50137 (id: 2147483646 rack: null): SyncGroupRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', generationId=1, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', assignment=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:17.334 [kafka-coordinator-heartbeat-thread | test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending SYNC_GROUP request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=9, headerVersion=2) and timeout 30000 to node 2147483646: SyncGroupRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', generationId=1, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', assignment=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:17.367 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received SYNC_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=9, headerVersion=2): SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:17.367 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received successful SyncGroup response: SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 1, 0, 19, 116, 101, 115, 116, 45, 99, 111, 110, 115, 117, 109, 101, 114, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:17.367 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', protocol='range'}
16:54:17.367 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Executing onJoinComplete with generation 1 and memberId consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f
16:54:17.367 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Notifying assignor about the new Assignment(partitions=[test-consumer-topic-0, test-consumer-topic-1, test-consumer-topic-2])
16:54:17.369 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Adding newly assigned partitions: test-consumer-topic-0, test-consumer-topic-1, test-consumer-topic-2
16:54:17.372 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Fetching committed offsets for partitions: [test-consumer-topic-1, test-consumer-topic-2, test-consumer-topic-0]
16:54:17.373 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending OFFSET_FETCH request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=10, headerVersion=2) and timeout 30000 to node 2147483646: OffsetFetchRequestData(groupId='', topics=[], groups=[OffsetFetchRequestGroup(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', topics=[OffsetFetchRequestTopics(name='test-consumer-topic', partitionIndexes=[1, 2, 0])])], requireStable=true)
16:54:17.379 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received OFFSET_FETCH response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=10, headerVersion=2): OffsetFetchResponseData(throttleTimeMs=0, topics=[], errorCode=0, groups=[OffsetFetchResponseGroup(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', topics=[OffsetFetchResponseTopics(name='test-consumer-topic', partitions=[OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)])], errorCode=0)])
16:54:17.380 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Found no committed offset for partition test-consumer-topic-1
16:54:17.380 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Found no committed offset for partition test-consumer-topic-2
16:54:17.380 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Found no committed offset for partition test-consumer-topic-0
16:54:17.381 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcher -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending ListOffsetRequest ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='test-consumer-topic', partitions=[ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])]) to broker localhost:50137 (id: 1 rack: null)
16:54:17.381 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending LIST_OFFSETS request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=11, headerVersion=2) and timeout 30000 to node 1: ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='test-consumer-topic', partitions=[ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])])
16:54:17.388 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received LIST_OFFSETS response from node 1 for request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=11, headerVersion=2): ListOffsetsResponseData(throttleTimeMs=0, topics=[ListOffsetsTopicResponse(name='test-consumer-topic', partitions=[ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)])])
16:54:17.389 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Handling ListOffsetResponse response for test-consumer-topic-1. Fetched offset 0, timestamp -1
16:54:17.389 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Handling ListOffsetResponse response for test-consumer-topic-2. Fetched offset 0, timestamp -1
16:54:17.389 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Handling ListOffsetResponse response for test-consumer-topic-0. Fetched offset 0, timestamp -1
16:54:17.389 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Not replacing existing epoch 0 with new epoch 0 for partition test-consumer-topic-1
16:54:17.390 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Resetting offset for partition test-consumer-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:17.390 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Not replacing existing epoch 0 with new epoch 0 for partition test-consumer-topic-2
16:54:17.390 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Resetting offset for partition test-consumer-topic-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:17.390 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Not replacing existing epoch 0 with new epoch 0 for partition test-consumer-topic-0
16:54:17.390 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Resetting offset for partition test-consumer-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:17.390 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Added READ_UNCOMMITTED fetch request for partition test-consumer-topic-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:17.390 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Added READ_UNCOMMITTED fetch request for partition test-consumer-topic-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:17.390 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Added READ_UNCOMMITTED fetch request for partition test-consumer-topic-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:17.390 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Built full fetch (sessionId=INVALID, epoch=INITIAL) for node 1 with 3 partition(s).
16:54:17.391 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(test-consumer-topic-1, test-consumer-topic-2, test-consumer-topic-0), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:17.391 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:17.391 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=12, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=0, sessionEpoch=0, topics=[FetchTopic(topic='test-consumer-topic', topicId=yTUsZqGcQVSdF0_bWB4Sqg, partitions=[FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:17.417 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=12, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=2127626112, responses=[FetchableTopicResponse(topic='', topicId=yTUsZqGcQVSdF0_bWB4Sqg, partitions=[PartitionData(partitionIndex=1, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=203])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=126, buffer=java.nio.HeapByteBuffer[pos=0 lim=126 cap=166])), PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=3]))])])
16:54:17.418 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Node 1 sent a full fetch response that created a new incremental fetch session 2127626112 with 3 response partition(s)
16:54:17.419 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Fetch READ_UNCOMMITTED at offset 0 for partition test-consumer-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=203]))
16:54:17.419 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Fetch READ_UNCOMMITTED at offset 0 for partition test-consumer-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=126, buffer=java.nio.HeapByteBuffer[pos=0 lim=126 cap=166]))
16:54:17.419 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Fetch READ_UNCOMMITTED at offset 0 for partition test-consumer-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=3]))
16:54:17.419 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:17.421 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Added READ_UNCOMMITTED fetch request for partition test-consumer-topic-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:17.421 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Added READ_UNCOMMITTED fetch request for partition test-consumer-topic-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:17.421 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Added READ_UNCOMMITTED fetch request for partition test-consumer-topic-2 at position FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:17.421 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Built incremental fetch (sessionId=2127626112, epoch=1) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:17.421 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(test-consumer-topic-2), toForget=(), toReplace=(), implied=(test-consumer-topic-1, test-consumer-topic-0), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:17.421 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:17.421 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=13, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=2127626112, sessionEpoch=1, topics=[FetchTopic(topic='test-consumer-topic', topicId=yTUsZqGcQVSdF0_bWB4Sqg, partitions=[FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=1, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
Successfully consumed message from topic: test-consumer-topic, partition: 2, offset: 0
16:54:17.423 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending synchronous auto-commit of offsets {test-consumer-topic-1=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}, test-consumer-topic-2=OffsetAndMetadata{offset=1, leaderEpoch=0, metadata=''}, test-consumer-topic-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}}
16:54:17.423 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending OFFSET_COMMIT request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=14, headerVersion=2) and timeout 30000 to node 2147483646: OffsetCommitRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', generationIdOrMemberEpoch=1, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, retentionTimeMs=-1, topics=[OffsetCommitRequestTopic(name='test-consumer-topic', partitions=[OffsetCommitRequestPartition(partitionIndex=1, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=2, committedOffset=1, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=0, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata='')])])
16:54:17.445 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received OFFSET_COMMIT response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=14, headerVersion=2): OffsetCommitResponseData(throttleTimeMs=0, topics=[OffsetCommitResponseTopic(name='test-consumer-topic', partitions=[OffsetCommitResponsePartition(partitionIndex=1, errorCode=0), OffsetCommitResponsePartition(partitionIndex=2, errorCode=0), OffsetCommitResponsePartition(partitionIndex=0, errorCode=0)])])
16:54:17.446 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Committed offset 0 for partition test-consumer-topic-1
16:54:17.446 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Committed offset 1 for partition test-consumer-topic-2
16:54:17.446 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Committed offset 0 for partition test-consumer-topic-0
16:54:17.446 [kafka-coordinator-heartbeat-thread | test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Heartbeat thread has closed
16:54:17.447 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Executing onLeavePrepare with generation Generation{generationId=1, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', protocol='range'}
16:54:17.447 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Revoke previously assigned partitions test-consumer-topic-0, test-consumer-topic-1, test-consumer-topic-2
16:54:17.447 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Member consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f sending LeaveGroup request to coordinator localhost:50137 (id: 2147483646 rack: null) due to the consumer is being closed
16:54:17.447 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending LEAVE_GROUP request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=15, headerVersion=2) and timeout 30000 to node 2147483646: LeaveGroupRequestData(groupId='test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133', memberId='', members=[MemberIdentity(memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, reason='the consumer is being closed')])
16:54:17.448 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:17.448 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Request joining group due to: consumer pro-actively leaving the group
16:54:17.457 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received LEAVE_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=15, headerVersion=2): LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, errorCode=0)])
16:54:17.457 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] LeaveGroup response with Generation{generationId=1, memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', protocol='range'} returned successfully: ClientResponse(receivedTimeMs=1759524857457, latencyMs=10, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=15, headerVersion=2), responseBody=LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5-d3bd5681-2830-4339-ab17-72d0f66f823f', groupInstanceId=null, errorCode=0)]))
16:54:17.457 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Set the metadata for next fetch request to close the existing session ID=2127626112
16:54:17.457 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Built full fetch (sessionId=2127626112, epoch=FINAL) for node 1 with 0 partition(s).
16:54:17.457 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:17.457 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:17.457 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=16, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=2127626112, sessionEpoch=-1, topics=[], forgottenTopicsData=[], rackId='')
16:54:17.948 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=13, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=2127626112, responses=[])
16:54:17.949 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Node 1 sent a full fetch response that created a new incremental fetch session 2127626112 with 0 response partition(s)
16:54:17.949 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:17.954 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, correlationId=16, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=0, responses=[])
16:54:17.954 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Successfully sent a close message for fetch session: 2127626112 to node: localhost:50137 (id: 1 rack: null)
16:54:17.955 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:17.955 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:17.955 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:17.957 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5 unregistered
16:54:17.957 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133-5, groupId=test-consumer-group-f4c5d7fc-bb8b-4a69-b4e2-bd3a42382133] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testStreamRecordKafkaRoundTrip" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="0.704">
    <system-out><![CDATA[16:54:17.960 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:17.960 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Initializing the Kafka consumer
16:54:17.964 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:17.964 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:17.964 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524857964
16:54:17.964 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Kafka consumer initialized
16:54:17.965 [main] INFO org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Subscribed to topic(s): stream-record-test-topic
16:54:17.967 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 131072
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	buffer.memory = 268435456
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = zstd
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 300000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 60000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

16:54:17.967 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-3] Instantiated an idempotent producer.
16:54:17.971 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:17.971 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:17.971 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524857971
16:54:17.971 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-3] Kafka producer started
16:54:17.971 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-3] Starting Kafka producer I/O thread.
16:54:17.971 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-3] Transition from state UNINITIALIZED to INITIALIZING
16:54:17.971 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-3] Enqueuing transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:17.972 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Initialize connection to node localhost:50137 (id: -1 rack: null) for sending metadata request
16:54:17.972 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:17.972 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:17.973 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-3] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:17.973 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Completed connection to node -1. Fetching API versions.
16:54:17.973 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Initiating API versions fetch from node -1.
16:54:17.973 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-3, correlationId=0, headerVersion=2) and timeout 60000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:17.975 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-3, correlationId=0, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:17.975 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:17.975 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='stream-record-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:17.975 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-3, correlationId=1, headerVersion=2) and timeout 60000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='stream-record-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:17.975 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-3] Sending transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) to node localhost:50137 (id: -1 rack: null) with correlation ID 2
16:54:17.975 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending INIT_PRODUCER_ID request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-3, correlationId=2, headerVersion=2) and timeout 60000 to node -1: InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:17.986 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-3, correlationId=1, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='stream-record-test-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:17.986 [kafka-producer-network-thread | producer-3] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Error while fetching metadata with correlation id 1 : {stream-record-test-topic=LEADER_NOT_AVAILABLE}
16:54:17.986 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-3] Requesting metadata update for topic stream-record-test-topic due to error LEADER_NOT_AVAILABLE
16:54:17.986 [kafka-producer-network-thread | producer-3] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-3] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:17.986 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-3] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:17.986 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Received INIT_PRODUCER_ID response from node -1 for request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-3, correlationId=2, headerVersion=2): InitProducerIdResponseData(throttleTimeMs=0, errorCode=0, producerId=2, producerEpoch=0)
16:54:17.986 [kafka-producer-network-thread | producer-3] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-3] ProducerId set to 2 with epoch 0
16:54:17.986 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-3] Transition from state INITIALIZING to READY
16:54:18.088 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:18.088 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.088 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:18.088 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-3] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:18.088 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Completed connection to node 1. Fetching API versions.
16:54:18.088 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Initiating API versions fetch from node 1.
16:54:18.089 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-3, correlationId=3, headerVersion=2) and timeout 60000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.091 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-3, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.091 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.091 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='stream-record-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:18.091 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-3, correlationId=4, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='stream-record-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:18.092 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-3, correlationId=4, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='stream-record-test-topic', topicId=bRx9fV_rRbq8g8kI3oaQlw, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:18.092 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-3] Setting the last seen epoch of partition stream-record-test-topic-0 to 0 since the last known epoch was undefined.
16:54:18.092 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-3] Setting the last seen epoch of partition stream-record-test-topic-1 to 0 since the last known epoch was undefined.
16:54:18.092 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-3] Setting the last seen epoch of partition stream-record-test-topic-2 to 0 since the last known epoch was undefined.
16:54:18.092 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-3] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=stream-record-test-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=stream-record-test-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=stream-record-test-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:18.098 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-3] ProducerId of partition stream-record-test-topic-1 set to 2 with epoch 0. Reinitialize sequence at beginning.
16:54:18.098 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-3] Assigned producerId 2 and producerEpoch 0 to batch with base sequence 0 being sent to partition stream-record-test-topic-1
16:54:18.099 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-3, correlationId=5, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[stream-record-test-topic-1=375]}
16:54:18.102 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-3] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-3, correlationId=5, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='stream-record-test-topic', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.102 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-3] ProducerId: 2; Set last ack'd sequence number for topic-partition stream-record-test-topic-1 to 0
16:54:18.102 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16:54:18.102 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-3] Beginning shutdown of Kafka producer I/O thread, sending remaining records.
16:54:18.103 [kafka-producer-network-thread | producer-3] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-3] Shutdown of Kafka producer I/O thread has completed.
16:54:18.103 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:18.103 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:18.103 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:18.103 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.producer for producer-3 unregistered
16:54:18.103 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-3] Kafka producer has been closed
16:54:18.103 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending FindCoordinator request to broker localhost:50137 (id: -1 rack: null)
16:54:18.103 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.103 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:18.104 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:18.104 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Completed connection to node -1. Fetching API versions.
16:54:18.104 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Initiating API versions fetch from node -1.
16:54:18.104 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=1, headerVersion=2) and timeout 30000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.106 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=1, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.107 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.107 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='stream-record-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:18.107 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=2, headerVersion=2) and timeout 30000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='stream-record-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:18.107 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=0, headerVersion=2) and timeout 30000 to node -1: FindCoordinatorRequestData(key='', keyType=0, coordinatorKeys=[test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824])
16:54:18.110 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=2, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='stream-record-test-topic', topicId=bRx9fV_rRbq8g8kI3oaQlw, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:18.110 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Setting the last seen epoch of partition stream-record-test-topic-0 to 0 since the last known epoch was undefined.
16:54:18.110 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Setting the last seen epoch of partition stream-record-test-topic-1 to 0 since the last known epoch was undefined.
16:54:18.110 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Setting the last seen epoch of partition stream-record-test-topic-2 to 0 since the last known epoch was undefined.
16:54:18.110 [main] INFO org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:18.110 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=stream-record-test-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=stream-record-test-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=stream-record-test-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:18.111 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received FIND_COORDINATOR response from node -1 for request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=0, headerVersion=2): FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')])
16:54:18.111 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received FindCoordinator response ClientResponse(receivedTimeMs=1759524858111, latencyMs=8, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=0, headerVersion=2), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')]))
16:54:18.111 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Discovered group coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:18.111 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.111 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Initiating connection to node localhost:50137 (id: 2147483646 rack: null) using address localhost/127.0.0.1
16:54:18.111 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Executing onJoinPrepare with generation -1 and memberId 
16:54:18.111 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Marking assigned partitions pending for revocation: []
16:54:18.111 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending asynchronous auto-commit of offsets {}
16:54:18.111 [kafka-coordinator-heartbeat-thread | test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Heartbeat thread started
16:54:18.112 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 2147483646
16:54:18.112 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Completed connection to node 2147483646. Fetching API versions.
16:54:18.112 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Initiating API versions fetch from node 2147483646.
16:54:18.112 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=3, headerVersion=2) and timeout 30000 to node 2147483646: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.112 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] (Re-)joining group
16:54:18.112 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Joining group with current subscription: [stream-record-test-topic]
16:54:18.112 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:18.114 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received API_VERSIONS response from node 2147483646 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.114 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Node 2147483646 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.114 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=4, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')
16:54:18.114 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Completed asynchronous auto-commit of offsets {}
16:54:18.116 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=4, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=79, generationId=-1, protocolType=null, protocolName=null, leader='', skipAssignment=false, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', members=[])
16:54:18.116 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] JoinGroup failed due to non-fatal error: MEMBER_ID_REQUIRED. Will set the member id as consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003 and then rejoin. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
16:54:18.116 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Request joining group due to: need to re-join with the given member-id: consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003
16:54:18.116 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
16:54:18.116 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] (Re-)joining group
16:54:18.116 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Joining group with current subscription: [stream-record-test-topic]
16:54:18.116 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:18.116 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=5, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')
16:54:18.118 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=5, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', skipAssignment=false, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:18.119 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', skipAssignment=false, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:18.119 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Enabling heartbeat thread
16:54:18.119 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', protocol='range'}
16:54:18.119 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Performing assignment using strategy range with subscriptions {consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003=Subscription(topics=[stream-record-test-topic], ownedPartitions=[], groupInstanceId=null, generationId=-1, rackId=null)}
16:54:18.119 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Finished assignment for group at generation 1: {consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003=Assignment(partitions=[stream-record-test-topic-0, stream-record-test-topic-1, stream-record-test-topic-2])}
16:54:18.119 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending leader SyncGroup to coordinator localhost:50137 (id: 2147483646 rack: null): SyncGroupRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', generationId=1, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', assignment=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:18.119 [kafka-coordinator-heartbeat-thread | test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending SYNC_GROUP request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=6, headerVersion=2) and timeout 30000 to node 2147483646: SyncGroupRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', generationId=1, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', assignment=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:18.121 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received SYNC_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=6, headerVersion=2): SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:18.122 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received successful SyncGroup response: SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 1, 0, 24, 115, 116, 114, 101, 97, 109, 45, 114, 101, 99, 111, 114, 100, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:18.122 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', protocol='range'}
16:54:18.122 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Executing onJoinComplete with generation 1 and memberId consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003
16:54:18.122 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Notifying assignor about the new Assignment(partitions=[stream-record-test-topic-0, stream-record-test-topic-1, stream-record-test-topic-2])
16:54:18.122 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Adding newly assigned partitions: stream-record-test-topic-0, stream-record-test-topic-1, stream-record-test-topic-2
16:54:18.122 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Fetching committed offsets for partitions: [stream-record-test-topic-0, stream-record-test-topic-1, stream-record-test-topic-2]
16:54:18.122 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending OFFSET_FETCH request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=7, headerVersion=2) and timeout 30000 to node 2147483646: OffsetFetchRequestData(groupId='', topics=[], groups=[OffsetFetchRequestGroup(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', topics=[OffsetFetchRequestTopics(name='stream-record-test-topic', partitionIndexes=[0, 1, 2])])], requireStable=true)
16:54:18.124 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received OFFSET_FETCH response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=7, headerVersion=2): OffsetFetchResponseData(throttleTimeMs=0, topics=[], errorCode=0, groups=[OffsetFetchResponseGroup(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', topics=[OffsetFetchResponseTopics(name='stream-record-test-topic', partitions=[OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)])], errorCode=0)])
16:54:18.124 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Found no committed offset for partition stream-record-test-topic-0
16:54:18.124 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Found no committed offset for partition stream-record-test-topic-1
16:54:18.124 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Found no committed offset for partition stream-record-test-topic-2
16:54:18.124 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcher -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending ListOffsetRequest ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='stream-record-test-topic', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])]) to broker localhost:50137 (id: 1 rack: null)
16:54:18.124 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.124 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:18.125 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:18.125 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Completed connection to node 1. Fetching API versions.
16:54:18.125 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Initiating API versions fetch from node 1.
16:54:18.125 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=9, headerVersion=2) and timeout 30000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.127 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=9, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.127 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.127 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending LIST_OFFSETS request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=8, headerVersion=2) and timeout 30000 to node 1: ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='stream-record-test-topic', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])])
16:54:18.128 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received LIST_OFFSETS response from node 1 for request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=8, headerVersion=2): ListOffsetsResponseData(throttleTimeMs=0, topics=[ListOffsetsTopicResponse(name='stream-record-test-topic', partitions=[ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)])])
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Handling ListOffsetResponse response for stream-record-test-topic-0. Fetched offset 0, timestamp -1
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Handling ListOffsetResponse response for stream-record-test-topic-1. Fetched offset 0, timestamp -1
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Handling ListOffsetResponse response for stream-record-test-topic-2. Fetched offset 0, timestamp -1
16:54:18.128 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Not replacing existing epoch 0 with new epoch 0 for partition stream-record-test-topic-0
16:54:18.128 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Resetting offset for partition stream-record-test-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:18.128 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Not replacing existing epoch 0 with new epoch 0 for partition stream-record-test-topic-1
16:54:18.128 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Resetting offset for partition stream-record-test-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:18.128 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Not replacing existing epoch 0 with new epoch 0 for partition stream-record-test-topic-2
16:54:18.128 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Resetting offset for partition stream-record-test-topic-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Added READ_UNCOMMITTED fetch request for partition stream-record-test-topic-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Added READ_UNCOMMITTED fetch request for partition stream-record-test-topic-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Added READ_UNCOMMITTED fetch request for partition stream-record-test-topic-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.128 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Built full fetch (sessionId=INVALID, epoch=INITIAL) for node 1 with 3 partition(s).
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(stream-record-test-topic-0, stream-record-test-topic-1, stream-record-test-topic-2), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.128 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.129 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=10, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=0, sessionEpoch=0, topics=[FetchTopic(topic='stream-record-test-topic', topicId=bRx9fV_rRbq8g8kI3oaQlw, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.130 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=10, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=2004963117, responses=[FetchableTopicResponse(topic='', topicId=bRx9fV_rRbq8g8kI3oaQlw, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=453])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=375, buffer=java.nio.HeapByteBuffer[pos=0 lim=375 cap=415])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=3]))])])
16:54:18.130 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Node 1 sent a full fetch response that created a new incremental fetch session 2004963117 with 3 response partition(s)
16:54:18.130 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Fetch READ_UNCOMMITTED at offset 0 for partition stream-record-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=453]))
16:54:18.130 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Fetch READ_UNCOMMITTED at offset 0 for partition stream-record-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=375, buffer=java.nio.HeapByteBuffer[pos=0 lim=375 cap=415]))
16:54:18.130 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Fetch READ_UNCOMMITTED at offset 0 for partition stream-record-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=3]))
16:54:18.130 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.131 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Added READ_UNCOMMITTED fetch request for partition stream-record-test-topic-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.131 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Added READ_UNCOMMITTED fetch request for partition stream-record-test-topic-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.131 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Added READ_UNCOMMITTED fetch request for partition stream-record-test-topic-1 at position FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.131 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Built incremental fetch (sessionId=2004963117, epoch=1) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.131 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(stream-record-test-topic-1), toForget=(), toReplace=(), implied=(stream-record-test-topic-0, stream-record-test-topic-2), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.131 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.131 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=11, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=2004963117, sessionEpoch=1, topics=[FetchTopic(topic='stream-record-test-topic', topicId=bRx9fV_rRbq8g8kI3oaQlw, partitions=[FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=1, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
Successfully verified round-trip for StreamRecord: StreamRecord{stream=CUSTOMER_STREAM, action=INSERT, rowId=customer-123, columns=4}
16:54:18.132 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending synchronous auto-commit of offsets {stream-record-test-topic-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}, stream-record-test-topic-1=OffsetAndMetadata{offset=1, leaderEpoch=0, metadata=''}, stream-record-test-topic-2=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}}
16:54:18.132 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending OFFSET_COMMIT request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=12, headerVersion=2) and timeout 30000 to node 2147483646: OffsetCommitRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', generationIdOrMemberEpoch=1, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, retentionTimeMs=-1, topics=[OffsetCommitRequestTopic(name='stream-record-test-topic', partitions=[OffsetCommitRequestPartition(partitionIndex=0, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=1, committedOffset=1, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=2, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata='')])])
16:54:18.135 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received OFFSET_COMMIT response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=12, headerVersion=2): OffsetCommitResponseData(throttleTimeMs=0, topics=[OffsetCommitResponseTopic(name='stream-record-test-topic', partitions=[OffsetCommitResponsePartition(partitionIndex=0, errorCode=0), OffsetCommitResponsePartition(partitionIndex=1, errorCode=0), OffsetCommitResponsePartition(partitionIndex=2, errorCode=0)])])
16:54:18.135 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Committed offset 0 for partition stream-record-test-topic-0
16:54:18.135 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Committed offset 1 for partition stream-record-test-topic-1
16:54:18.135 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Committed offset 0 for partition stream-record-test-topic-2
16:54:18.135 [kafka-coordinator-heartbeat-thread | test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Heartbeat thread has closed
16:54:18.136 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Executing onLeavePrepare with generation Generation{generationId=1, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', protocol='range'}
16:54:18.136 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Revoke previously assigned partitions stream-record-test-topic-0, stream-record-test-topic-1, stream-record-test-topic-2
16:54:18.136 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Member consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003 sending LeaveGroup request to coordinator localhost:50137 (id: 2147483646 rack: null) due to the consumer is being closed
16:54:18.136 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending LEAVE_GROUP request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=13, headerVersion=2) and timeout 30000 to node 2147483646: LeaveGroupRequestData(groupId='test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824', memberId='', members=[MemberIdentity(memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, reason='the consumer is being closed')])
16:54:18.136 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:18.136 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Request joining group due to: consumer pro-actively leaving the group
16:54:18.138 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received LEAVE_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=13, headerVersion=2): LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, errorCode=0)])
16:54:18.138 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] LeaveGroup response with Generation{generationId=1, memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', protocol='range'} returned successfully: ClientResponse(receivedTimeMs=1759524858137, latencyMs=1, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=13, headerVersion=2), responseBody=LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6-2a871243-d357-4a6a-a5e6-f60bbb57a003', groupInstanceId=null, errorCode=0)]))
16:54:18.138 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Set the metadata for next fetch request to close the existing session ID=2004963117
16:54:18.138 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Built full fetch (sessionId=2004963117, epoch=FINAL) for node 1 with 0 partition(s).
16:54:18.138 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.138 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.138 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=14, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=2004963117, sessionEpoch=-1, topics=[], forgottenTopicsData=[], rackId='')
16:54:18.652 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=11, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=2004963117, responses=[])
16:54:18.652 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Node 1 sent a full fetch response that created a new incremental fetch session 2004963117 with 0 response partition(s)
16:54:18.652 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.653 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, correlationId=14, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=0, responses=[])
16:54:18.653 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Successfully sent a close message for fetch session: 2004963117 to node: localhost:50137 (id: 1 rack: null)
16:54:18.656 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:18.656 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:18.657 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:18.663 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6 unregistered
16:54:18.663 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824-6, groupId=test-consumer-group-7d10ec87-9f3e-42ae-a05b-b71e3e83d824] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testHighThroughputScenario" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="0.718">
    <system-out><![CDATA[16:54:18.664 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:18.664 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initializing the Kafka consumer
16:54:18.666 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:18.666 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:18.666 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524858666
16:54:18.666 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Kafka consumer initialized
16:54:18.666 [main] INFO org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Subscribed to topic(s): high-throughput-test-topic
16:54:18.666 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FindCoordinator request to broker localhost:50137 (id: -1 rack: null)
16:54:18.666 [Thread-14] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.666 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:18.667 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 131072
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	buffer.memory = 268435456
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = zstd
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 300000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 60000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

16:54:18.667 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-4] Instantiated an idempotent producer.
16:54:18.667 [Thread-14] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:18.667 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Completed connection to node -1. Fetching API versions.
16:54:18.667 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initiating API versions fetch from node -1.
16:54:18.667 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=1, headerVersion=2) and timeout 30000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.668 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:18.668 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:18.668 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524858668
16:54:18.668 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-4] Kafka producer started
16:54:18.668 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-4] Starting Kafka producer I/O thread.
16:54:18.668 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] Transition from state UNINITIALIZED to INITIALIZING
16:54:18.668 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] Enqueuing transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:18.668 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Initialize connection to node localhost:50137 (id: -1 rack: null) for sending metadata request
16:54:18.668 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.668 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:18.669 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-4] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:18.669 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Completed connection to node -1. Fetching API versions.
16:54:18.669 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Initiating API versions fetch from node -1.
16:54:18.669 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-4, correlationId=0, headerVersion=2) and timeout 60000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.670 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=1, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.670 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.670 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:18.670 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=2, headerVersion=2) and timeout 30000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:18.670 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=0, headerVersion=2) and timeout 30000 to node -1: FindCoordinatorRequestData(key='', keyType=0, coordinatorKeys=[test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5])
16:54:18.672 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-4, correlationId=0, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.672 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.672 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:18.672 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-4, correlationId=1, headerVersion=2) and timeout 60000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:18.673 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-4] Sending transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) to node localhost:50137 (id: -1 rack: null) with correlation ID 2
16:54:18.674 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending INIT_PRODUCER_ID request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-4, correlationId=2, headerVersion=2) and timeout 60000 to node -1: InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:18.676 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-4, correlationId=1, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=3, name='high-throughput-test-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:18.676 [kafka-producer-network-thread | producer-4] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {high-throughput-test-topic=UNKNOWN_TOPIC_OR_PARTITION}
16:54:18.676 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-4] Requesting metadata update for topic high-throughput-test-topic due to error UNKNOWN_TOPIC_OR_PARTITION
16:54:18.676 [kafka-producer-network-thread | producer-4] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-4] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:18.676 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-4] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:18.676 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received INIT_PRODUCER_ID response from node -1 for request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-4, correlationId=2, headerVersion=2): InitProducerIdResponseData(throttleTimeMs=0, errorCode=0, producerId=3, producerEpoch=0)
16:54:18.676 [kafka-producer-network-thread | producer-4] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId set to 3 with epoch 0
16:54:18.676 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] Transition from state INITIALIZING to READY
16:54:18.684 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=2, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='high-throughput-test-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:18.685 [Thread-14] WARN org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Error while fetching metadata with correlation id 2 : {high-throughput-test-topic=LEADER_NOT_AVAILABLE}
16:54:18.685 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Requesting metadata update for topic high-throughput-test-topic due to error LEADER_NOT_AVAILABLE
16:54:18.685 [Thread-14] INFO org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:18.685 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:18.686 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FIND_COORDINATOR response from node -1 for request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=0, headerVersion=2): FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')])
16:54:18.686 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FindCoordinator response ClientResponse(receivedTimeMs=1759524858686, latencyMs=20, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=0, headerVersion=2), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')]))
16:54:18.686 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Discovered group coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:18.686 [Thread-14] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.686 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initiating connection to node localhost:50137 (id: 2147483646 rack: null) using address localhost/127.0.0.1
16:54:18.687 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Executing onJoinPrepare with generation -1 and memberId 
16:54:18.687 [kafka-coordinator-heartbeat-thread | test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Heartbeat thread started
16:54:18.687 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Marking assigned partitions pending for revocation: []
16:54:18.687 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending asynchronous auto-commit of offsets {}
16:54:18.688 [Thread-14] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 2147483646
16:54:18.688 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Completed connection to node 2147483646. Fetching API versions.
16:54:18.688 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initiating API versions fetch from node 2147483646.
16:54:18.688 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=3, headerVersion=2) and timeout 30000 to node 2147483646: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.688 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] (Re-)joining group
16:54:18.688 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Joining group with current subscription: [high-throughput-test-topic]
16:54:18.688 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:18.690 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received API_VERSIONS response from node 2147483646 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.691 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 2147483646 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.691 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=4, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')
16:54:18.691 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Completed asynchronous auto-commit of offsets {}
16:54:18.693 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=4, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=79, generationId=-1, protocolType=null, protocolName=null, leader='', skipAssignment=false, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', members=[])
16:54:18.693 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] JoinGroup failed due to non-fatal error: MEMBER_ID_REQUIRED. Will set the member id as consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3 and then rejoin. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
16:54:18.693 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Request joining group due to: need to re-join with the given member-id: consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3
16:54:18.693 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
16:54:18.693 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] (Re-)joining group
16:54:18.693 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Joining group with current subscription: [high-throughput-test-topic]
16:54:18.693 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:18.693 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=5, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')
16:54:18.697 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=5, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', skipAssignment=false, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:18.697 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', skipAssignment=false, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:18.697 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Enabling heartbeat thread
16:54:18.697 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', protocol='range'}
16:54:18.778 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:18.778 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.778 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:18.780 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-4] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:18.780 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Completed connection to node 1. Fetching API versions.
16:54:18.780 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Initiating API versions fetch from node 1.
16:54:18.780 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-4, correlationId=3, headerVersion=2) and timeout 60000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.782 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-4, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.782 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.782 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:18.782 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-4, correlationId=4, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:18.783 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-4, correlationId=4, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:18.784 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-4] Setting the last seen epoch of partition high-throughput-test-topic-0 to 0 since the last known epoch was undefined.
16:54:18.784 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-4] Setting the last seen epoch of partition high-throughput-test-topic-1 to 0 since the last known epoch was undefined.
16:54:18.784 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-4] Setting the last seen epoch of partition high-throughput-test-topic-2 to 0 since the last known epoch was undefined.
16:54:18.784 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-4] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=high-throughput-test-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=high-throughput-test-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=high-throughput-test-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:18.785 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:18.785 [Thread-14] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:18.785 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:18.786 [Thread-14] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:18.786 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Completed connection to node 1. Fetching API versions.
16:54:18.786 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Initiating API versions fetch from node 1.
16:54:18.786 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=6, headerVersion=2) and timeout 30000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:18.788 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=6, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:18.788 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:18.789 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:18.789 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=7, headerVersion=2) and timeout 30000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='high-throughput-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=7, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Setting the last seen epoch of partition high-throughput-test-topic-0 to 0 since the last known epoch was undefined.
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Setting the last seen epoch of partition high-throughput-test-topic-1 to 0 since the last known epoch was undefined.
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Setting the last seen epoch of partition high-throughput-test-topic-2 to 0 since the last known epoch was undefined.
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=high-throughput-test-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=high-throughput-test-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=high-throughput-test-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Performing assignment using strategy range with subscriptions {consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3=Subscription(topics=[high-throughput-test-topic], ownedPartitions=[], groupInstanceId=null, generationId=-1, rackId=null)}
16:54:18.791 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Finished assignment for group at generation 1: {consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3=Assignment(partitions=[high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2])}
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending leader SyncGroup to coordinator localhost:50137 (id: 2147483646 rack: null): SyncGroupRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', generationId=1, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', assignment=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:18.791 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending SYNC_GROUP request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=8, headerVersion=2) and timeout 30000 to node 2147483646: SyncGroupRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', generationId=1, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', assignment=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:18.792 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId of partition high-throughput-test-topic-0 set to 3 with epoch 0. Reinitialize sequence at beginning.
16:54:18.792 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 0 being sent to partition high-throughput-test-topic-0
16:54:18.792 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId of partition high-throughput-test-topic-1 set to 3 with epoch 0. Reinitialize sequence at beginning.
16:54:18.792 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 0 being sent to partition high-throughput-test-topic-1
16:54:18.792 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId of partition high-throughput-test-topic-2 set to 3 with epoch 0. Reinitialize sequence at beginning.
16:54:18.792 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 0 being sent to partition high-throughput-test-topic-2
16:54:18.793 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=5, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=520,high-throughput-test-topic-1=679,high-throughput-test-topic-2=736]}
16:54:18.796 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=5, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.796 [kafka-coordinator-heartbeat-thread | test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received SYNC_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=8, headerVersion=2): SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:18.797 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 7
16:54:18.797 [kafka-coordinator-heartbeat-thread | test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received successful SyncGroup response: SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 1, 0, 26, 104, 105, 103, 104, 45, 116, 104, 114, 111, 117, 103, 104, 112, 117, 116, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:18.797 [kafka-coordinator-heartbeat-thread | test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', protocol='range'}
16:54:18.797 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 14
16:54:18.797 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 18
16:54:18.797 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 19 being sent to partition high-throughput-test-topic-2
16:54:18.797 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 8 being sent to partition high-throughput-test-topic-0
16:54:18.797 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 15 being sent to partition high-throughput-test-topic-1
16:54:18.798 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Executing onJoinComplete with generation 1 and memberId consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3
16:54:18.798 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Notifying assignor about the new Assignment(partitions=[high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2])
16:54:18.798 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding newly assigned partitions: high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2
16:54:18.798 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=6, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=508,high-throughput-test-topic-1=558,high-throughput-test-topic-2=542]}
16:54:18.798 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetching committed offsets for partitions: [high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2]
16:54:18.798 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending OFFSET_FETCH request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=9, headerVersion=2) and timeout 30000 to node 2147483646: OffsetFetchRequestData(groupId='', topics=[], groups=[OffsetFetchRequestGroup(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', topics=[OffsetFetchRequestTopics(name='high-throughput-test-topic', partitionIndexes=[0, 1, 2])])], requireStable=true)
16:54:18.800 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received OFFSET_FETCH response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=9, headerVersion=2): OffsetFetchResponseData(throttleTimeMs=0, topics=[], errorCode=0, groups=[OffsetFetchResponseGroup(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', topics=[OffsetFetchResponseTopics(name='high-throughput-test-topic', partitions=[OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)])], errorCode=0)])
16:54:18.800 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Found no committed offset for partition high-throughput-test-topic-0
16:54:18.801 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Found no committed offset for partition high-throughput-test-topic-1
16:54:18.801 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Found no committed offset for partition high-throughput-test-topic-2
16:54:18.801 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcher -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending ListOffsetRequest ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='high-throughput-test-topic', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])]) to broker localhost:50137 (id: 1 rack: null)
16:54:18.801 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending LIST_OFFSETS request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=10, headerVersion=2) and timeout 30000 to node 1: ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='high-throughput-test-topic', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])])
16:54:18.802 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received LIST_OFFSETS response from node 1 for request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=10, headerVersion=2): ListOffsetsResponseData(throttleTimeMs=0, topics=[ListOffsetsTopicResponse(name='high-throughput-test-topic', partitions=[ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)])])
16:54:18.802 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 26 being sent to partition high-throughput-test-topic-1
16:54:18.802 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Handling ListOffsetResponse response for high-throughput-test-topic-0. Fetched offset 0, timestamp -1
16:54:18.802 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Handling ListOffsetResponse response for high-throughput-test-topic-1. Fetched offset 0, timestamp -1
16:54:18.802 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Handling ListOffsetResponse response for high-throughput-test-topic-2. Fetched offset 0, timestamp -1
16:54:18.802 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Not replacing existing epoch 0 with new epoch 0 for partition high-throughput-test-topic-0
16:54:18.802 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Resetting offset for partition high-throughput-test-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:18.802 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Not replacing existing epoch 0 with new epoch 0 for partition high-throughput-test-topic-1
16:54:18.802 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Resetting offset for partition high-throughput-test-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:18.802 [Thread-14] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Not replacing existing epoch 0 with new epoch 0 for partition high-throughput-test-topic-2
16:54:18.802 [Thread-14] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Resetting offset for partition high-throughput-test-topic-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:18.803 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.803 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.803 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.803 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built full fetch (sessionId=INVALID, epoch=INITIAL) for node 1 with 3 partition(s).
16:54:18.803 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.803 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.803 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 29 being sent to partition high-throughput-test-topic-2
16:54:18.803 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=11, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=0, sessionEpoch=0, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.803 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 17 being sent to partition high-throughput-test-topic-0
16:54:18.803 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=7, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=735,high-throughput-test-topic-1=687,high-throughput-test-topic-2=723]}
16:54:18.805 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=6, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=8, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=15, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=19, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.805 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 16
16:54:18.805 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 25
16:54:18.805 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 28
16:54:18.806 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=11, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=17, lastStableOffset=17, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1028, buffer=java.nio.HeapByteBuffer[pos=0 lim=1028 cap=3622])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=26, lastStableOffset=26, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1237, buffer=java.nio.HeapByteBuffer[pos=0 lim=1237 cap=2556])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=29, lastStableOffset=29, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1278, buffer=java.nio.HeapByteBuffer[pos=0 lim=1278 cap=1281]))])])
16:54:18.806 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent a full fetch response that created a new incremental fetch session 182544646 with 3 response partition(s)
16:54:18.806 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 0 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=17, lastStableOffset=17, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1028, buffer=java.nio.HeapByteBuffer[pos=0 lim=1028 cap=3622]))
16:54:18.806 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 0 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=26, lastStableOffset=26, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1237, buffer=java.nio.HeapByteBuffer[pos=0 lim=1237 cap=2556]))
16:54:18.806 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 0 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=29, lastStableOffset=29, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1278, buffer=java.nio.HeapByteBuffer[pos=0 lim=1278 cap=1281]))
16:54:18.806 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.807 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=17, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.807 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=26, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.807 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=29, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.807 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=1) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.807 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.807 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.808 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=12, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=1, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=17, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=26, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=29, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.810 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 35 being sent to partition high-throughput-test-topic-0
16:54:18.810 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 42 being sent to partition high-throughput-test-topic-1
16:54:18.811 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 48 being sent to partition high-throughput-test-topic-2
16:54:18.811 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=8, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=839,high-throughput-test-topic-1=955,high-throughput-test-topic-2=828]}
16:54:18.811 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=7, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=17, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=26, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=29, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.811 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 34
16:54:18.812 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 41
16:54:18.812 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 47
16:54:18.813 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=12, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=35, lastStableOffset=35, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=735, buffer=java.nio.HeapByteBuffer[pos=0 lim=735 cap=2224])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=42, lastStableOffset=42, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=687, buffer=java.nio.HeapByteBuffer[pos=0 lim=687 cap=1451])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=48, lastStableOffset=48, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=723, buffer=java.nio.HeapByteBuffer[pos=0 lim=723 cap=726]))])])
16:54:18.813 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.813 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 17 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=35, lastStableOffset=35, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=735, buffer=java.nio.HeapByteBuffer[pos=0 lim=735 cap=2224]))
16:54:18.813 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 26 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=42, lastStableOffset=42, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=687, buffer=java.nio.HeapByteBuffer[pos=0 lim=687 cap=1451]))
16:54:18.813 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 29 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=48, lastStableOffset=48, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=723, buffer=java.nio.HeapByteBuffer[pos=0 lim=723 cap=726]))
16:54:18.813 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.815 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=35, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.815 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=42, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.815 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=48, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.815 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=2) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.815 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.815 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.815 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=13, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=2, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=35, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=42, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=48, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.816 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 71 being sent to partition high-throughput-test-topic-2
16:54:18.817 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 60 being sent to partition high-throughput-test-topic-0
16:54:18.817 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 74 being sent to partition high-throughput-test-topic-1
16:54:18.817 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=9, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=720,high-throughput-test-topic-1=692,high-throughput-test-topic-2=591]}
16:54:18.820 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=8, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=35, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=42, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=48, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.821 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 59
16:54:18.821 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 73
16:54:18.821 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 70
16:54:18.821 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 93 being sent to partition high-throughput-test-topic-1
16:54:18.821 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 85 being sent to partition high-throughput-test-topic-2
16:54:18.821 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 80 being sent to partition high-throughput-test-topic-0
16:54:18.821 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=13, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=60, lastStableOffset=60, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=839, buffer=java.nio.HeapByteBuffer[pos=0 lim=839 cap=2701])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=74, lastStableOffset=74, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=955, buffer=java.nio.HeapByteBuffer[pos=0 lim=955 cap=1824])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=71, lastStableOffset=71, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=828, buffer=java.nio.HeapByteBuffer[pos=0 lim=828 cap=831]))])])
16:54:18.822 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=10, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=791,high-throughput-test-topic-1=805,high-throughput-test-topic-2=1031]}
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 35 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=60, lastStableOffset=60, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=839, buffer=java.nio.HeapByteBuffer[pos=0 lim=839 cap=2701]))
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 42 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=74, lastStableOffset=74, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=955, buffer=java.nio.HeapByteBuffer[pos=0 lim=955 cap=1824]))
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 48 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=71, lastStableOffset=71, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=828, buffer=java.nio.HeapByteBuffer[pos=0 lim=828 cap=831]))
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=60, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=74, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=71, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=3) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.822 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=14, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=3, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=60, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=74, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=71, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.823 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=9, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=60, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=74, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=71, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.823 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 79
16:54:18.823 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 92
16:54:18.823 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 84
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=14, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=80, lastStableOffset=80, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=720, buffer=java.nio.HeapByteBuffer[pos=0 lim=720 cap=2082])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=93, lastStableOffset=93, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=692, buffer=java.nio.HeapByteBuffer[pos=0 lim=692 cap=1324])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=85, lastStableOffset=85, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=591, buffer=java.nio.HeapByteBuffer[pos=0 lim=591 cap=594]))])])
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 60 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=80, lastStableOffset=80, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=720, buffer=java.nio.HeapByteBuffer[pos=0 lim=720 cap=2082]))
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 74 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=93, lastStableOffset=93, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=692, buffer=java.nio.HeapByteBuffer[pos=0 lim=692 cap=1324]))
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 71 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=85, lastStableOffset=85, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=591, buffer=java.nio.HeapByteBuffer[pos=0 lim=591 cap=594]))
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.826 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=10, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=80, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=93, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=85, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.826 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 100
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=80, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=93, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.826 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=85, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.827 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=4) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.827 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 114
16:54:18.827 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.827 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.827 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 116
16:54:18.827 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=15, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=4, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=80, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=93, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=85, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.827 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 101 being sent to partition high-throughput-test-topic-0
16:54:18.827 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 115 being sent to partition high-throughput-test-topic-1
16:54:18.827 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 117 being sent to partition high-throughput-test-topic-2
16:54:18.827 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=11, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=1291,high-throughput-test-topic-1=1411,high-throughput-test-topic-2=1114]}
16:54:18.830 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=15, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=101, lastStableOffset=101, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=791, buffer=java.nio.HeapByteBuffer[pos=0 lim=791 cap=2706])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=115, lastStableOffset=115, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=805, buffer=java.nio.HeapByteBuffer[pos=0 lim=805 cap=1877])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=117, lastStableOffset=117, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1031, buffer=java.nio.HeapByteBuffer[pos=0 lim=1031 cap=1034]))])])
16:54:18.830 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.830 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 80 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=101, lastStableOffset=101, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=791, buffer=java.nio.HeapByteBuffer[pos=0 lim=791 cap=2706]))
16:54:18.830 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 93 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=115, lastStableOffset=115, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=805, buffer=java.nio.HeapByteBuffer[pos=0 lim=805 cap=1877]))
16:54:18.830 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 85 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=117, lastStableOffset=117, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1031, buffer=java.nio.HeapByteBuffer[pos=0 lim=1031 cap=1034]))
16:54:18.830 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.831 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=101, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.831 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=115, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.831 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=117, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.831 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=5) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.831 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.831 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.831 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=16, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=5, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=101, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=115, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=117, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.833 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 156 being sent to partition high-throughput-test-topic-2
16:54:18.833 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 149 being sent to partition high-throughput-test-topic-0
16:54:18.834 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 168 being sent to partition high-throughput-test-topic-1
16:54:18.834 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=12, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=1504,high-throughput-test-topic-1=1556,high-throughput-test-topic-2=1422]}
16:54:18.836 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=11, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=101, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=115, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=117, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.836 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 148
16:54:18.836 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=16, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=149, lastStableOffset=149, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1291, buffer=java.nio.HeapByteBuffer[pos=0 lim=1291 cap=3895])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=168, lastStableOffset=168, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1411, buffer=java.nio.HeapByteBuffer[pos=0 lim=1411 cap=2566])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=156, lastStableOffset=156, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1114, buffer=java.nio.HeapByteBuffer[pos=0 lim=1114 cap=1117]))])])
16:54:18.836 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 167
16:54:18.836 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.836 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 155
16:54:18.836 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 101 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=149, lastStableOffset=149, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1291, buffer=java.nio.HeapByteBuffer[pos=0 lim=1291 cap=3895]))
16:54:18.837 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 115 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=168, lastStableOffset=168, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1411, buffer=java.nio.HeapByteBuffer[pos=0 lim=1411 cap=2566]))
16:54:18.837 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 117 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=156, lastStableOffset=156, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1114, buffer=java.nio.HeapByteBuffer[pos=0 lim=1114 cap=1117]))
16:54:18.837 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.838 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=149, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.838 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=168, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.838 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=156, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.838 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=6) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.838 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.838 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.838 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=17, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=6, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=149, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=168, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=156, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.839 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 227 being sent to partition high-throughput-test-topic-1
16:54:18.840 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 207 being sent to partition high-throughput-test-topic-2
16:54:18.840 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 205 being sent to partition high-throughput-test-topic-0
16:54:18.840 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=13, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=1034,high-throughput-test-topic-1=879,high-throughput-test-topic-2=1159]}
16:54:18.841 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=12, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=149, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=168, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=156, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.841 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 204
16:54:18.842 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 226
16:54:18.842 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 206
16:54:18.842 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=17, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=205, lastStableOffset=205, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1504, buffer=java.nio.HeapByteBuffer[pos=0 lim=1504 cap=4561])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=227, lastStableOffset=227, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1556, buffer=java.nio.HeapByteBuffer[pos=0 lim=1556 cap=3019])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=207, lastStableOffset=207, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1422, buffer=java.nio.HeapByteBuffer[pos=0 lim=1422 cap=1425]))])])
16:54:18.842 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.842 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 149 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=205, lastStableOffset=205, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1504, buffer=java.nio.HeapByteBuffer[pos=0 lim=1504 cap=4561]))
16:54:18.842 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 168 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=227, lastStableOffset=227, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1556, buffer=java.nio.HeapByteBuffer[pos=0 lim=1556 cap=3019]))
16:54:18.842 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 156 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=207, lastStableOffset=207, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1422, buffer=java.nio.HeapByteBuffer[pos=0 lim=1422 cap=1425]))
16:54:18.842 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.845 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=205, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.845 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=227, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.845 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=207, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.845 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=7) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.845 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.845 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.845 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=18, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=7, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=205, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=227, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=207, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.846 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 239 being sent to partition high-throughput-test-topic-0
16:54:18.846 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 253 being sent to partition high-throughput-test-topic-1
16:54:18.846 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 247 being sent to partition high-throughput-test-topic-2
16:54:18.847 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=14, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=1740,high-throughput-test-topic-1=1243,high-throughput-test-topic-2=1472]}
16:54:18.847 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=13, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=205, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=227, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=207, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.847 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 238
16:54:18.847 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 252
16:54:18.848 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 246
16:54:18.849 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=18, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=239, lastStableOffset=239, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1034, buffer=java.nio.HeapByteBuffer[pos=0 lim=1034 cap=3151])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=253, lastStableOffset=253, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=879, buffer=java.nio.HeapByteBuffer[pos=0 lim=879 cap=2079])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=247, lastStableOffset=247, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1159, buffer=java.nio.HeapByteBuffer[pos=0 lim=1159 cap=1162]))])])
16:54:18.849 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.849 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 205 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=239, lastStableOffset=239, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1034, buffer=java.nio.HeapByteBuffer[pos=0 lim=1034 cap=3151]))
16:54:18.849 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 227 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=253, lastStableOffset=253, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=879, buffer=java.nio.HeapByteBuffer[pos=0 lim=879 cap=2079]))
16:54:18.849 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 207 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=247, lastStableOffset=247, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1159, buffer=java.nio.HeapByteBuffer[pos=0 lim=1159 cap=1162]))
16:54:18.849 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.852 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 302 being sent to partition high-throughput-test-topic-2
16:54:18.852 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=239, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.852 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=253, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.852 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=247, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.852 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=8) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.852 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.852 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.852 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 306 being sent to partition high-throughput-test-topic-0
16:54:18.852 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=19, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=8, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=239, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=253, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=247, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.852 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-4] Assigned producerId 3 and producerEpoch 0 to batch with base sequence 298 being sent to partition high-throughput-test-topic-1
16:54:18.852 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=15, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[high-throughput-test-topic-0=1133,high-throughput-test-topic-1=797,high-throughput-test-topic-2=889]}
16:54:18.856 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=14, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=239, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=253, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=247, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.856 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 305
16:54:18.856 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 297
16:54:18.857 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 301
16:54:18.857 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=19, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=306, lastStableOffset=306, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1740, buffer=java.nio.HeapByteBuffer[pos=0 lim=1740 cap=4534])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=298, lastStableOffset=298, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1243, buffer=java.nio.HeapByteBuffer[pos=0 lim=1243 cap=2756])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=302, lastStableOffset=302, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1472, buffer=java.nio.HeapByteBuffer[pos=0 lim=1472 cap=1475]))])])
16:54:18.857 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.857 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-4] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-4, correlationId=15, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='high-throughput-test-topic', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=306, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=1, errorCode=0, baseOffset=298, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null), PartitionProduceResponse(index=2, errorCode=0, baseOffset=302, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:18.857 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 239 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=306, lastStableOffset=306, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1740, buffer=java.nio.HeapByteBuffer[pos=0 lim=1740 cap=4534]))
16:54:18.857 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-0 to 347
16:54:18.857 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 253 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=298, lastStableOffset=298, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1243, buffer=java.nio.HeapByteBuffer[pos=0 lim=1243 cap=2756]))
16:54:18.857 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 247 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=302, lastStableOffset=302, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1472, buffer=java.nio.HeapByteBuffer[pos=0 lim=1472 cap=1475]))
16:54:18.857 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.857 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-1 to 320
16:54:18.858 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-4] ProducerId: 3; Set last ack'd sequence number for topic-partition high-throughput-test-topic-2 to 330
16:54:18.858 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16:54:18.858 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-4] Beginning shutdown of Kafka producer I/O thread, sending remaining records.
16:54:18.858 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=306, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.858 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=298, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.858 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=302, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.858 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=9) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.858 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.858 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.858 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=20, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=9, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=306, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=298, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=302, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:18.859 [kafka-producer-network-thread | producer-4] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-4] Shutdown of Kafka producer I/O thread has completed.
16:54:18.859 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:18.859 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:18.859 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:18.859 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.producer for producer-4 unregistered
16:54:18.859 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-4] Kafka producer has been closed
16:54:18.861 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=20, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[FetchableTopicResponse(topic='', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=348, lastStableOffset=348, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1133, buffer=java.nio.HeapByteBuffer[pos=0 lim=1133 cap=2898])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=321, lastStableOffset=321, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=797, buffer=java.nio.HeapByteBuffer[pos=0 lim=797 cap=1727])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=331, lastStableOffset=331, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=889, buffer=java.nio.HeapByteBuffer[pos=0 lim=889 cap=892]))])])
16:54:18.861 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent an incremental fetch response with throttleTimeMs = 0 for session 182544646 with 3 response partition(s)
16:54:18.861 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 306 for partition high-throughput-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=348, lastStableOffset=348, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1133, buffer=java.nio.HeapByteBuffer[pos=0 lim=1133 cap=2898]))
16:54:18.861 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 298 for partition high-throughput-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=321, lastStableOffset=321, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=797, buffer=java.nio.HeapByteBuffer[pos=0 lim=797 cap=1727]))
16:54:18.861 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Fetch READ_UNCOMMITTED at offset 302 for partition high-throughput-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=331, lastStableOffset=331, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=889, buffer=java.nio.HeapByteBuffer[pos=0 lim=889 cap=892]))
16:54:18.861 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.862 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-0 at position FetchPosition{offset=348, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.862 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-1 at position FetchPosition{offset=321, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.862 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Added READ_UNCOMMITTED fetch request for partition high-throughput-test-topic-2 at position FetchPosition{offset=331, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:18.862 [Thread-14] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built incremental fetch (sessionId=182544646, epoch=10) for node 1. Added 0 partition(s), altered 3 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 3 partition(s)
16:54:18.862 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.862 [Thread-14] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.862 [Thread-14] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=21, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=10, topics=[FetchTopic(topic='high-throughput-test-topic', topicId=38aehcIdTzyMQ8DAMtpvsg, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=348, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=321, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=331, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
Throughput test completed:
  Records sent: 1000
Consumer received 1000 unique records
  Send duration: 193ms
  Throughput: 5181.35 records/second
16:54:18.863 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending synchronous auto-commit of offsets {high-throughput-test-topic-0=OffsetAndMetadata{offset=348, leaderEpoch=0, metadata=''}, high-throughput-test-topic-1=OffsetAndMetadata{offset=321, leaderEpoch=0, metadata=''}, high-throughput-test-topic-2=OffsetAndMetadata{offset=331, leaderEpoch=0, metadata=''}}
16:54:18.863 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending OFFSET_COMMIT request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=22, headerVersion=2) and timeout 30000 to node 2147483646: OffsetCommitRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', generationIdOrMemberEpoch=1, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, retentionTimeMs=-1, topics=[OffsetCommitRequestTopic(name='high-throughput-test-topic', partitions=[OffsetCommitRequestPartition(partitionIndex=0, committedOffset=348, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=1, committedOffset=321, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=2, committedOffset=331, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata='')])])
16:54:18.866 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received OFFSET_COMMIT response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=22, headerVersion=2): OffsetCommitResponseData(throttleTimeMs=0, topics=[OffsetCommitResponseTopic(name='high-throughput-test-topic', partitions=[OffsetCommitResponsePartition(partitionIndex=0, errorCode=0), OffsetCommitResponsePartition(partitionIndex=1, errorCode=0), OffsetCommitResponsePartition(partitionIndex=2, errorCode=0)])])
16:54:18.866 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Committed offset 348 for partition high-throughput-test-topic-0
16:54:18.866 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Committed offset 321 for partition high-throughput-test-topic-1
16:54:18.866 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Committed offset 331 for partition high-throughput-test-topic-2
16:54:18.866 [kafka-coordinator-heartbeat-thread | test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Heartbeat thread has closed
16:54:18.866 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Executing onLeavePrepare with generation Generation{generationId=1, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', protocol='range'}
16:54:18.866 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Revoke previously assigned partitions high-throughput-test-topic-0, high-throughput-test-topic-1, high-throughput-test-topic-2
16:54:18.867 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Member consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3 sending LeaveGroup request to coordinator localhost:50137 (id: 2147483646 rack: null) due to the consumer is being closed
16:54:18.867 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending LEAVE_GROUP request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=23, headerVersion=2) and timeout 30000 to node 2147483646: LeaveGroupRequestData(groupId='test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5', memberId='', members=[MemberIdentity(memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, reason='the consumer is being closed')])
16:54:18.867 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:18.867 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Request joining group due to: consumer pro-actively leaving the group
16:54:18.868 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received LEAVE_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=23, headerVersion=2): LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, errorCode=0)])
16:54:18.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] LeaveGroup response with Generation{generationId=1, memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', protocol='range'} returned successfully: ClientResponse(receivedTimeMs=1759524858868, latencyMs=1, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=23, headerVersion=2), responseBody=LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7-f21a43e1-31d1-4420-b10a-c484b9787fc3', groupInstanceId=null, errorCode=0)]))
16:54:18.868 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Set the metadata for next fetch request to close the existing session ID=182544646
16:54:18.868 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Built full fetch (sessionId=182544646, epoch=FINAL) for node 1 with 0 partition(s).
16:54:18.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:18.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:18.868 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=24, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=182544646, sessionEpoch=-1, topics=[], forgottenTopicsData=[], rackId='')
16:54:19.369 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=21, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=182544646, responses=[])
16:54:19.369 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Node 1 sent a full fetch response that created a new incremental fetch session 182544646 with 0 response partition(s)
16:54:19.370 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:19.371 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, correlationId=24, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=0, responses=[])
16:54:19.372 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Successfully sent a close message for fetch session: 182544646 to node: localhost:50137 (id: 1 rack: null)
16:54:19.372 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:19.372 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:19.372 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:19.380 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7 unregistered
16:54:19.380 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5-7, groupId=test-consumer-group-784f32d5-3acf-40ab-a197-13d8f46f7cd5] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testErrorHandlingScenario" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="0.823">
    <system-out><![CDATA[16:54:19.386 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:19.386 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initializing the Kafka consumer
16:54:19.391 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:19.391 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:19.391 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524859391
16:54:19.392 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Kafka consumer initialized
16:54:19.392 [main] INFO org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Subscribed to topic(s): error-handling-test-topic, snowflake-cdc-errors
16:54:19.394 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 131072
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	buffer.memory = 268435456
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = zstd
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 300000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 60000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

16:54:19.394 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-5] Instantiated an idempotent producer.
16:54:19.396 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:19.396 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:19.396 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524859396
16:54:19.396 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-5] Kafka producer started
16:54:19.396 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-5] Starting Kafka producer I/O thread.
16:54:19.396 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] Transition from state UNINITIALIZED to INITIALIZING
16:54:19.396 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] Enqueuing transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:19.397 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Initialize connection to node localhost:50137 (id: -1 rack: null) for sending metadata request
16:54:19.397 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:19.397 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:19.398 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-5] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:19.398 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Completed connection to node -1. Fetching API versions.
16:54:19.398 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Initiating API versions fetch from node -1.
16:54:19.398 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-5, correlationId=0, headerVersion=2) and timeout 60000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:19.403 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-5, correlationId=0, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:19.403 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:19.403 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:19.403 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-5, correlationId=1, headerVersion=2) and timeout 60000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:19.404 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-5] Sending transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) to node localhost:50137 (id: -1 rack: null) with correlation ID 2
16:54:19.404 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending INIT_PRODUCER_ID request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-5, correlationId=2, headerVersion=2) and timeout 60000 to node -1: InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:19.419 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-5, correlationId=1, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='error-handling-test-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:19.420 [kafka-producer-network-thread | producer-5] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Error while fetching metadata with correlation id 1 : {error-handling-test-topic=LEADER_NOT_AVAILABLE}
16:54:19.420 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-5] Requesting metadata update for topic error-handling-test-topic due to error LEADER_NOT_AVAILABLE
16:54:19.420 [kafka-producer-network-thread | producer-5] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-5] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:19.420 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-5] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:19.420 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Received INIT_PRODUCER_ID response from node -1 for request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-5, correlationId=2, headerVersion=2): InitProducerIdResponseData(throttleTimeMs=0, errorCode=0, producerId=4, producerEpoch=0)
16:54:19.420 [kafka-producer-network-thread | producer-5] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] ProducerId set to 4 with epoch 0
16:54:19.420 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] Transition from state INITIALIZING to READY
16:54:19.520 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:19.520 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:19.520 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:19.521 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-5] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:19.521 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Completed connection to node 1. Fetching API versions.
16:54:19.521 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Initiating API versions fetch from node 1.
16:54:19.521 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-5, correlationId=3, headerVersion=2) and timeout 60000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:19.524 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-5, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:19.524 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:19.524 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:19.524 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-5, correlationId=4, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:19.527 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-5, correlationId=4, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='error-handling-test-topic', topicId=utD4-aUCSsinuqWIEyp5Wg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:19.527 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-5] Setting the last seen epoch of partition error-handling-test-topic-0 to 0 since the last known epoch was undefined.
16:54:19.527 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-5] Setting the last seen epoch of partition error-handling-test-topic-1 to 0 since the last known epoch was undefined.
16:54:19.527 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-5] Setting the last seen epoch of partition error-handling-test-topic-2 to 0 since the last known epoch was undefined.
16:54:19.527 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-5] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=error-handling-test-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=error-handling-test-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=error-handling-test-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:19.533 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] ProducerId of partition error-handling-test-topic-1 set to 4 with epoch 0. Reinitialize sequence at beginning.
16:54:19.533 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-5] Assigned producerId 4 and producerEpoch 0 to batch with base sequence 0 being sent to partition error-handling-test-topic-1
16:54:19.534 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-5, correlationId=5, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[error-handling-test-topic-1=279]}
16:54:19.538 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-5, correlationId=5, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='error-handling-test-topic', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:19.538 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] ProducerId: 4; Set last ack'd sequence number for topic-partition error-handling-test-topic-1 to 0
16:54:19.544 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] ProducerId of partition error-handling-test-topic-2 set to 4 with epoch 0. Reinitialize sequence at beginning.
16:54:19.544 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-5] Assigned producerId 4 and producerEpoch 0 to batch with base sequence 0 being sent to partition error-handling-test-topic-2
16:54:19.545 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-5, correlationId=6, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[error-handling-test-topic-2=115]}
16:54:19.550 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-5] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-5, correlationId=6, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='error-handling-test-topic', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:19.550 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-5] ProducerId: 4; Set last ack'd sequence number for topic-partition error-handling-test-topic-2 to 0
16:54:19.550 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16:54:19.550 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-5] Beginning shutdown of Kafka producer I/O thread, sending remaining records.
16:54:19.552 [kafka-producer-network-thread | producer-5] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-5] Shutdown of Kafka producer I/O thread has completed.
16:54:19.552 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:19.552 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:19.552 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:19.552 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.producer for producer-5 unregistered
16:54:19.552 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-5] Kafka producer has been closed
16:54:19.552 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending FindCoordinator request to broker localhost:50137 (id: -1 rack: null)
16:54:19.552 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:19.552 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:19.553 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:19.554 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Completed connection to node -1. Fetching API versions.
16:54:19.554 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initiating API versions fetch from node -1.
16:54:19.554 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=1, headerVersion=2) and timeout 30000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:19.556 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=1, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:19.556 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:19.556 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='snowflake-cdc-errors')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:19.556 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=2, headerVersion=2) and timeout 30000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='snowflake-cdc-errors')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:19.557 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=0, headerVersion=2) and timeout 30000 to node -1: FindCoordinatorRequestData(key='', keyType=0, coordinatorKeys=[test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3])
16:54:19.570 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=2, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='error-handling-test-topic', topicId=utD4-aUCSsinuqWIEyp5Wg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648), MetadataResponseTopic(errorCode=5, name='snowflake-cdc-errors', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:19.570 [main] WARN org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Error while fetching metadata with correlation id 2 : {snowflake-cdc-errors=LEADER_NOT_AVAILABLE}
16:54:19.570 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Setting the last seen epoch of partition error-handling-test-topic-0 to 0 since the last known epoch was undefined.
16:54:19.570 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Setting the last seen epoch of partition error-handling-test-topic-1 to 0 since the last known epoch was undefined.
16:54:19.570 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Setting the last seen epoch of partition error-handling-test-topic-2 to 0 since the last known epoch was undefined.
16:54:19.570 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Requesting metadata update for topic snowflake-cdc-errors due to error LEADER_NOT_AVAILABLE
16:54:19.570 [main] INFO org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:19.570 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=error-handling-test-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=error-handling-test-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=error-handling-test-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:19.570 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received FIND_COORDINATOR response from node -1 for request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=0, headerVersion=2): FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')])
16:54:19.571 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received FindCoordinator response ClientResponse(receivedTimeMs=1759524859570, latencyMs=18, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=0, headerVersion=2), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')]))
16:54:19.571 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Discovered group coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:19.571 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:19.571 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initiating connection to node localhost:50137 (id: 2147483646 rack: null) using address localhost/127.0.0.1
16:54:19.571 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Executing onJoinPrepare with generation -1 and memberId 
16:54:19.571 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Marking assigned partitions pending for revocation: []
16:54:19.571 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending asynchronous auto-commit of offsets {}
16:54:19.571 [kafka-coordinator-heartbeat-thread | test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Heartbeat thread started
16:54:19.572 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 2147483646
16:54:19.572 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Completed connection to node 2147483646. Fetching API versions.
16:54:19.572 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initiating API versions fetch from node 2147483646.
16:54:19.572 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=3, headerVersion=2) and timeout 30000 to node 2147483646: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:19.573 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] (Re-)joining group
16:54:19.573 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Joining group with current subscription: [error-handling-test-topic, snowflake-cdc-errors]
16:54:19.573 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:19.575 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received API_VERSIONS response from node 2147483646 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:19.575 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Node 2147483646 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:19.575 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=4, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')
16:54:19.576 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Completed asynchronous auto-commit of offsets {}
16:54:19.579 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=4, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=79, generationId=-1, protocolType=null, protocolName=null, leader='', skipAssignment=false, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', members=[])
16:54:19.579 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] JoinGroup failed due to non-fatal error: MEMBER_ID_REQUIRED. Will set the member id as consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723 and then rejoin. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
16:54:19.579 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Request joining group due to: need to re-join with the given member-id: consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723
16:54:19.579 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
16:54:19.579 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] (Re-)joining group
16:54:19.579 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Joining group with current subscription: [error-handling-test-topic, snowflake-cdc-errors]
16:54:19.583 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:19.584 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=5, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')
16:54:19.591 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=5, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', skipAssignment=false, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:19.591 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', skipAssignment=false, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:19.591 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Enabling heartbeat thread
16:54:19.591 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', protocol='range'}
16:54:19.670 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:19.671 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:19.671 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:19.672 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:19.672 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Completed connection to node 1. Fetching API versions.
16:54:19.672 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Initiating API versions fetch from node 1.
16:54:19.672 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=6, headerVersion=2) and timeout 30000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:19.674 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=6, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:19.675 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:19.675 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='snowflake-cdc-errors')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:19.675 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=7, headerVersion=2) and timeout 30000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='error-handling-test-topic'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='snowflake-cdc-errors')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:19.677 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=7, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='error-handling-test-topic', topicId=utD4-aUCSsinuqWIEyp5Wg, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648), MetadataResponseTopic(errorCode=0, name='snowflake-cdc-errors', topicId=NyHBccvCS66Aet7T_OKWWw, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:19.677 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Updating last seen epoch for partition error-handling-test-topic-0 from 0 to epoch 0 from new metadata
16:54:19.677 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Updating last seen epoch for partition error-handling-test-topic-2 from 0 to epoch 0 from new metadata
16:54:19.677 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Updating last seen epoch for partition error-handling-test-topic-1 from 0 to epoch 0 from new metadata
16:54:19.677 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Setting the last seen epoch of partition snowflake-cdc-errors-0 to 0 since the last known epoch was undefined.
16:54:19.677 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Setting the last seen epoch of partition snowflake-cdc-errors-1 to 0 since the last known epoch was undefined.
16:54:19.677 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Setting the last seen epoch of partition snowflake-cdc-errors-2 to 0 since the last known epoch was undefined.
16:54:19.678 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=snowflake-cdc-errors-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=error-handling-test-topic-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=snowflake-cdc-errors-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=error-handling-test-topic-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=snowflake-cdc-errors-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=error-handling-test-topic-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:19.678 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Performing assignment using strategy range with subscriptions {consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723=Subscription(topics=[error-handling-test-topic, snowflake-cdc-errors], ownedPartitions=[], groupInstanceId=null, generationId=-1, rackId=null)}
16:54:19.678 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Finished assignment for group at generation 1: {consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723=Assignment(partitions=[error-handling-test-topic-0, error-handling-test-topic-1, error-handling-test-topic-2, snowflake-cdc-errors-0, snowflake-cdc-errors-1, snowflake-cdc-errors-2])}
16:54:19.678 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending leader SyncGroup to coordinator localhost:50137 (id: 2147483646 rack: null): SyncGroupRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', generationId=1, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', assignment=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:19.678 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending SYNC_GROUP request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=8, headerVersion=2) and timeout 30000 to node 2147483646: SyncGroupRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', generationId=1, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', assignment=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:19.681 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received SYNC_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=8, headerVersion=2): SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:19.681 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received successful SyncGroup response: SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 2, 0, 25, 101, 114, 114, 111, 114, 45, 104, 97, 110, 100, 108, 105, 110, 103, 45, 116, 101, 115, 116, 45, 116, 111, 112, 105, 99, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 20, 115, 110, 111, 119, 102, 108, 97, 107, 101, 45, 99, 100, 99, 45, 101, 114, 114, 111, 114, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:19.681 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', protocol='range'}
16:54:19.682 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Executing onJoinComplete with generation 1 and memberId consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723
16:54:19.682 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Notifying assignor about the new Assignment(partitions=[error-handling-test-topic-0, error-handling-test-topic-1, error-handling-test-topic-2, snowflake-cdc-errors-0, snowflake-cdc-errors-1, snowflake-cdc-errors-2])
16:54:19.682 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Adding newly assigned partitions: error-handling-test-topic-0, error-handling-test-topic-1, error-handling-test-topic-2, snowflake-cdc-errors-0, snowflake-cdc-errors-1, snowflake-cdc-errors-2
16:54:19.682 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Fetching committed offsets for partitions: [error-handling-test-topic-0, snowflake-cdc-errors-0, snowflake-cdc-errors-2, error-handling-test-topic-2, snowflake-cdc-errors-1, error-handling-test-topic-1]
16:54:19.682 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending OFFSET_FETCH request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=9, headerVersion=2) and timeout 30000 to node 2147483646: OffsetFetchRequestData(groupId='', topics=[], groups=[OffsetFetchRequestGroup(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', topics=[OffsetFetchRequestTopics(name='error-handling-test-topic', partitionIndexes=[0, 2, 1]), OffsetFetchRequestTopics(name='snowflake-cdc-errors', partitionIndexes=[0, 2, 1])])], requireStable=true)
16:54:19.683 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received OFFSET_FETCH response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=9, headerVersion=2): OffsetFetchResponseData(throttleTimeMs=0, topics=[], errorCode=0, groups=[OffsetFetchResponseGroup(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', topics=[OffsetFetchResponseTopics(name='error-handling-test-topic', partitions=[OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)]), OffsetFetchResponseTopics(name='snowflake-cdc-errors', partitions=[OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)])], errorCode=0)])
16:54:19.683 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Found no committed offset for partition error-handling-test-topic-0
16:54:19.683 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Found no committed offset for partition snowflake-cdc-errors-0
16:54:19.683 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Found no committed offset for partition error-handling-test-topic-2
16:54:19.683 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Found no committed offset for partition snowflake-cdc-errors-2
16:54:19.684 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Found no committed offset for partition error-handling-test-topic-1
16:54:19.684 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Found no committed offset for partition snowflake-cdc-errors-1
16:54:19.684 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcher -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending ListOffsetRequest ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='snowflake-cdc-errors', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)]), ListOffsetsTopic(name='error-handling-test-topic', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])]) to broker localhost:50137 (id: 1 rack: null)
16:54:19.685 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending LIST_OFFSETS request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=10, headerVersion=2) and timeout 30000 to node 1: ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='snowflake-cdc-errors', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)]), ListOffsetsTopic(name='error-handling-test-topic', partitions=[ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])])
16:54:19.686 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received LIST_OFFSETS response from node 1 for request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=10, headerVersion=2): ListOffsetsResponseData(throttleTimeMs=0, topics=[ListOffsetsTopicResponse(name='snowflake-cdc-errors', partitions=[ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)]), ListOffsetsTopicResponse(name='error-handling-test-topic', partitions=[ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)])])
16:54:19.686 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Handling ListOffsetResponse response for snowflake-cdc-errors-0. Fetched offset 0, timestamp -1
16:54:19.686 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Handling ListOffsetResponse response for snowflake-cdc-errors-2. Fetched offset 0, timestamp -1
16:54:19.686 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Handling ListOffsetResponse response for snowflake-cdc-errors-1. Fetched offset 0, timestamp -1
16:54:19.686 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Handling ListOffsetResponse response for error-handling-test-topic-0. Fetched offset 0, timestamp -1
16:54:19.686 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Handling ListOffsetResponse response for error-handling-test-topic-2. Fetched offset 0, timestamp -1
16:54:19.686 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Handling ListOffsetResponse response for error-handling-test-topic-1. Fetched offset 0, timestamp -1
16:54:19.686 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Not replacing existing epoch 0 with new epoch 0 for partition error-handling-test-topic-0
16:54:19.686 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Resetting offset for partition error-handling-test-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:19.686 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Not replacing existing epoch 0 with new epoch 0 for partition snowflake-cdc-errors-0
16:54:19.686 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Resetting offset for partition snowflake-cdc-errors-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:19.686 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Not replacing existing epoch 0 with new epoch 0 for partition snowflake-cdc-errors-2
16:54:19.686 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Resetting offset for partition snowflake-cdc-errors-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:19.686 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Not replacing existing epoch 0 with new epoch 0 for partition error-handling-test-topic-2
16:54:19.686 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Resetting offset for partition error-handling-test-topic-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:19.686 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Not replacing existing epoch 0 with new epoch 0 for partition snowflake-cdc-errors-1
16:54:19.686 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Resetting offset for partition snowflake-cdc-errors-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:19.686 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Not replacing existing epoch 0 with new epoch 0 for partition error-handling-test-topic-1
16:54:19.686 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Resetting offset for partition error-handling-test-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition snowflake-cdc-errors-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition snowflake-cdc-errors-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition snowflake-cdc-errors-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition error-handling-test-topic-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition error-handling-test-topic-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition error-handling-test-topic-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Built full fetch (sessionId=INVALID, epoch=INITIAL) for node 1 with 6 partition(s).
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(snowflake-cdc-errors-0, snowflake-cdc-errors-2, snowflake-cdc-errors-1, error-handling-test-topic-2, error-handling-test-topic-1, error-handling-test-topic-0), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:19.687 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=11, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=0, sessionEpoch=0, topics=[FetchTopic(topic='snowflake-cdc-errors', topicId=NyHBccvCS66Aet7T_OKWWw, partitions=[FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)]), FetchTopic(topic='error-handling-test-topic', topicId=utD4-aUCSsinuqWIEyp5Wg, partitions=[FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:19.691 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=11, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=1982068439, responses=[FetchableTopicResponse(topic='', topicId=NyHBccvCS66Aet7T_OKWWw, partitions=[PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=601])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=564])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=527]))]), FetchableTopicResponse(topic='', topicId=utD4-aUCSsinuqWIEyp5Wg, partitions=[PartitionData(partitionIndex=2, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=115, buffer=java.nio.HeapByteBuffer[pos=0 lim=115 cap=472])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=279, buffer=java.nio.HeapByteBuffer[pos=0 lim=279 cap=319])), PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=3]))])])
16:54:19.691 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Node 1 sent a full fetch response that created a new incremental fetch session 1982068439 with 6 response partition(s)
16:54:19.691 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Fetch READ_UNCOMMITTED at offset 0 for partition snowflake-cdc-errors-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=601]))
16:54:19.691 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Fetch READ_UNCOMMITTED at offset 0 for partition snowflake-cdc-errors-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=564]))
16:54:19.691 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Fetch READ_UNCOMMITTED at offset 0 for partition snowflake-cdc-errors-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=527]))
16:54:19.691 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Fetch READ_UNCOMMITTED at offset 0 for partition error-handling-test-topic-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=115, buffer=java.nio.HeapByteBuffer[pos=0 lim=115 cap=472]))
16:54:19.691 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Fetch READ_UNCOMMITTED at offset 0 for partition error-handling-test-topic-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=1, lastStableOffset=1, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=279, buffer=java.nio.HeapByteBuffer[pos=0 lim=279 cap=319]))
16:54:19.691 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Fetch READ_UNCOMMITTED at offset 0 for partition error-handling-test-topic-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=0, buffer=java.nio.HeapByteBuffer[pos=0 lim=0 cap=3]))
16:54:19.691 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition snowflake-cdc-errors-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition snowflake-cdc-errors-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition snowflake-cdc-errors-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition error-handling-test-topic-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition error-handling-test-topic-2 at position FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Added READ_UNCOMMITTED fetch request for partition error-handling-test-topic-1 at position FetchPosition{offset=1, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Built incremental fetch (sessionId=1982068439, epoch=1) for node 1. Added 0 partition(s), altered 2 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 6 partition(s)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(error-handling-test-topic-2, error-handling-test-topic-1), toForget=(), toReplace=(), implied=(snowflake-cdc-errors-0, snowflake-cdc-errors-2, snowflake-cdc-errors-1, error-handling-test-topic-0), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:19.692 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:19.693 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=12, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=1982068439, sessionEpoch=1, topics=[FetchTopic(topic='error-handling-test-topic', topicId=utD4-aUCSsinuqWIEyp5Wg, partitions=[FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=1, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=1, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
Received valid record from topic: error-handling-test-topic
16:54:19.693 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending synchronous auto-commit of offsets {error-handling-test-topic-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}, snowflake-cdc-errors-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}, snowflake-cdc-errors-2=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}, error-handling-test-topic-2=OffsetAndMetadata{offset=1, leaderEpoch=0, metadata=''}, snowflake-cdc-errors-1=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}, error-handling-test-topic-1=OffsetAndMetadata{offset=1, leaderEpoch=0, metadata=''}}
16:54:19.693 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending OFFSET_COMMIT request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=13, headerVersion=2) and timeout 30000 to node 2147483646: OffsetCommitRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', generationIdOrMemberEpoch=1, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, retentionTimeMs=-1, topics=[OffsetCommitRequestTopic(name='error-handling-test-topic', partitions=[OffsetCommitRequestPartition(partitionIndex=0, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=2, committedOffset=1, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=1, committedOffset=1, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata='')]), OffsetCommitRequestTopic(name='snowflake-cdc-errors', partitions=[OffsetCommitRequestPartition(partitionIndex=0, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=2, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=1, committedOffset=0, committedLeaderEpoch=-1, commitTimestamp=-1, committedMetadata='')])])
16:54:19.697 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received OFFSET_COMMIT response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=13, headerVersion=2): OffsetCommitResponseData(throttleTimeMs=0, topics=[OffsetCommitResponseTopic(name='error-handling-test-topic', partitions=[OffsetCommitResponsePartition(partitionIndex=2, errorCode=0), OffsetCommitResponsePartition(partitionIndex=1, errorCode=0), OffsetCommitResponsePartition(partitionIndex=0, errorCode=0)]), OffsetCommitResponseTopic(name='snowflake-cdc-errors', partitions=[OffsetCommitResponsePartition(partitionIndex=2, errorCode=0), OffsetCommitResponsePartition(partitionIndex=0, errorCode=0), OffsetCommitResponsePartition(partitionIndex=1, errorCode=0)])])
16:54:19.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Committed offset 1 for partition error-handling-test-topic-2
16:54:19.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Committed offset 1 for partition error-handling-test-topic-1
16:54:19.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Committed offset 0 for partition error-handling-test-topic-0
16:54:19.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Committed offset 0 for partition snowflake-cdc-errors-2
16:54:19.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Committed offset 0 for partition snowflake-cdc-errors-0
16:54:19.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Committed offset 0 for partition snowflake-cdc-errors-1
16:54:19.697 [kafka-coordinator-heartbeat-thread | test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Heartbeat thread has closed
16:54:19.697 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Executing onLeavePrepare with generation Generation{generationId=1, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', protocol='range'}
16:54:19.697 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Revoke previously assigned partitions error-handling-test-topic-0, error-handling-test-topic-1, error-handling-test-topic-2, snowflake-cdc-errors-0, snowflake-cdc-errors-1, snowflake-cdc-errors-2
16:54:19.697 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Member consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723 sending LeaveGroup request to coordinator localhost:50137 (id: 2147483646 rack: null) due to the consumer is being closed
16:54:19.697 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending LEAVE_GROUP request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=14, headerVersion=2) and timeout 30000 to node 2147483646: LeaveGroupRequestData(groupId='test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3', memberId='', members=[MemberIdentity(memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, reason='the consumer is being closed')])
16:54:19.697 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:19.697 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Request joining group due to: consumer pro-actively leaving the group
16:54:19.702 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received LEAVE_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=14, headerVersion=2): LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, errorCode=0)])
16:54:19.703 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] LeaveGroup response with Generation{generationId=1, memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', protocol='range'} returned successfully: ClientResponse(receivedTimeMs=1759524859702, latencyMs=5, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=14, headerVersion=2), responseBody=LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8-49380626-68db-4a74-98e9-23a488d5c723', groupInstanceId=null, errorCode=0)]))
16:54:19.703 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Set the metadata for next fetch request to close the existing session ID=1982068439
16:54:19.703 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Built full fetch (sessionId=1982068439, epoch=FINAL) for node 1 with 0 partition(s).
16:54:19.703 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:19.703 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:19.703 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=15, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=1982068439, sessionEpoch=-1, topics=[], forgottenTopicsData=[], rackId='')
16:54:20.200 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=12, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=1982068439, responses=[])
16:54:20.200 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Node 1 sent a full fetch response that created a new incremental fetch session 1982068439 with 0 response partition(s)
16:54:20.200 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:20.202 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, correlationId=15, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=0, responses=[])
16:54:20.202 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Successfully sent a close message for fetch session: 1982068439 to node: localhost:50137 (id: 1 rack: null)
16:54:20.203 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:20.203 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:20.203 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:20.205 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8 unregistered
16:54:20.205 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3-8, groupId=test-consumer-group-815df118-6d04-406f-b731-c0a8e4347dc3] Kafka consumer has been closed
]]></system-out>
  </testcase>
  <testcase name="testMultipleTopicsScenario" classname="com.snowflake.kafka.integration.SnowflakeKafkaIntegrationTest" time="1.211">
    <system-out><![CDATA[16:54:20.210 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

16:54:20.210 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Initializing the Kafka consumer
16:54:20.212 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:20.212 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:20.212 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524860212
16:54:20.212 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Kafka consumer initialized
16:54:20.213 [main] INFO org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Subscribed to topic(s): product-changes, order-changes, customer-changes
16:54:20.214 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 131072
	bootstrap.servers = [PLAINTEXT://localhost:50137]
	buffer.memory = 268435456
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = zstd
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 300000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 5
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 60000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

16:54:20.215 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-6] Instantiated an idempotent producer.
16:54:20.217 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.0
16:54:20.217 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 60e845626d8a465a
16:54:20.217 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1759524860217
16:54:20.217 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-6] Starting Kafka producer I/O thread.
16:54:20.217 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] Transition from state UNINITIALIZED to INITIALIZING
16:54:20.218 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] Enqueuing transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:20.218 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-6] Kafka producer started
16:54:20.218 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Initialize connection to node localhost:50137 (id: -1 rack: null) for sending metadata request
16:54:20.218 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:20.218 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:20.219 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-6] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:20.219 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Completed connection to node -1. Fetching API versions.
16:54:20.219 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Initiating API versions fetch from node -1.
16:54:20.219 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-6, correlationId=0, headerVersion=2) and timeout 60000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:20.223 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-6, correlationId=0, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:20.224 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:20.224 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:20.224 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=1, headerVersion=2) and timeout 60000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:20.224 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-6] Sending transactional request InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) to node localhost:50137 (id: -1 rack: null) with correlation ID 2
16:54:20.224 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending INIT_PRODUCER_ID request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-6, correlationId=2, headerVersion=2) and timeout 60000 to node -1: InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1)
16:54:20.240 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=1, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='product-changes', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:20.240 [kafka-producer-network-thread | producer-6] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Error while fetching metadata with correlation id 1 : {product-changes=LEADER_NOT_AVAILABLE}
16:54:20.240 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Requesting metadata update for topic product-changes due to error LEADER_NOT_AVAILABLE
16:54:20.240 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:20.240 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[], controller=localhost:50137 (id: 1 rack: null)}
16:54:20.240 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received INIT_PRODUCER_ID response from node -1 for request with header RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=producer-6, correlationId=2, headerVersion=2): InitProducerIdResponseData(throttleTimeMs=0, errorCode=0, producerId=5, producerEpoch=0)
16:54:20.240 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId set to 5 with epoch 0
16:54:20.240 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] Transition from state INITIALIZING to READY
16:54:20.341 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Initialize connection to node localhost:50137 (id: 1 rack: null) for sending metadata request
16:54:20.341 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:20.341 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:20.343 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.common.network.Selector -- [Producer clientId=producer-6] Created socket with SO_RCVBUF = 326640, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:20.343 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Completed connection to node 1. Fetching API versions.
16:54:20.343 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Initiating API versions fetch from node 1.
16:54:20.343 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-6, correlationId=3, headerVersion=2) and timeout 60000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:20.345 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-6, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:20.345 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:20.346 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:20.346 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=4, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:20.347 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=4, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='product-changes', topicId=eBRtfLSOSZWfyQRckJSePQ, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:20.347 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition product-changes-0 to 0 since the last known epoch was undefined.
16:54:20.347 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition product-changes-1 to 0 since the last known epoch was undefined.
16:54:20.347 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition product-changes-2 to 0 since the last known epoch was undefined.
16:54:20.347 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=product-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:20.352 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition product-changes-0 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.352 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition product-changes-0
16:54:20.353 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=5, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-0=278]}
16:54:20.356 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=5, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.356 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-0 to 0
16:54:20.364 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition product-changes-2 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.364 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition product-changes-2
16:54:20.365 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=6, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-2=280]}
16:54:20.367 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=6, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.367 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-2 to 0
16:54:20.372 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition product-changes-0
16:54:20.373 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=7, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-0=279]}
16:54:20.374 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=7, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.374 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-0 to 1
16:54:20.379 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 2 being sent to partition product-changes-0
16:54:20.380 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=8, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-0=280]}
16:54:20.381 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=8, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=2, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.381 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-0 to 2
16:54:20.387 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition product-changes-2
16:54:20.388 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=9, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-2=280]}
16:54:20.389 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=9, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.389 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-2 to 1
16:54:20.395 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition product-changes-1 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.395 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition product-changes-1
16:54:20.396 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=10, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-1=280]}
16:54:20.398 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=10, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.398 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-1 to 0
16:54:20.405 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition product-changes-1
16:54:20.406 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=11, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-1=280]}
16:54:20.408 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=11, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.408 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-1 to 1
16:54:20.416 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 2 being sent to partition product-changes-1
16:54:20.417 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=12, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-1=280]}
16:54:20.419 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=12, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=2, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.419 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-1 to 2
16:54:20.426 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 2 being sent to partition product-changes-2
16:54:20.426 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=13, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-2=280]}
16:54:20.428 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=13, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=2, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.428 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-2 to 2
16:54:20.433 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 3 being sent to partition product-changes-2
16:54:20.434 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=14, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[product-changes-2=280]}
16:54:20.437 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=14, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='product-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=3, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.437 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition product-changes-2 to 3
16:54:20.438 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:20.438 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=15, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:20.447 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=15, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='order-changes', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:20.447 [kafka-producer-network-thread | producer-6] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Error while fetching metadata with correlation id 15 : {order-changes=LEADER_NOT_AVAILABLE}
16:54:20.447 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Requesting metadata update for topic order-changes due to error LEADER_NOT_AVAILABLE
16:54:20.449 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Updated cluster metadata updateVersion 4 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=product-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:20.547 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:20.547 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=16, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:20.550 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=16, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='product-changes', topicId=eBRtfLSOSZWfyQRckJSePQ, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648), MetadataResponseTopic(errorCode=0, name='order-changes', topicId=cI5xjwgmQZW30L0q2Q3M5g, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:20.550 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition product-changes-0 to 0 since the associated topicId changed from null to eBRtfLSOSZWfyQRckJSePQ
16:54:20.550 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition product-changes-2 to 0 since the associated topicId changed from null to eBRtfLSOSZWfyQRckJSePQ
16:54:20.550 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition product-changes-1 to 0 since the associated topicId changed from null to eBRtfLSOSZWfyQRckJSePQ
16:54:20.550 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition order-changes-0 to 0 since the last known epoch was undefined.
16:54:20.550 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition order-changes-1 to 0 since the last known epoch was undefined.
16:54:20.550 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition order-changes-2 to 0 since the last known epoch was undefined.
16:54:20.550 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Updated cluster metadata updateVersion 5 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=product-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:20.555 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition order-changes-1 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.555 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition order-changes-1
16:54:20.556 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=17, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-1=276]}
16:54:20.558 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=17, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.558 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-1 to 0
16:54:20.564 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition order-changes-0 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.564 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition order-changes-0
16:54:20.565 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=18, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-0=276]}
16:54:20.566 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=18, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.566 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-0 to 0
16:54:20.571 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition order-changes-0
16:54:20.571 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=19, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-0=275]}
16:54:20.572 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=19, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.572 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-0 to 1
16:54:20.578 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 2 being sent to partition order-changes-0
16:54:20.579 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=20, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-0=276]}
16:54:20.582 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=20, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=2, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.582 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-0 to 2
16:54:20.588 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition order-changes-2 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.588 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition order-changes-2
16:54:20.589 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=21, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-2=275]}
16:54:20.591 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=21, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.591 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-2 to 0
16:54:20.597 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition order-changes-1
16:54:20.597 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=22, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-1=275]}
16:54:20.599 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=22, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.599 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-1 to 1
16:54:20.605 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition order-changes-2
16:54:20.605 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=23, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-2=275]}
16:54:20.606 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=23, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.606 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-2 to 1
16:54:20.612 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 3 being sent to partition order-changes-0
16:54:20.612 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=24, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-0=275]}
16:54:20.613 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=24, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=3, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.613 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-0 to 3
16:54:20.619 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 2 being sent to partition order-changes-2
16:54:20.619 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=25, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-2=276]}
16:54:20.621 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=25, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=2, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.621 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-2 to 2
16:54:20.626 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 3 being sent to partition order-changes-2
16:54:20.627 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=26, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[order-changes-2=275]}
16:54:20.628 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=26, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='order-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=3, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.628 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition order-changes-2 to 3
16:54:20.628 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='customer-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:20.628 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=27, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='customer-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:20.635 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=27, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=5, name='customer-changes', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:20.635 [kafka-producer-network-thread | producer-6] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Error while fetching metadata with correlation id 27 : {customer-changes=LEADER_NOT_AVAILABLE}
16:54:20.635 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Requesting metadata update for topic customer-changes due to error LEADER_NOT_AVAILABLE
16:54:20.635 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Updated cluster metadata updateVersion 6 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=product-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:20.736 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='customer-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: 1 rack: null)
16:54:20.736 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=28, headerVersion=2) and timeout 60000 to node 1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='customer-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:20.738 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-6, correlationId=28, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='product-changes', topicId=eBRtfLSOSZWfyQRckJSePQ, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648), MetadataResponseTopic(errorCode=0, name='order-changes', topicId=cI5xjwgmQZW30L0q2Q3M5g, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648), MetadataResponseTopic(errorCode=0, name='customer-changes', topicId=DfV0uLbzTIWsDJUZN4L0zA, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:20.738 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition product-changes-0 to 0 since the associated topicId changed from null to eBRtfLSOSZWfyQRckJSePQ
16:54:20.739 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition product-changes-2 to 0 since the associated topicId changed from null to eBRtfLSOSZWfyQRckJSePQ
16:54:20.739 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition product-changes-1 to 0 since the associated topicId changed from null to eBRtfLSOSZWfyQRckJSePQ
16:54:20.739 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition order-changes-0 to 0 since the associated topicId changed from null to cI5xjwgmQZW30L0q2Q3M5g
16:54:20.739 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition order-changes-2 to 0 since the associated topicId changed from null to cI5xjwgmQZW30L0q2Q3M5g
16:54:20.739 [kafka-producer-network-thread | producer-6] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Resetting the last seen epoch of partition order-changes-1 to 0 since the associated topicId changed from null to cI5xjwgmQZW30L0q2Q3M5g
16:54:20.739 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition customer-changes-0 to 0 since the last known epoch was undefined.
16:54:20.739 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition customer-changes-1 to 0 since the last known epoch was undefined.
16:54:20.739 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Setting the last seen epoch of partition customer-changes-2 to 0 since the last known epoch was undefined.
16:54:20.739 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.Metadata -- [Producer clientId=producer-6] Updated cluster metadata updateVersion 7 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=product-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=customer-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=customer-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=customer-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:20.745 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition customer-changes-0 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.745 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition customer-changes-0
16:54:20.746 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=29, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-0=279]}
16:54:20.749 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=29, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.749 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-0 to 0
16:54:20.756 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition customer-changes-1 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.756 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition customer-changes-1
16:54:20.756 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=30, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-1=279]}
16:54:20.759 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=30, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.759 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-1 to 0
16:54:20.765 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition customer-changes-1
16:54:20.765 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=31, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-1=279]}
16:54:20.767 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=31, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.767 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-1 to 1
16:54:20.773 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId of partition customer-changes-2 set to 5 with epoch 0. Reinitialize sequence at beginning.
16:54:20.773 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 0 being sent to partition customer-changes-2
16:54:20.773 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=32, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-2=279]}
16:54:20.775 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=32, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=0, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.775 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-2 to 0
16:54:20.781 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 2 being sent to partition customer-changes-1
16:54:20.781 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=33, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-1=279]}
16:54:20.784 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=33, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=2, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.784 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-1 to 2
16:54:20.790 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition customer-changes-0
16:54:20.790 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=34, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-0=279]}
16:54:20.792 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=34, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.792 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-0 to 1
16:54:20.798 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 2 being sent to partition customer-changes-0
16:54:20.798 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=35, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-0=279]}
16:54:20.802 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=35, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=2, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.802 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-0 to 2
16:54:20.808 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 3 being sent to partition customer-changes-1
16:54:20.808 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=36, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-1=279]}
16:54:20.811 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=36, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=1, errorCode=0, baseOffset=3, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.811 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-1 to 3
16:54:20.817 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 3 being sent to partition customer-changes-0
16:54:20.817 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=37, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-0=279]}
16:54:20.819 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=37, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=0, errorCode=0, baseOffset=3, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.819 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-0 to 3
16:54:20.826 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.RecordAccumulator -- [Producer clientId=producer-6] Assigned producerId 5 and producerEpoch 0 to batch with base sequence 1 being sent to partition customer-changes-2
16:54:20.826 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=38, headerVersion=2) and timeout 60000 to node 1: {acks=-1,timeout=60000,partitionSizes=[customer-changes-2=279]}
16:54:20.828 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-6] Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=9, clientId=producer-6, correlationId=38, headerVersion=2): ProduceResponseData(responses=[TopicProduceResponse(name='customer-changes', partitionResponses=[PartitionProduceResponse(index=2, errorCode=0, baseOffset=1, logAppendTimeMs=-1, logStartOffset=0, recordErrors=[], errorMessage=null)])], throttleTimeMs=0)
16:54:20.828 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-6] ProducerId: 5; Set last ack'd sequence number for topic-partition customer-changes-2 to 1
16:54:20.828 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
16:54:20.828 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-6] Beginning shutdown of Kafka producer I/O thread, sending remaining records.
16:54:20.829 [kafka-producer-network-thread | producer-6] DEBUG org.apache.kafka.clients.producer.internals.Sender -- [Producer clientId=producer-6] Shutdown of Kafka producer I/O thread has completed.
16:54:20.829 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:20.829 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:20.829 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:20.829 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.producer for producer-6 unregistered
16:54:20.829 [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-6] Kafka producer has been closed
16:54:20.832 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending FindCoordinator request to broker localhost:50137 (id: -1 rack: null)
16:54:20.833 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:20.833 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Initiating connection to node localhost:50137 (id: -1 rack: null) using address localhost/127.0.0.1
16:54:20.833 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node -1
16:54:20.833 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Completed connection to node -1. Fetching API versions.
16:54:20.833 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Initiating API versions fetch from node -1.
16:54:20.834 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=1, headerVersion=2) and timeout 30000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:20.836 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=1, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:20.836 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:20.836 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='customer-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:50137 (id: -1 rack: null)
16:54:20.837 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=2, headerVersion=2) and timeout 30000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='product-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='order-changes'), MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='customer-changes')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
16:54:20.837 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=0, headerVersion=2) and timeout 30000 to node -1: FindCoordinatorRequestData(key='', keyType=0, coordinatorKeys=[test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f])
16:54:20.839 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=2, headerVersion=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='localhost', port=50137, rack=null)], clusterId='68rYXGUVS86JmY125F2z-Q', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='product-changes', topicId=eBRtfLSOSZWfyQRckJSePQ, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648), MetadataResponseTopic(errorCode=0, name='order-changes', topicId=cI5xjwgmQZW30L0q2Q3M5g, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648), MetadataResponseTopic(errorCode=0, name='customer-changes', topicId=DfV0uLbzTIWsDJUZN4L0zA, isInternal=false, partitions=[MetadataResponsePartition(errorCode=0, partitionIndex=0, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=1, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[]), MetadataResponsePartition(errorCode=0, partitionIndex=2, leaderId=1, leaderEpoch=0, replicaNodes=[1], isrNodes=[1], offlineReplicas=[])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648)
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition product-changes-0 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition product-changes-2 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition product-changes-1 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition order-changes-0 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition order-changes-2 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition order-changes-1 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition customer-changes-0 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition customer-changes-1 to 0 since the last known epoch was undefined.
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Setting the last seen epoch of partition customer-changes-2 to 0 since the last known epoch was undefined.
16:54:20.839 [main] INFO org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Cluster ID: 68rYXGUVS86JmY125F2z-Q
16:54:20.839 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='68rYXGUVS86JmY125F2z-Q', nodes={1=localhost:50137 (id: 1 rack: null)}, partitions=[PartitionMetadata(error=NONE, partition=product-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=product-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=order-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=customer-changes-2, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=customer-changes-0, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=), PartitionMetadata(error=NONE, partition=customer-changes-1, leader=Optional[1], leaderEpoch=Optional[0], replicas=1, isr=1, offlineReplicas=)], controller=localhost:50137 (id: 1 rack: null)}
16:54:20.839 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received FIND_COORDINATOR response from node -1 for request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=0, headerVersion=2): FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')])
16:54:20.839 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received FindCoordinator response ClientResponse(receivedTimeMs=1759524860839, latencyMs=7, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=0, headerVersion=2), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', nodeId=1, host='localhost', port=50137, errorCode=0, errorMessage='')]))
16:54:20.839 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Discovered group coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:20.839 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:20.839 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Initiating connection to node localhost:50137 (id: 2147483646 rack: null) using address localhost/127.0.0.1
16:54:20.840 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Executing onJoinPrepare with generation -1 and memberId 
16:54:20.840 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Marking assigned partitions pending for revocation: []
16:54:20.840 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending asynchronous auto-commit of offsets {}
16:54:20.840 [kafka-coordinator-heartbeat-thread | test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Heartbeat thread started
16:54:20.840 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 2147483646
16:54:20.840 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Completed connection to node 2147483646. Fetching API versions.
16:54:20.840 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Initiating API versions fetch from node 2147483646.
16:54:20.840 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=3, headerVersion=2) and timeout 30000 to node 2147483646: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:20.840 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] (Re-)joining group
16:54:20.840 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Joining group with current subscription: [product-changes, order-changes, customer-changes]
16:54:20.841 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:20.842 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received API_VERSIONS response from node 2147483646 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=3, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:20.842 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Node 2147483646 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:20.842 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=4, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='')
16:54:20.842 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Completed asynchronous auto-commit of offsets {}
16:54:20.845 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=4, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=79, generationId=-1, protocolType=null, protocolName=null, leader='', skipAssignment=false, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', members=[])
16:54:20.845 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] JoinGroup failed due to non-fatal error: MEMBER_ID_REQUIRED. Will set the member id as consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5 and then rejoin. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
16:54:20.845 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Request joining group due to: need to re-join with the given member-id: consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5
16:54:20.845 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
16:54:20.845 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] (Re-)joining group
16:54:20.845 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Joining group with current subscription: [product-changes, order-changes, customer-changes]
16:54:20.845 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending JoinGroup (JoinGroupRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')) to coordinator localhost:50137 (id: 2147483646 rack: null)
16:54:20.845 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending JOIN_GROUP request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=5, headerVersion=2) and timeout 305000 to node 2147483646: JoinGroupRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, protocolType='consumer', protocols=[JoinGroupRequestProtocol(name='range', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]), JoinGroupRequestProtocol(name='cooperative-sticky', metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 4, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])], reason='rebalance failed due to MemberIdRequiredException')
16:54:20.849 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received JOIN_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=JOIN_GROUP, apiVersion=9, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=5, headerVersion=2): JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', skipAssignment=false, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:20.849 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='consumer', protocolName='range', leader='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', skipAssignment=false, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', members=[JoinGroupResponseMember(memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, metadata=[0, 3, 0, 0, 0, 3, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1])])
16:54:20.849 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Enabling heartbeat thread
16:54:20.849 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', protocol='range'}
16:54:20.849 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Performing assignment using strategy range with subscriptions {consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5=Subscription(topics=[customer-changes, order-changes, product-changes], ownedPartitions=[], groupInstanceId=null, generationId=-1, rackId=null)}
16:54:20.850 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Finished assignment for group at generation 1: {consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5=Assignment(partitions=[order-changes-0, order-changes-1, order-changes-2, product-changes-0, product-changes-1, product-changes-2, customer-changes-0, customer-changes-1, customer-changes-2])}
16:54:20.850 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending leader SyncGroup to coordinator localhost:50137 (id: 2147483646 rack: null): SyncGroupRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', generationId=1, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', assignment=[0, 3, 0, 0, 0, 3, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:20.850 [kafka-coordinator-heartbeat-thread | test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending SYNC_GROUP request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=6, headerVersion=2) and timeout 30000 to node 2147483646: SyncGroupRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', generationId=1, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, protocolType='consumer', protocolName='range', assignments=[SyncGroupRequestAssignment(memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', assignment=[0, 3, 0, 0, 0, 3, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])])
16:54:20.854 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received SYNC_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=SYNC_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=6, headerVersion=2): SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 3, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:20.855 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received successful SyncGroup response: SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='consumer', protocolName='range', assignment=[0, 3, 0, 0, 0, 3, 0, 13, 111, 114, 100, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 15, 112, 114, 111, 100, 117, 99, 116, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 16, 99, 117, 115, 116, 111, 109, 101, 114, 45, 99, 104, 97, 110, 103, 101, 115, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, -1, -1, -1, -1])
16:54:20.855 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', protocol='range'}
16:54:20.855 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Executing onJoinComplete with generation 1 and memberId consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5
16:54:20.855 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Notifying assignor about the new Assignment(partitions=[order-changes-0, order-changes-1, order-changes-2, product-changes-0, product-changes-1, product-changes-2, customer-changes-0, customer-changes-1, customer-changes-2])
16:54:20.855 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Adding newly assigned partitions: customer-changes-0, customer-changes-1, customer-changes-2, order-changes-0, order-changes-1, order-changes-2, product-changes-0, product-changes-1, product-changes-2
16:54:20.855 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetching committed offsets for partitions: [order-changes-1, product-changes-1, order-changes-0, product-changes-2, product-changes-0, order-changes-2, customer-changes-2, customer-changes-0, customer-changes-1]
16:54:20.855 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending OFFSET_FETCH request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=7, headerVersion=2) and timeout 30000 to node 2147483646: OffsetFetchRequestData(groupId='', topics=[], groups=[OffsetFetchRequestGroup(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', topics=[OffsetFetchRequestTopics(name='order-changes', partitionIndexes=[1, 0, 2]), OffsetFetchRequestTopics(name='product-changes', partitionIndexes=[1, 2, 0]), OffsetFetchRequestTopics(name='customer-changes', partitionIndexes=[2, 0, 1])])], requireStable=true)
16:54:20.856 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received OFFSET_FETCH response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_FETCH, apiVersion=8, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=7, headerVersion=2): OffsetFetchResponseData(throttleTimeMs=0, topics=[], errorCode=0, groups=[OffsetFetchResponseGroup(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', topics=[OffsetFetchResponseTopics(name='product-changes', partitions=[OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)]), OffsetFetchResponseTopics(name='order-changes', partitions=[OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)]), OffsetFetchResponseTopics(name='customer-changes', partitions=[OffsetFetchResponsePartitions(partitionIndex=1, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=2, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartitions(partitionIndex=0, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0)])], errorCode=0)])
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition product-changes-1
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition order-changes-1
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition product-changes-2
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition order-changes-0
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition product-changes-0
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition order-changes-2
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition customer-changes-2
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition customer-changes-0
16:54:20.856 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Found no committed offset for partition customer-changes-1
16:54:20.857 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcher -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending ListOffsetRequest ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='product-changes', partitions=[ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)]), ListOffsetsTopic(name='order-changes', partitions=[ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)]), ListOffsetsTopic(name='customer-changes', partitions=[ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])]) to broker localhost:50137 (id: 1 rack: null)
16:54:20.857 [main] DEBUG org.apache.kafka.clients.ClientUtils -- Resolved host localhost as 127.0.0.1
16:54:20.857 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Initiating connection to node localhost:50137 (id: 1 rack: null) using address localhost/127.0.0.1
16:54:20.857 [main] DEBUG org.apache.kafka.common.network.Selector -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Created socket with SO_RCVBUF = 342972, SO_SNDBUF = 146988, SO_TIMEOUT = 0 to node 1
16:54:20.857 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Completed connection to node 1. Fetching API versions.
16:54:20.857 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Initiating API versions fetch from node 1.
16:54:20.857 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=9, headerVersion=2) and timeout 30000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.6.0')
16:54:20.859 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=9, headerVersion=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=7), ApiVersion(apiKey=5, minVersion=0, maxVersion=4), ApiVersion(apiKey=6, minVersion=0, maxVersion=8), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=3), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=2), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=58, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[], zkMigrationReady=false)
16:54:20.860 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Node 1 has finalized features epoch: 0, finalized features: [], supported features: [], ZK migration ready: false, API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 7 [usable: 7], StopReplica(5): 0 to 4 [usable: 4], UpdateMetadata(6): 0 to 8 [usable: 8], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 9], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 5], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 3], CreateAcls(30): 0 to 3 [usable: 3], DeleteAcls(31): 0 to 3 [usable: 3], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 4], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 3 [usable: 3], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 3 [usable: 3], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], DescribeQuorum(55): UNSUPPORTED, AlterPartition(56): 0 to 2 [usable: 2], UpdateFeatures(57): 0 to 1 [usable: 1], Envelope(58): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): UNSUPPORTED, DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0], ConsumerGroupHeartbeat(68): UNSUPPORTED).
16:54:20.860 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending LIST_OFFSETS request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=8, headerVersion=2) and timeout 30000 to node 1: ListOffsetsRequestData(replicaId=-1, isolationLevel=0, topics=[ListOffsetsTopic(name='product-changes', partitions=[ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)]), ListOffsetsTopic(name='order-changes', partitions=[ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)]), ListOffsetsTopic(name='customer-changes', partitions=[ListOffsetsPartition(partitionIndex=2, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=0, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1), ListOffsetsPartition(partitionIndex=1, currentLeaderEpoch=0, timestamp=-2, maxNumOffsets=1)])])
16:54:20.861 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received LIST_OFFSETS response from node 1 for request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=7, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=8, headerVersion=2): ListOffsetsResponseData(throttleTimeMs=0, topics=[ListOffsetsTopicResponse(name='product-changes', partitions=[ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)]), ListOffsetsTopicResponse(name='order-changes', partitions=[ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)]), ListOffsetsTopicResponse(name='customer-changes', partitions=[ListOffsetsPartitionResponse(partitionIndex=2, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=0, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0), ListOffsetsPartitionResponse(partitionIndex=1, errorCode=0, oldStyleOffsets=[], timestamp=-1, offset=0, leaderEpoch=0)])])
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for product-changes-1. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for product-changes-2. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for product-changes-0. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for order-changes-1. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for order-changes-0. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for order-changes-2. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for customer-changes-2. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for customer-changes-0. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.consumer.internals.OffsetFetcherUtils -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Handling ListOffsetResponse response for customer-changes-1. Fetched offset 0, timestamp -1
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition product-changes-1
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition product-changes-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition order-changes-1
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition order-changes-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition product-changes-2
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition product-changes-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition order-changes-0
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition order-changes-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition product-changes-0
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition product-changes-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition order-changes-2
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition order-changes-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition customer-changes-2
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition customer-changes-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition customer-changes-0
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition customer-changes-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.861 [main] DEBUG org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Not replacing existing epoch 0 with new epoch 0 for partition customer-changes-1
16:54:20.861 [main] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting offset for partition customer-changes-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}}.
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition order-changes-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition order-changes-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition order-changes-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition product-changes-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition product-changes-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition product-changes-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition customer-changes-2 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition customer-changes-0 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition customer-changes-1 at position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Built full fetch (sessionId=INVALID, epoch=INITIAL) for node 1 with 9 partition(s).
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(order-changes-1, order-changes-0, order-changes-2, product-changes-1, product-changes-2, product-changes-0, customer-changes-2, customer-changes-0, customer-changes-1), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:20.862 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=10, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=0, sessionEpoch=0, topics=[FetchTopic(topic='order-changes', topicId=cI5xjwgmQZW30L0q2Q3M5g, partitions=[FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)]), FetchTopic(topic='product-changes', topicId=eBRtfLSOSZWfyQRckJSePQ, partitions=[FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)]), FetchTopic(topic='customer-changes', topicId=DfV0uLbzTIWsDJUZN4L0zA, partitions=[FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=0, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
16:54:20.867 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=10, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=1391881732, responses=[FetchableTopicResponse(topic='', topicId=cI5xjwgmQZW30L0q2Q3M5g, partitions=[PartitionData(partitionIndex=1, errorCode=0, highWatermark=2, lastStableOffset=2, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=551, buffer=java.nio.HeapByteBuffer[pos=0 lim=551 cap=8684])), PartitionData(partitionIndex=0, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1102, buffer=java.nio.HeapByteBuffer[pos=0 lim=1102 cap=8095])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1101, buffer=java.nio.HeapByteBuffer[pos=0 lim=1101 cap=6955]))]), FetchableTopicResponse(topic='', topicId=eBRtfLSOSZWfyQRckJSePQ, partitions=[PartitionData(partitionIndex=1, errorCode=0, highWatermark=3, lastStableOffset=3, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=840, buffer=java.nio.HeapByteBuffer[pos=0 lim=840 cap=5798])), PartitionData(partitionIndex=2, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1120, buffer=java.nio.HeapByteBuffer[pos=0 lim=1120 cap=4920])), PartitionData(partitionIndex=0, errorCode=0, highWatermark=3, lastStableOffset=3, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=837, buffer=java.nio.HeapByteBuffer[pos=0 lim=837 cap=3762]))]), FetchableTopicResponse(topic='', topicId=DfV0uLbzTIWsDJUZN4L0zA, partitions=[PartitionData(partitionIndex=2, errorCode=0, highWatermark=2, lastStableOffset=2, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=558, buffer=java.nio.HeapByteBuffer[pos=0 lim=558 cap=2869])), PartitionData(partitionIndex=0, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1116, buffer=java.nio.HeapByteBuffer[pos=0 lim=1116 cap=2273])), PartitionData(partitionIndex=1, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1116, buffer=java.nio.HeapByteBuffer[pos=0 lim=1116 cap=1119]))])])
16:54:20.867 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Node 1 sent a full fetch response that created a new incremental fetch session 1391881732 with 9 response partition(s)
16:54:20.867 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition order-changes-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=2, lastStableOffset=2, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=551, buffer=java.nio.HeapByteBuffer[pos=0 lim=551 cap=8684]))
16:54:20.867 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition order-changes-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1102, buffer=java.nio.HeapByteBuffer[pos=0 lim=1102 cap=8095]))
16:54:20.867 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition order-changes-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1101, buffer=java.nio.HeapByteBuffer[pos=0 lim=1101 cap=6955]))
16:54:20.867 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition product-changes-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=3, lastStableOffset=3, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=840, buffer=java.nio.HeapByteBuffer[pos=0 lim=840 cap=5798]))
16:54:20.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition product-changes-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1120, buffer=java.nio.HeapByteBuffer[pos=0 lim=1120 cap=4920]))
16:54:20.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition product-changes-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=3, lastStableOffset=3, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=837, buffer=java.nio.HeapByteBuffer[pos=0 lim=837 cap=3762]))
16:54:20.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition customer-changes-2 returned fetch data PartitionData(partitionIndex=2, errorCode=0, highWatermark=2, lastStableOffset=2, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=558, buffer=java.nio.HeapByteBuffer[pos=0 lim=558 cap=2869]))
16:54:20.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition customer-changes-0 returned fetch data PartitionData(partitionIndex=0, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1116, buffer=java.nio.HeapByteBuffer[pos=0 lim=1116 cap=2273]))
16:54:20.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Fetch READ_UNCOMMITTED at offset 0 for partition customer-changes-1 returned fetch data PartitionData(partitionIndex=1, errorCode=0, highWatermark=4, lastStableOffset=4, logStartOffset=0, divergingEpoch=EpochEndOffset(epoch=-1, endOffset=-1), currentLeader=LeaderIdAndEpoch(leaderId=-1, leaderEpoch=-1), snapshotId=SnapshotId(endOffset=-1, epoch=-1), abortedTransactions=null, preferredReadReplica=-1, records=MemoryRecords(size=1116, buffer=java.nio.HeapByteBuffer[pos=0 lim=1116 cap=1119]))
16:54:20.868 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition order-changes-1 at position FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition order-changes-0 at position FetchPosition{offset=4, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition order-changes-2 at position FetchPosition{offset=4, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition product-changes-1 at position FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition product-changes-2 at position FetchPosition{offset=4, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition product-changes-0 at position FetchPosition{offset=3, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition customer-changes-2 at position FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition customer-changes-0 at position FetchPosition{offset=4, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Added READ_UNCOMMITTED fetch request for partition customer-changes-1 at position FetchPosition{offset=4, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[localhost:50137 (id: 1 rack: null)], epoch=0}} to node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Built incremental fetch (sessionId=1391881732, epoch=1) for node 1. Added 0 partition(s), altered 9 partition(s), removed 0 partition(s), replaced 0 partition(s) out of 9 partition(s)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(order-changes-1, order-changes-0, order-changes-2, product-changes-1, product-changes-2, product-changes-0, customer-changes-2, customer-changes-0, customer-changes-1), toForget=(), toReplace=(), implied=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:20.882 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=11, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=1391881732, sessionEpoch=1, topics=[FetchTopic(topic='order-changes', topicId=cI5xjwgmQZW30L0q2Q3M5g, partitions=[FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=2, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=4, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=4, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)]), FetchTopic(topic='product-changes', topicId=eBRtfLSOSZWfyQRckJSePQ, partitions=[FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=3, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=4, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=3, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)]), FetchTopic(topic='customer-changes', topicId=DfV0uLbzTIWsDJUZN4L0zA, partitions=[FetchPartition(partition=2, currentLeaderEpoch=0, fetchOffset=2, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=0, currentLeaderEpoch=0, fetchOffset=4, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576), FetchPartition(partition=1, currentLeaderEpoch=0, fetchOffset=4, lastFetchedEpoch=-1, logStartOffset=-1, partitionMaxBytes=1048576)])], forgottenTopicsData=[], rackId='')
Multi-topic test completed:
  product-changes: sent=10, received=10
  order-changes: sent=10, received=10
  customer-changes: sent=10, received=10
16:54:20.884 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending synchronous auto-commit of offsets {order-changes-1=OffsetAndMetadata{offset=2, leaderEpoch=0, metadata=''}, product-changes-1=OffsetAndMetadata{offset=3, leaderEpoch=0, metadata=''}, order-changes-0=OffsetAndMetadata{offset=4, leaderEpoch=0, metadata=''}, product-changes-2=OffsetAndMetadata{offset=4, leaderEpoch=0, metadata=''}, product-changes-0=OffsetAndMetadata{offset=3, leaderEpoch=0, metadata=''}, order-changes-2=OffsetAndMetadata{offset=4, leaderEpoch=0, metadata=''}, customer-changes-2=OffsetAndMetadata{offset=2, leaderEpoch=0, metadata=''}, customer-changes-0=OffsetAndMetadata{offset=4, leaderEpoch=0, metadata=''}, customer-changes-1=OffsetAndMetadata{offset=4, leaderEpoch=0, metadata=''}}
16:54:20.884 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending OFFSET_COMMIT request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=12, headerVersion=2) and timeout 30000 to node 2147483646: OffsetCommitRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', generationIdOrMemberEpoch=1, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, retentionTimeMs=-1, topics=[OffsetCommitRequestTopic(name='order-changes', partitions=[OffsetCommitRequestPartition(partitionIndex=1, committedOffset=2, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=0, committedOffset=4, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=2, committedOffset=4, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata='')]), OffsetCommitRequestTopic(name='product-changes', partitions=[OffsetCommitRequestPartition(partitionIndex=1, committedOffset=3, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=2, committedOffset=4, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=0, committedOffset=3, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata='')]), OffsetCommitRequestTopic(name='customer-changes', partitions=[OffsetCommitRequestPartition(partitionIndex=2, committedOffset=2, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=0, committedOffset=4, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata=''), OffsetCommitRequestPartition(partitionIndex=1, committedOffset=4, committedLeaderEpoch=0, commitTimestamp=-1, committedMetadata='')])])
16:54:20.887 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received OFFSET_COMMIT response from node 2147483646 for request with header RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=12, headerVersion=2): OffsetCommitResponseData(throttleTimeMs=0, topics=[OffsetCommitResponseTopic(name='product-changes', partitions=[OffsetCommitResponsePartition(partitionIndex=2, errorCode=0), OffsetCommitResponsePartition(partitionIndex=0, errorCode=0), OffsetCommitResponsePartition(partitionIndex=1, errorCode=0)]), OffsetCommitResponseTopic(name='order-changes', partitions=[OffsetCommitResponsePartition(partitionIndex=0, errorCode=0), OffsetCommitResponsePartition(partitionIndex=1, errorCode=0), OffsetCommitResponsePartition(partitionIndex=2, errorCode=0)]), OffsetCommitResponseTopic(name='customer-changes', partitions=[OffsetCommitResponsePartition(partitionIndex=1, errorCode=0), OffsetCommitResponsePartition(partitionIndex=2, errorCode=0), OffsetCommitResponsePartition(partitionIndex=0, errorCode=0)])])
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 4 for partition product-changes-2
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 3 for partition product-changes-0
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 3 for partition product-changes-1
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 4 for partition order-changes-0
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 2 for partition order-changes-1
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 4 for partition order-changes-2
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 4 for partition customer-changes-1
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 2 for partition customer-changes-2
16:54:20.887 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Committed offset 4 for partition customer-changes-0
16:54:20.888 [kafka-coordinator-heartbeat-thread | test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Heartbeat thread has closed
16:54:20.888 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Executing onLeavePrepare with generation Generation{generationId=1, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', protocol='range'}
16:54:20.888 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Revoke previously assigned partitions customer-changes-0, customer-changes-1, customer-changes-2, order-changes-0, order-changes-1, order-changes-2, product-changes-0, product-changes-1, product-changes-2
16:54:20.888 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Member consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5 sending LeaveGroup request to coordinator localhost:50137 (id: 2147483646 rack: null) due to the consumer is being closed
16:54:20.888 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending LEAVE_GROUP request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=13, headerVersion=2) and timeout 30000 to node 2147483646: LeaveGroupRequestData(groupId='test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f', memberId='', members=[MemberIdentity(memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, reason='the consumer is being closed')])
16:54:20.888 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Resetting generation and member id due to: consumer pro-actively leaving the group
16:54:20.888 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Request joining group due to: consumer pro-actively leaving the group
16:54:20.890 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received LEAVE_GROUP response from node 2147483646 for request with header RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=13, headerVersion=2): LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, errorCode=0)])
16:54:20.890 [main] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] LeaveGroup response with Generation{generationId=1, memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', protocol='range'} returned successfully: ClientResponse(receivedTimeMs=1759524860890, latencyMs=2, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=LEAVE_GROUP, apiVersion=5, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=13, headerVersion=2), responseBody=LeaveGroupResponseData(throttleTimeMs=0, errorCode=0, members=[MemberResponse(memberId='consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9-8a7e0eda-a793-4bac-8444-5e6696371dc5', groupInstanceId=null, errorCode=0)]))
16:54:20.890 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Set the metadata for next fetch request to close the existing session ID=1391881732
16:54:20.890 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Built full fetch (sessionId=1391881732, epoch=FINAL) for node 1 with 0 partition(s).
16:54:20.890 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending READ_UNCOMMITTED FullFetchRequest(toSend=(), canUseTopicIds=True) to broker localhost:50137 (id: 1 rack: null)
16:54:20.890 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Adding pending request for node localhost:50137 (id: 1 rack: null)
16:54:20.890 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Sending FETCH request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=14, headerVersion=2) and timeout 30000 to node 1: FetchRequestData(clusterId=null, replicaId=-1, replicaState=ReplicaState(replicaId=-1, replicaEpoch=-1), maxWaitMs=500, minBytes=1, maxBytes=52428800, isolationLevel=0, sessionId=1391881732, sessionEpoch=-1, topics=[], forgottenTopicsData=[], rackId='')
16:54:21.416 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=11, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=1391881732, responses=[])
16:54:21.416 [main] DEBUG org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Node 1 sent a full fetch response that created a new incremental fetch session 1391881732 with 0 response partition(s)
16:54:21.416 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Removing pending request for node localhost:50137 (id: 1 rack: null)
16:54:21.416 [main] DEBUG org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Received FETCH response from node 1 for request with header RequestHeader(apiKey=FETCH, apiVersion=13, clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, correlationId=14, headerVersion=2): FetchResponseData(throttleTimeMs=0, errorCode=0, sessionId=0, responses=[])
16:54:21.416 [main] DEBUG org.apache.kafka.clients.consumer.internals.AbstractFetch -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Successfully sent a close message for fetch session: 1391881732 to node: localhost:50137 (id: 1 rack: null)
16:54:21.417 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed
16:54:21.417 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter
16:54:21.417 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed
16:54:21.418 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9 unregistered
16:54:21.418 [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f-9, groupId=test-consumer-group-0e546036-63e1-48fa-b629-ece3db26214f] Kafka consumer has been closed
]]></system-out>
  </testcase>
</testsuite>